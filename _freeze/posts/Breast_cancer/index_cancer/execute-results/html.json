{
  "hash": "5b82944170e49c5a1d8a5593b0493a58",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classifying Breast Cancer: A Machine Learning Approach in R\" # More specific title\nauthor:\n  - name: Abdoulie Jallow\n    url: https://jallow-code.github.io/\ndate: \"2025-01-12\" # Standard date format\ncategories: [Learn, R, Machine Learning, Classification, Healthcare Data] # More specific categories\n---\n\n\n\n## Introduction\n\nBreast cancer remains a significant health concern globally. Early and accurate diagnosis is crucial for effective treatment and improving patient outcomes. Machine learning offers powerful tools to aid in this process by analyzing complex patterns in patient data.\n\nThis blog post walks through a practical exercise in building and evaluating machine learning classification models using R. We'll use the well-known Breast Cancer Wisconsin (Original) dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original) to classify tumors as either benign (non-cancerous) or malignant (cancerous) based on microscopic characteristics of cell nuclei.\n\nWe will cover the following steps:\n\n1.  Loading and exploring the dataset.\n2.  Preprocessing the data (handling missing values, encoding variables).\n3.  Analyzing feature relationships and addressing collinearity.\n4.  Splitting the dataset into training and testing sets.\n5.  Training various classification algorithms (Logistic Regression, KNN, LDA, QDA, Naive Bayes).\n6.  Evaluating model performance using metrics such as accuracy, AUC-ROC, and precision-recall curves.\n\nLet's dive in!\n\n## Exploratory Analysis and Preprocessing\n\nBefore building any models, understanding and preparing the data is essential. This involves inspecting the data structure, identifying and handling missing values, correcting data types, and exploring relationships between variables.\n\n### Understanding the Data Source\n\nThe first step is always to consult the dataset's documentation. It provides vital information about the variables, their meaning, potential issues like missing value codes, and the number of instances. For this dataset, the documentation tells us about the features measured from digitized images of fine needle aspirates (FNAs) of breast masses.\n\n### Importing the Dataset\n\nWe load the data, which is provided as a comma-separated file without headers. Based on the documentation, we assign meaningful column names\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the path to the dataset\npath <- \"dataset/breast-cancer-wisconsin.data\"\n\n# Read the data using read.table as it's a simple CSV without headers\ndata <- read.table(path, sep = \",\")\n\n# Assign column names based on the dataset documentation\ncolnames(data) <- c(\"Sample_code_number\", \"Clump_thickness\",\n                    \"Uniformity_of_cell_size\", \"Uniformity_of_cell_shape\",\n                    \"Marginal_adhesion\", \"Single_epithelial_cell_size\",\n                    \"Bare_nuclei\", \"Bland_chromatin\", \"Normal_nucleoli\", \n                    \"Mitoses\", \"Class\")\n\n# Display the structure of the data\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t699 obs. of  11 variables:\n $ Sample_code_number         : int  1000025 1002945 1015425 1016277 1017023 1017122 1018099 1018561 1033078 1033078 ...\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Uniformity_of_cell_shape   : int  1 4 1 8 1 10 1 2 1 1 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : chr  \"1\" \"10\" \"2\" \"4\" ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Normal_nucleoli            : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Mitoses                    : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : int  2 2 2 2 2 4 2 2 2 2 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Display the first few rows\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sample_code_number Clump_thickness Uniformity_of_cell_size\n1            1000025               5                       1\n2            1002945               5                       4\n3            1015425               3                       1\n4            1016277               6                       8\n5            1017023               4                       1\n6            1017122               8                      10\n  Uniformity_of_cell_shape Marginal_adhesion Single_epithelial_cell_size\n1                        1                 1                           2\n2                        4                 5                           7\n3                        1                 1                           2\n4                        8                 1                           3\n5                        1                 3                           2\n6                       10                 8                           7\n  Bare_nuclei Bland_chromatin Normal_nucleoli Mitoses Class\n1           1               3               1       1     2\n2          10               3               2       1     2\n3           2               3               1       1     2\n4           4               3               7       1     2\n5           1               3               1       1     2\n6          10               9               7       1     4\n```\n\n\n:::\n:::\n\n\n### Missing Values and Variable Encoding\n\nThe documentation reveals that missing values are coded as `\"?\"`. We need to replace these with R's standard `NA` representation. We also need to handle variables encoded incorrectly\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace \"?\" with NA\ndata[data == \"?\"] <- NA\n\n# Check how many missing values per column\ncolSums(is.na(data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Sample_code_number             Clump_thickness \n                          0                           0 \n    Uniformity_of_cell_size    Uniformity_of_cell_shape \n                          0                           0 \n          Marginal_adhesion Single_epithelial_cell_size \n                          0                           0 \n                Bare_nuclei             Bland_chromatin \n                         16                           0 \n            Normal_nucleoli                     Mitoses \n                          0                           0 \n                      Class \n                          0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# For simplicity in this example, we remove rows with any missing values.\n# In a real-world scenario, imputation might be considered.\ndata <- na.omit(data)\ncat(\"Dimensions after removing NAs:\", dim(data), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDimensions after removing NAs: 683 11 \n```\n\n\n:::\n\n```{.r .cell-code}\n# The 'Sample_code_number' is an identifier and not useful for classification.\ndata$Sample_code_number <- NULL\n\n# The 'Class' variable is coded as 2 for benign and 4 for malignant.\n# Convert it to a factor with meaningful labels.\ndata$Class <- factor(ifelse(data$Class == 2, \"benign\", \"malignant\"),\n                     levels = c(\"benign\", \"malignant\")) # Explicitly set levels\n\n# The 'Bare_nuclei' column was read as character due to the \"?\" values.\n# Now that NAs are handled, convert it to integer.\ndata$Bare_nuclei <- as.integer(data$Bare_nuclei)\n\n# Verify the structure again\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t683 obs. of  10 variables:\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Uniformity_of_cell_shape   : int  1 4 1 8 1 10 1 2 1 1 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : int  1 10 2 4 1 10 10 1 1 1 ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Normal_nucleoli            : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Mitoses                    : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the distribution of the target variable\nsummary(data$Class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   benign malignant \n      444       239 \n```\n\n\n:::\n:::\n\n\n::: callout-tip\nAlways check the documentation! As seen here, understanding how missing values (`\"?\"`) and the target variable (`Class`) were encoded was crucial for correct preprocessing. Failing to do this can lead to errors or incorrect model results.\n:::\n\n### Exploring Variable Distributions and Relationships\n\nLet's examine the predictor variables. Since they are all numerical (ratings from 1-10), we can look at their distributions and correlations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics for numerical predictors\nsummary(data[, -which(names(data) == \"Class\")]) # Exclude the Class variable\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Clump_thickness  Uniformity_of_cell_size Uniformity_of_cell_shape\n Min.   : 1.000   Min.   : 1.000          Min.   : 1.000          \n 1st Qu.: 2.000   1st Qu.: 1.000          1st Qu.: 1.000          \n Median : 4.000   Median : 1.000          Median : 1.000          \n Mean   : 4.442   Mean   : 3.151          Mean   : 3.215          \n 3rd Qu.: 6.000   3rd Qu.: 5.000          3rd Qu.: 5.000          \n Max.   :10.000   Max.   :10.000          Max.   :10.000          \n Marginal_adhesion Single_epithelial_cell_size  Bare_nuclei    \n Min.   : 1.00     Min.   : 1.000              Min.   : 1.000  \n 1st Qu.: 1.00     1st Qu.: 2.000              1st Qu.: 1.000  \n Median : 1.00     Median : 2.000              Median : 1.000  \n Mean   : 2.83     Mean   : 3.234              Mean   : 3.545  \n 3rd Qu.: 4.00     3rd Qu.: 4.000              3rd Qu.: 6.000  \n Max.   :10.00     Max.   :10.000              Max.   :10.000  \n Bland_chromatin  Normal_nucleoli    Mitoses      \n Min.   : 1.000   Min.   : 1.00   Min.   : 1.000  \n 1st Qu.: 2.000   1st Qu.: 1.00   1st Qu.: 1.000  \n Median : 3.000   Median : 1.00   Median : 1.000  \n Mean   : 3.445   Mean   : 2.87   Mean   : 1.603  \n 3rd Qu.: 5.000   3rd Qu.: 4.00   3rd Qu.: 1.000  \n Max.   :10.000   Max.   :10.00   Max.   :10.000  \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize distributions\n data %>%\n  select(-Class) %>%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %>%\n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  facet_wrap(~variable, scales = \"free_y\") +\n  labs(title = \"Distribution of Predictor Variables\", x = \"Value (1-10)\", y = \"Frequency\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_cancer_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Analyze correlations among continuous variables\n# Select only numeric predictors for correlation analysis\nnumeric_predictors <- data %>% select(-Class) %>% names()\ndata_corr <- cor(data[, numeric_predictors])\n\n# Visualize the correlation matrix\ncorrplot(data_corr, method = \"circle\", type = \"upper\", tl.col = \"black\", tl.srt = 45, order = \"hclust\")\n```\n\n::: {.cell-output-display}\n![](index_cancer_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Use pairs.panels for scatter plots and correlation values (optional, can be busy)\n# pairs.panels(data[, numeric_predictors], cex.labels = 0.7, ellipses = FALSE, lm=TRUE)\n```\n:::\n\n\nWe observe strong positive correlations between several variables, particularly `Uniformity_of_cell_size` and `Uniformity_of_cell_shape` (correlation coefficient often > 0.9). High collinearity can sometimes destabilize model coefficients (especially in linear models) and make interpretation harder.\n\nA common strategy is to remove one of the highly correlated variables. Let's remove Uniformity_of_cell_shape as it's highly correlated with Uniformity_of_cell_size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove 'Uniformity_of_cell_shape' due to high correlation with 'Uniformity_of_cell_size'\ndata$Uniformity_of_cell_shape <- NULL\n\n# Check the structure again\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t683 obs. of  9 variables:\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : int  1 10 2 4 1 10 10 1 1 1 ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Normal_nucleoli            : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Mitoses                    : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n```\n\n\n:::\n:::\n\n\n### Checking for Class Imbalance\n\nClass imbalance occurs when one class is much more frequent than the other. This can bias models towards the majority class. Let's check our target variable `Class`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate class proportions\nimbalance_summary <- data |>\n  count(Class) |>\n  mutate(Percentage = round((n / sum(n)) * 100, 1)) # Use sum(n) instead of nrow(data) just in case\n\nprint(imbalance_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Class   n Percentage\n1    benign 444         65\n2 malignant 239         35\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize class distribution\nggplot(imbalance_summary, aes(x = Class, y = n, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(n, \"\\n(\", Percentage, \"%)\"))) +\n  labs(title = \"Distribution of Target Variable (Class)\", x = \"Tumor Class\", y = \"Number of Samples\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Paired\")\n```\n\n::: {.cell-output-display}\n![](index_cancer_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe dataset shows a moderate imbalance: approximately 65% benign and 35% malignant. While not extreme, this is something to keep in mind during evaluation. Metrics like the Area Under the Precision-Recall Curve (AUC-PR) can be more informative than ROC-AUC in such cases. For this analysis, we'll proceed without specific resampling techniques (like SMOTE, upsampling, or downsampling), but they could be considered if performance on the minority class (malignant) is poor.\n\n### Splitting the Dataset\n\nTo evaluate our models' generalization ability, we split the data into a training set (used to build the models) and a test set (used for final, unbiased evaluation). We'll use a 70% / 30% split, stratified by the Class variable to maintain the class proportions in both sets.\n\nWe will also set up cross-validation (CV) within the training process. CV provides a more robust estimate of performance by repeatedly splitting the training data into smaller train/validation folds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a stratified split (70% train, 30% test)\ntrainIndex <- createDataPartition(data$Class, p = 0.7, list = FALSE, times = 1)\n\ndata.trn <- data[trainIndex, ]\ndata.tst <- data[-trainIndex, ]\n\ncat(\"Training set dimensions:\", dim(data.trn), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining set dimensions: 479 9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Test set dimensions:\", dim(data.tst), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest set dimensions: 204 9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training set class proportions:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining set class proportions:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(prop.table(table(data.trn$Class)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   benign malignant \n0.6492693 0.3507307 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Test set class proportions:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest set class proportions:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(prop.table(table(data.tst$Class)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   benign malignant \n0.6519608 0.3480392 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Set up control parameters for cross-validation within caret::train\n# We use 10-fold cross-validation\nctrl <- trainControl(method = \"cv\",\n                     number = 10,\n                     classProbs = TRUE, # Calculate class probabilities needed for ROC/PR\n                     savePredictions = \"final\", # Save predictions from CV folds\n                     summaryFunction = twoClassSummary) # Use metrics like ROC AUC\n```\n:::\n\n\n## Modeling\n\nNow, we'll train several different classification algorithms on the training data. caret provides a unified interface for this. We'll evaluate them using the cross-validation strategy defined above and then assess their final performance on the held-out test set.\n\nOur goal is to predict the `Class` variable based on the other features.\n\n### Logistic regression\n\nLogistic regression is one of the simplest and most interpretable classification algorithms. It models the probability of an instance belonging to a class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the Logistic Regression model\nglm.fit <- train(Class ~ .,\n                 data = data.trn,\n                 method = \"glm\",          # Generalized Linear Model\n                 family = \"binomial\",     # Specifies logistic regression\n                 trControl = ctrl,\n                 metric = \"ROC\")          # Optimize based on ROC AUC during CV\n\n# Print the best model summary (based on CV)\nprint(glm.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized Linear Model \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 432, 431, 432, 430, 431, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.9945209  0.9839718  0.9529412\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variable Importance for Logistic Regression\nvarImp(glm.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nglm variable importance\n\n                            Overall\nBare_nuclei                  100.00\nClump_thickness               72.20\nMarginal_adhesion             50.85\nNormal_nucleoli               48.85\nBland_chromatin               48.19\nMitoses                       46.49\nUniformity_of_cell_size       21.39\nSingle_epithelial_cell_size    0.00\n```\n\n\n:::\n\n```{.r .cell-code}\n# --- Evaluation on Test Set ---\n# Predict probabilities on the test set\nglm.pred.prob <- predict(glm.fit, newdata = data.tst, type = \"prob\")\n\n# Predict class based on a 0.5 probability threshold\nglm.pred.class <- ifelse(glm.pred.prob[, \"malignant\"] > 0.5, \"malignant\", \"benign\")\nglm.pred.class <- factor(glm.pred.class, levels = levels(data.tst$Class)) # Ensure factor levels match\n\n# Create confusion matrix\nlogistic_cm <- confusionMatrix(data = glm.pred.class,\n                               reference = data.tst$Class,\n                               positive = \"malignant\") # Specify the \"positive\" class\nprint(logistic_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       129         6\n  malignant      4        65\n                                          \n               Accuracy : 0.951           \n                 95% CI : (0.9117, 0.9762)\n    No Information Rate : 0.652           \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.8913          \n                                          \n Mcnemar's Test P-Value : 0.7518          \n                                          \n            Sensitivity : 0.9155          \n            Specificity : 0.9699          \n         Pos Pred Value : 0.9420          \n         Neg Pred Value : 0.9556          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3186          \n   Detection Prevalence : 0.3382          \n      Balanced Accuracy : 0.9427          \n                                          \n       'Positive' Class : malignant       \n                                          \n```\n\n\n:::\n:::\n\n\n### K-Nearest Neighbors (KNN)\n\nKNN is a non-parametric, instance-based learner. It classifies a new sample based on the majority class of its `'K'` nearest neighbors in the feature space. KNN requires features to be scaled. caret handles this automatically using the `preProcess` argument. We will also tune the hyperparameter `k` (number of neighbors).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the KNN model\n# caret will automatically test different values of 'k' specified in tuneGrid\nknn.fit <- train(Class ~ .,\n                 data = data.trn,\n                 method = \"knn\",\n                 trControl = ctrl,\n                 preProcess = c(\"center\", \"scale\"), # Crucial for KNN\n                 tuneGrid = data.frame(k = seq(3, 21, by = 2)), # Tune k (odd values often preferred)\n                 metric = \"ROC\")\n\n# Print the best tuning parameter found during CV\nprint(knn.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nk-Nearest Neighbors \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 431, 430, 431, 431, 431, ... \nResampling results across tuning parameters:\n\n  k   ROC        Sens       Spec     \n   3  0.9815999  0.9806452  0.9647059\n   5  0.9911023  0.9775202  0.9529412\n   7  0.9915856  0.9807460  0.9588235\n   9  0.9916716  0.9807460  0.9529412\n  11  0.9914819  0.9807460  0.9529412\n  13  0.9918643  0.9775202  0.9529412\n  15  0.9914848  0.9775202  0.9529412\n  17  0.9914848  0.9775202  0.9529412\n  19  0.9914848  0.9775202  0.9470588\n  21  0.9922438  0.9775202  0.9470588\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was k = 21.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the tuning results (ROC vs. k)\nplot(knn.fit)\n```\n\n::: {.cell-output-display}\n![](index_cancer_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# --- Evaluation on Test Set ---\n# Predict classes directly on the test set (KNN in caret often defaults to class)\nknn.pred.class <- predict(knn.fit, newdata = data.tst)\n\n# Create confusion matrix\nknn_cm <- confusionMatrix(data = knn.pred.class,\n                          reference = data.tst$Class,\n                          positive = \"malignant\")\nprint(knn_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       129         4\n  malignant      4        67\n                                          \n               Accuracy : 0.9608          \n                 95% CI : (0.9242, 0.9829)\n    No Information Rate : 0.652           \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9136          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9437          \n            Specificity : 0.9699          \n         Pos Pred Value : 0.9437          \n         Neg Pred Value : 0.9699          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3284          \n   Detection Prevalence : 0.3480          \n      Balanced Accuracy : 0.9568          \n                                          \n       'Positive' Class : malignant       \n                                          \n```\n\n\n:::\n:::\n\n\n### Linear Discriminant Analysis\n\nLDA is a linear classifier that assumes features are normally distributed within each class and have equal covariance matrices. It finds a linear combination of features that maximizes separability between classes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the LDA model\nlda.fit <- train(Class ~ .,\n                 data = data.trn,\n                 method = \"lda\",\n                 trControl = ctrl,\n                 metric = \"ROC\")\n\nprint(lda.fit) # LDA doesn't have hyperparameters to tune in this basic form\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Discriminant Analysis \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 431, 432, 431, 431, 431, ... \nResampling results:\n\n  ROC        Sens      Spec     \n  0.9946758  0.983871  0.9165441\n```\n\n\n:::\n\n```{.r .cell-code}\n# --- Evaluation on Test Set ---\n# Predict probabilities\nlda.pred.prob <- predict(lda.fit, newdata = data.tst, type = \"prob\")\n# Predict class\nlda.pred.class <- ifelse(lda.pred.prob[, \"malignant\"] > 0.5, \"malignant\", \"benign\")\nlda.pred.class <- factor(lda.pred.class, levels = levels(data.tst$Class))\n\n# Create confusion matrix\nlda_cm <- confusionMatrix(data = lda.pred.class,\n                          reference = data.tst$Class,\n                          positive = \"malignant\")\nprint(lda_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       130         7\n  malignant      3        64\n                                          \n               Accuracy : 0.951           \n                 95% CI : (0.9117, 0.9762)\n    No Information Rate : 0.652           \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.8905          \n                                          \n Mcnemar's Test P-Value : 0.3428          \n                                          \n            Sensitivity : 0.9014          \n            Specificity : 0.9774          \n         Pos Pred Value : 0.9552          \n         Neg Pred Value : 0.9489          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3137          \n   Detection Prevalence : 0.3284          \n      Balanced Accuracy : 0.9394          \n                                          \n       'Positive' Class : malignant       \n                                          \n```\n\n\n:::\n:::\n\n\n### Quadradic Discriminant Analysis\n\nQDA is similar to LDA but more flexible. It does not assume equal covariance matrices across classes, allowing for quadratic decision boundaries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the QDA model\nqda.fit <- train(Class ~ .,\n                 data = data.trn,\n                 method = \"qda\",\n                 trControl = ctrl,\n                 metric = \"ROC\")\n\nprint(qda.fit) # QDA also has no hyperparameters to tune here\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuadratic Discriminant Analysis \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 432, 431, 430, 431, 432, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.9896407  0.9517137  0.9819853\n```\n\n\n:::\n\n```{.r .cell-code}\n# --- Evaluation on Test Set ---\n# Predict probabilities\nqda.pred.prob <- predict(qda.fit, newdata = data.tst, type = \"prob\")\n# Predict class\nqda.pred.class <- ifelse(qda.pred.prob[, \"malignant\"] > 0.5, \"malignant\", \"benign\")\nqda.pred.class <- factor(qda.pred.class, levels = levels(data.tst$Class))\n\n# Create confusion matrix\nqda_cm <- confusionMatrix(data = qda.pred.class,\n                          reference = data.tst$Class,\n                          positive = \"malignant\")\nprint(qda_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       123         3\n  malignant     10        68\n                                          \n               Accuracy : 0.9363          \n                 95% CI : (0.8935, 0.9656)\n    No Information Rate : 0.652           \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.8627          \n                                          \n Mcnemar's Test P-Value : 0.09609         \n                                          \n            Sensitivity : 0.9577          \n            Specificity : 0.9248          \n         Pos Pred Value : 0.8718          \n         Neg Pred Value : 0.9762          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3333          \n   Detection Prevalence : 0.3824          \n      Balanced Accuracy : 0.9413          \n                                          \n       'Positive' Class : malignant       \n                                          \n```\n\n\n:::\n:::\n\n\n### Naive Bayes\n\nNaive Bayes is a probabilistic classifier based on Bayes' theorem. It makes a \"naive\" assumption that all predictor variables are independent given the class. Despite this simplification, it often works well in practice.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the Naive Bayes model\n# Note: Need to install e1071 or klaR package if not already installed for method='naive_bayes'\n# install.packages(\"e1071\")\nnb.fit <- train(Class ~ .,\n                data = data.trn,\n                method = \"naive_bayes\", # Uses e1071::naiveBayes implementation\n                trControl = ctrl,\n                metric = \"ROC\")\n\nprint(nb.fit) # Might show tuning parameters if the implementation supports them (e.g., Laplace correction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNaive Bayes \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 432, 431, 431, 430, 431, 431, ... \nResampling results across tuning parameters:\n\n  usekernel  ROC        Sens       Spec     \n  FALSE      0.9920956  0.9549395  0.9878676\n   TRUE      0.9936136  0.9742944  0.9819853\n\nTuning parameter 'laplace' was held constant at a value of 0\nTuning\n parameter 'adjust' was held constant at a value of 1\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were laplace = 0, usekernel = TRUE\n and adjust = 1.\n```\n\n\n:::\n\n```{.r .cell-code}\n# --- Evaluation on Test Set ---\n# Predict probabilities\nnb.pred.prob <- predict(nb.fit, newdata = data.tst, type = \"prob\")\n# Predict class\nnb.pred.class <- ifelse(nb.pred.prob[, \"malignant\"] > 0.5, \"malignant\", \"benign\")\nnb.pred.class <- factor(nb.pred.class, levels = levels(data.tst$Class))\n\n# Create confusion matrix\nnb_cm <- confusionMatrix(data = nb.pred.class,\n                         reference = data.tst$Class,\n                         positive = \"malignant\")\nprint(nb_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       126         4\n  malignant      7        67\n                                          \n               Accuracy : 0.9461          \n                 95% CI : (0.9056, 0.9728)\n    No Information Rate : 0.652           \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.8823          \n                                          \n Mcnemar's Test P-Value : 0.5465          \n                                          \n            Sensitivity : 0.9437          \n            Specificity : 0.9474          \n         Pos Pred Value : 0.9054          \n         Neg Pred Value : 0.9692          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3284          \n   Detection Prevalence : 0.3627          \n      Balanced Accuracy : 0.9455          \n                                          \n       'Positive' Class : malignant       \n                                          \n```\n\n\n:::\n:::\n\n\n## Performance Comparison\n\nWe've trained five different models. Let's compare their performance on the test set using standard evaluation metrics and visualizations. We'll focus on ROC curves, Precision-Recall curves, and overall Accuracy.\n\n### AUC-ROC Curve Analysis\n\nThe Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) at various classification thresholds. The Area Under the Curve (AUC-ROC) summarizes the model's ability to discriminate between the positive (malignant) and negative (benign) classes across all thresholds. An AUC of 1 indicates perfect discrimination, while 0.5 suggests random guessing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Function to calculate ROC curve using pROC\ncalculate_roc <- function(model, test_data, response_var, positive_class) {\n  pred_probs <- predict(model, newdata = test_data, type = \"prob\")\n  response <- test_data[[response_var]]\n  roc_curve <- roc(response = response,\n                   predictor = pred_probs[, positive_class],\n                   levels = levels(response),\n                   direction = \"<\", # Ensure correct direction based on probability meaning\n                   quiet = TRUE) # Suppress messages\n  return(roc_curve)\n}\n\n# Calculate ROC for each model\nroc_glm <- calculate_roc(glm.fit, data.tst, \"Class\", \"malignant\")\nroc_knn <- calculate_roc(knn.fit, data.tst, \"Class\", \"malignant\")\nroc_lda <- calculate_roc(lda.fit, data.tst, \"Class\", \"malignant\")\nroc_qda <- calculate_roc(qda.fit, data.tst, \"Class\", \"malignant\")\nroc_nb <- calculate_roc(nb.fit, data.tst, \"Class\", \"malignant\")\n\n# Extract AUC values\nauc_values <- c(\n  Logistic = auc(roc_glm),\n  KNN = auc(roc_knn),\n  LDA = auc(roc_lda),\n  QDA = auc(roc_qda),\n  `Naive Bayes` = auc(roc_nb)\n)\n\n# Plot ROC Curves using base R plotting and pROC integration\npar(pty = \"s\") # Set square aspect ratio\n\nplot(roc_glm, col = \"#1B9E77\", lwd = 2, print.auc = FALSE, legacy.axes = TRUE, main = \"Comparison of ROC Curves\") # Start plot\nplot(roc_knn, col = \"#D95F02\", lwd = 2, add = TRUE, print.auc = FALSE)\nplot(roc_lda, col = \"#7570B3\", lwd = 2, add = TRUE, print.auc = FALSE)\nplot(roc_qda, col = \"#E7298A\", lwd = 2, add = TRUE, print.auc = FALSE)\nplot(roc_nb, col = \"#66A61E\", lwd = 2, add = TRUE, print.auc = FALSE)\n\n# Add Legend\nlegend(\"bottomright\",\n       legend = paste(names(auc_values), \"(AUC =\", round(auc_values, 3), \")\"),\n       col = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\"),\n       lwd = 2, cex = 0.8)\n```\n\n::: {.cell-output-display}\n![ROC Curves for all models on the test set. Higher curves (closer to top-left) and larger AUC values indicate better discrimination.](index_cancer_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAll models achieve extremely high AUC-ROC values (close to 1.0) on the test set. This suggests that the classes in this dataset are highly separable using the given features. While excellent performance is encouraging, AUCs this high can sometimes warrant double-checking for potential data leakage or indicate that the dataset might not fully represent the complexity of real-world scenarios. However, given the standard preprocessing steps, it likely reflects the relatively clear separation in this classic dataset. Logistic Regression, KNN, and LDA show near-perfect discrimination.\n\n### Precision Recall Curve\n\nThe Precision-Recall (PR) curve plots Precision (Positive Predictive Value) against Recall (Sensitivity or True Positive Rate). It is particularly informative when dealing with imbalanced datasets, as it focuses on the performance for the positive class (malignant, in our case) without involving True Negatives. The Area Under the PR Curve (AUC-PR) summarizes this performance.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Function to calculate PR curve data using PRROC\ncalculate_pr <- function(model, test_data, true_labels, positive_class) {\n  pred_probs <- predict(model, newdata = test_data, type = \"prob\")[, positive_class]\n  true_binary <- ifelse(true_labels == positive_class, 1, 0)\n  pr <- pr.curve(scores.class0 = pred_probs, weights.class0 = true_binary, curve = TRUE)\n  return(pr)\n}\n\n# Calculate PR curves\npr_glm <- calculate_pr(glm.fit, data.tst, data.tst$Class, \"malignant\")\npr_knn <- calculate_pr(knn.fit, data.tst, data.tst$Class, \"malignant\")\npr_lda <- calculate_pr(lda.fit, data.tst, data.tst$Class, \"malignant\")\npr_qda <- calculate_pr(qda.fit, data.tst, data.tst$Class, \"malignant\")\npr_nb <- calculate_pr(nb.fit, data.tst, data.tst$Class, \"malignant\")\n\n# Extract AUC-PR values\nauc_pr_values <- c(\n  Logistic = pr_glm$auc.integral,\n  KNN = pr_knn$auc.integral,\n  LDA = pr_lda$auc.integral,\n  QDA = pr_qda$auc.integral,\n  `Naive Bayes` = pr_nb$auc.integral\n)\n\n# Plot PR curves using PRROC's plot function\nplot(pr_glm, main = \"Comparison of Precision-Recall Curves\", col = \"#1B9E77\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_knn, add = TRUE, col = \"#D95F02\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_lda, add = TRUE, col = \"#7570B3\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_qda, add = TRUE, col = \"#E7298A\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_nb, add = TRUE, col = \"#66A61E\", lwd = 2, auc.main = FALSE, legend = FALSE)\n\n# Add Legend\nlegend(\"bottomleft\",\n       legend = paste(names(auc_pr_values), \"(AUC-PR =\", round(auc_pr_values, 3), \")\"),\n       col = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\"),\n       lwd = 2, cex = 0.8, bty = \"n\") # bty=\"n\" removes legend box\n```\n\n::: {.cell-output-display}\n![Precision-Recall Curves for all models on the test set. Curves closer to the top-right indicate better performance.](index_cancer_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nSimilar to the ROC curves, the AUC-PR values are very high for all models, indicating excellent precision and recall for identifying malignant cases. Logistic Regression, KNN, and LDA again show top-tier performance with AUC-PR values very close to 1.0.\n\n### Accuracy\n\nAccuracy is the overall proportion of correctly classified instances. While useful, it can be misleading on imbalanced datasets if a model simply predicts the majority class well. However, since both Sensitivity and Specificity were high for most models (as seen in the confusion matrices), accuracy here provides a reasonable summary.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Consolidate model performance metrics from confusion matrices\nmodel_performance <- data.frame(\n  Model = c(\"Logistic\", \"KNN\", \"LDA\", \"QDA\", \"Naive Bayes\"),\n  Accuracy = c(logistic_cm$overall[\"Accuracy\"],\n               knn_cm$overall[\"Accuracy\"],\n               lda_cm$overall[\"Accuracy\"],\n               qda_cm$overall[\"Accuracy\"],\n               nb_cm$overall[\"Accuracy\"]),\n  LowerCI = c(logistic_cm$overall[\"AccuracyLower\"],\n              knn_cm$overall[\"AccuracyLower\"],\n              lda_cm$overall[\"AccuracyLower\"],\n              qda_cm$overall[\"AccuracyLower\"],\n              nb_cm$overall[\"AccuracyLower\"]),\n  UpperCI = c(logistic_cm$overall[\"AccuracyUpper\"],\n              knn_cm$overall[\"AccuracyUpper\"],\n              lda_cm$overall[\"AccuracyUpper\"],\n              qda_cm$overall[\"AccuracyUpper\"],\n              nb_cm$overall[\"AccuracyUpper\"])\n)\n\n# Plot using ggplot2\nggplot(model_performance, aes(x = Accuracy, y = reorder(Model, Accuracy))) + # Reorder models by accuracy\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_errorbarh(aes(xmin = LowerCI, xmax = UpperCI), height = 0.2, color = \"steelblue\") +\n  labs(title = \"Model Accuracy Comparison (with 95% CI)\",\n       x = \"Accuracy\",\n       y = \"Model\") +\n  scale_x_continuous(limits = c(0.85, 1.0)) + # Adjust limits based on observed values\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n```\n\n::: {.cell-output-display}\n![Accuracy of models on the test set with 95% confidence intervals.](index_cancer_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe accuracy plot confirms the high performance across all models, with most achieving accuracy above 95%. Logistic Regression, KNN, and LDA appear slightly superior, consistent with the AUC metrics. The confidence intervals are relatively tight, indicating stable performance estimates.\n\n## Conclusion\n\nIn this analysis, we successfully applied several machine learning classification techniques to the Breast Cancer Wisconsin dataset using R. After careful preprocessing, which included handling missing values and addressing collinearity, we trained and evaluated Logistic Regression, KNN, LDA, QDA, and Naive Bayes models.\n\nKey findings:\n\n- All models performed exceptionally well on this dataset, achieving high accuracy, AUC-ROC, and AUC-PR values, indicating strong predictive power for distinguishing between benign and malignant tumors based on the provided cell characteristics.\n- The high performance likely reflects the relatively clear separability of classes within this specific, well-studied dataset. Real-world medical datasets often present greater challenges.\n\nBased on these results, Logistic Regression or LDA could be considered excellent choices due to their high performance and relative simplicity/interpretability compared to KNN. KNN also performed exceptionally well after tuning.",
    "supporting": [
      "index_cancer_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}