<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abdoulie Jallow">
<meta name="dcterms.date" content="2024-11-14">
<meta name="description" content="I’m continuing my journey through the classic textbook An Introduction to Statistical Learning. In this post, I’ll walk through my solutions for the Chapter 4 exercises, which cover the topic of Classification. This book is the work of Gareth M. James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.">

<title>An Introduction to Statistical Learning – Abdoulie Jallow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-86d18d57d40d9ebbce0ff5423cfb353b.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-a430e38053e2dc7758acfb30ede2fcf5.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-86d18d57d40d9ebbce0ff5423cfb353b.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-493c6dc785330d9cf20a879c6a386dc1.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-dacc9576340fc99ee60ae369930cbefc.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-493c6dc785330d9cf20a879c6a386dc1.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/ek-styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Abdoulie Jallow</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jallow-code" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">An Introduction to Statistical Learning</h1>
                  <div>
        <div class="description">
          I’m continuing my journey through the classic textbook An Introduction to Statistical Learning. In this post, I’ll walk through my solutions for the Chapter 4 exercises, which cover the topic of Classification. This book is the work of Gareth M. James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Learn</div>
                <div class="quarto-category">R</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://jallow-code.github.io/">Abdoulie Jallow</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 14, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#classification" id="toc-classification" class="nav-link active" data-scroll-target="#classification">Classification</a>
  <ul>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#conceptual" id="toc-conceptual" class="nav-link" data-scroll-target="#conceptual">Conceptual</a></li>
  <li><a href="#applied" id="toc-applied" class="nav-link" data-scroll-target="#applied">Applied</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">






<!--
# Statistical Learning

## Summary

Statistical learning refers to a large collection of tools aimed at estimating the relationship between a set of predictors ($X = X_1, X_2, ..., X_p$) and a response ($Y$). This relationship can be expressed as $Y = f(X) + \epsilon$, where $f$ is some fixed but unknown function of $X_1, X_2, ..., X_p$ and $\epsilon$ represents random error which is independent of $X$ and has mean zero. These tools are generally classified into two categories: supervised learning and unsupervised learning. Supervised learning involves estimating an output based on one or more inputs. In contrast, unsupervised learning involves inputs but no supervising outputs.

Supervised learning has two primary purposes. The first objective is making predictions, where inputs ($X$) are readily available, but the output ($Y$) cannot be easily obtained. $Y$ can be estimated using $\hat{Y} = \hat{f}(X)$, where $\hat{f}$ is our estimate for the true function $f$. In this setting, one is not typically concerned with the exact form of $\hat{f}$ as long as it yields accurate predictions for $Y$. The second objective is making inferences, where we are interested in the association between $Y$ and $X$. Hence, the exact form of $\hat{f}$ is very important. Key questions in inference include: which predictors are significantly associated with the response? What is the nature of the relationship between each predictor and the response? Can the relationship be adequately modeled by a linear equation, or is a more complex model necessary?

Methods for estimating $f$ are either parametric (assuming a specific form for $f$, resulting in less flexible but more interpretable models) or non-parametric (making no assumptions about $f$, leading to more flexible but less interpretable models). Problems with a quantitative response are often referred to as regression problems, while those involving a qualitative response are referred to as classification problems. In regression, model accuracy is assessed by minimizing test mean squared error (MSE), which involves balancing the bias and variance of the estimate. Generally, more flexible methods increase variance and decrease bias, and finding the optimal balance minimizes test MSE.

In classification, model performance is evaluated using the test error rate. The Bayes classifier, a theoretical benchmark, minimizes this rate by assigning observations to the class with the highest conditional probability. The k-Nearest Neighbors (KNN) algorithm provides a practical approach by estimating these probabilities based on the K nearest training points. The flexibility of KNN is controlled by the value of K: small K leads to high variance and low bias, while large K results in low variance and high bias. Balancing this trade-off is crucial to avoid overfitting (high variance) or underfitting (high bias).

## Exercises

### Conceptual

#### Question 1

For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

a.  The sample size $n$ is extremely large, and the number of predictors $p$ is small.

> A flexible method will be better because the model does not need to work hard to find patterns in the data hence reducing the likelihood of overfitting. Additionally, a flexible method in this situation will results in a lower test mean squared error (MSE), as the reduction in bias compensates for the increased variance.

b.  The number of predictors $p$ is extremely large, and the number of observations $n$ is small.

> In this situation an Inflexible method preferable because there is higher chance for the model to pickup patterns that may not exist in the test data.

c.  The relationship between the predictors and response is highly non-linear.

> A flexible method is preferred because an inflexible method will lead to high bias.

d.  The variance of the error terms, i.e. $\sigma^2 = Var(\epsilon)$, is extremely high.

> In this case, an Inflexible method will perform better. With a flexible method, it may be difficult to differentiate the true pattern from the noise in the data. Flexible method have a higher variance because they fit more data.

#### Question 2

Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$.

a.  We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

> -   sample size: 500
> -   predictors: profit, number of employees, and industry (3)
> -   regression problem because the response is quantitative
> -   We are most interested in making inference.

b.  We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

> -   sample size: 20
> -   predictors: 13
> -   classification problem because the response is qualitative (success or failure)
> -   We are interested in making predictions.

c.  We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

> -   sample size: 52
> -   predictors: 3
> -   regression problem because the response is quantitative
> -   We are interested in making prediction.

#### Question 3

We now revisit the bias-variance decomposition.

a.  Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

b.  Explain why each of the five curves has the shape displayed in part (a).

> -   Squared Bias: Decreases as model flexibility increases since more flexible methods are better at capturing complex patterns in the data.
> -   Variance: Increases with greater flexibility, as flexible models are more sensitive to variations in the training data.
> -   Training Error: Decreases with increasing flexibility because more complex models can closely fit the training data.
> -   Test Error: Initially decreases with flexibility due to reduced bias but eventually increases as overfitting leads to higher variance.
> -   Bayes Error: Remains constant, as it is independent of the model's complexity and reflects the inherent noise in the data.

#### Question 4

You will now think of some real-life applications for statistical learning.

a.  Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

> Email Classification (Spam or Not Spam):
>
> -   Variables ($p$): Certain keywords, email structure, sender information
>
> -   Aim: Prediction
>
> Factors Associated with Winning an Election:
>
> -   Variables ($p$): Campaign budget, market trends, poll results, etc.
>
> -   Aim: Inference
>
> Food Spoilage (Healthy or Not Healthy for Consumption)
>
> -   Variables ($p$): Color, texture, smell, etc.
>
> -   Aim: Prediction

b.  Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

> Analyzing the Effect of Marketing Budget on Sales Response: Total revenue
>
> -   Variables ($p$): Marketing budget allocated to different channels (e.g., social media, TV ads, print media), time of year, and product price.
>
> -   Aim: Inference
>
> Predicting Housing Prices Response Variable: Sale price of a house.
>
> -   Predictors: Square footage, number of bedrooms, number of bathrooms, location etc
>
> -   Aim: Prediction

c.  Describe three real-life applications in which cluster analysis might be useful.

> -   Genomic data analysis for example RNA seq.
> -   finding categories in illnesses, cells, or organisms

#### Question 5

What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

> Inflexible methods are more interpretable and are generally preferred in cases with small $n$ (sample size) and large $p$ (number of predictors). However, they may suffer from high bias when the true underlying function is non-linear. Flexible methods, while capable of capturing non-linear patterns, are prone to overfitting, leading to high variance in error. In situations with a large $n$ and small $p$, flexible methods are preferred.

#### Question 6

Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?

> Parametric methods are more interpretable because they rely on a predefined model to generate the response. They also tend to perform better than non-parametric methods in high-dimensional settings, as they require estimating fewer parameters. However, a key disadvantage is that the model may not accurately reflect reality. If the assumed model is far from the true underlying relationship, the resulting estimates can be poor.
>
> In contrast, non-parametric methods are highly flexible and can adapt to a wide range of underlying patterns. However, this flexibility comes at a cost: they require a larger number of observations to produce accurate estimates, as they do not rely on a small set of parameters and are more prone to overfitting when data is limited

#### Question 7

The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

| Obs. | $X_1$ | $X_2$ | $X_3$ | $Y$   |
|------|-------|-------|-------|-------|
| 1    | 0     | 3     | 0     | Red   |
| 2    | 2     | 0     | 0     | Red   |
| 3    | 0     | 1     | 3     | Red   |
| 4    | 0     | 1     | 2     | Green |
| 5    | -1    | 0     | 1     | Green |
| 6    | 1     | 1     | 1     | Red   |

Suppose we wish to use this data set to make a prediction for $Y$ when $X_1 = X_2 = X_3 = 0$ using $K$-nearest neighbors.

a.  Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$. $$
    \text{Distance} = \sqrt{(X_{1_{\text{obs}}} - 0)^2 + (X_{2_{\text{obs}}} - 0)^2 + (X_{3_{\text{obs}}} - 0)^2}
    $$


::: {.cell}
::: {.cell-output .cell-output-stdout}

```
  Obs Distance     Y
1   5 1.414214 Green
2   6 1.732051   Red
3   2 2.000000   Red
4   4 2.236068 Green
5   1 3.000000   Red
6   3 3.162278   Red
```


:::
:::


b.  What is our prediction with $K = 1$? Why?

    > Our prediction will be `Green` because it have the smallest distance (obs. 5)

c.  What is our prediction with $K = 3$? Why?

    > When $K = 3$ our prediction will be `Red` because it is the majority (i.e, 2 Red and 1 Green).

d.  If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for $K$ to be large or small? Why?

> If the Bayes decision boundary is highly non-linear, a smaller $k$ is better. This is because smaller $k$ will capture the non-linear patterns in data. A large $k$ may lead to underfiting since the true boundary is highly non-linear.

### Applied


::: {.cell}

```{.r .cell-code}
library(knitr)
library(tidyverse)
library(psych)
library(ISLR2) 
```
:::


#### Question 8

This exercise relates to the `College` data set, which can be found in the file `College.csv`. It contains a number of variables for 777 different universities and colleges in the US. The variables are

-   `Private` : Public/private indicator
-   `Apps` : Number of applications received
-   `Accept` : Number of applicants accepted
-   `Enroll` : Number of new students enrolled
-   `Top10perc` : New students from top 10% of high school class
-   `Top25perc` : New students from top 25% of high school class
-   `F.Undergrad` : Number of full-time undergraduates
-   `P.Undergrad` : Number of part-time undergraduates
-   `Outstate` : Out-of-state tuition
-   `Room.Board` : Room and board costs
-   `Books` : Estimated book costs
-   `Personal` : Estimated personal spending
-   `PhD` : Percent of faculty with Ph.D.'s
-   `Terminal` : Percent of faculty with terminal degree
-   `S.F.Ratio` : Student/faculty ratio
-   `perc.alumni` : Percent of alumni who donate
-   `Expend` : Instructional expenditure per student
-   `Grad.Rate` : Graduation rate

Before reading the data into `R`, it can be viewed in Excel or a text editor.

a.  Use the `read.csv()` function to read the data into `R`. Call the loaded data `college`. Make sure that you have the directory set to the correct location for the data.


::: {.cell}

```{.r .cell-code}
college <- read.csv("College.csv")
```
:::


b.  Look at the data using the `View()` function. You should notice that the first column is just the name of each university. We don't really want `R` to treat this as data. However, it may be handy to have these names for later. Try the following commands:


::: {.cell}

```{.r .cell-code}
rownames(college) <- college[, 1]
View(college)
```
:::


You should see that there is now a `row.names` column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. `R` will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try


::: {.cell}

```{.r .cell-code}
college <- college [, -1]
View(college)
```
:::


Now you should see that the first data column is `Private`. Note that another column labeled `row.names` now appears before the `Private` column. However, this is not a data column but rather the name that R is giving to each row.

c.  

i.  Use the `summary()` function to produce a numerical summary of the variables in the data set.

ii. Use the `pairs()` function to produce a scatter plot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using `A[,1:10]`.

iii. Use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Private`.

iv. Create a new qualitative variable, called `Elite`, by *binning* the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.


::: {.cell}

```{.r .cell-code}
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
```
:::


Use the `summary()` function to see how many elite universities there are. Now use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Elite`.

v.  Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow=c(2,2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.

w.  Continue exploring the data, and provide a brief summary of what you discover.


::: {.cell}

```{.r .cell-code}
# First column(i.e, Private) is excluded since it is qualitative
pairs(college[, 2:11],
      cex = 0.2,
      col = 4,
      cex.labels = 0.6,  # Size of diagonal texts
      font.labels = 2)   # Font style of diagonal texts
```

::: {.cell-output-display}
![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}
:::

```{.r .cell-code}
school_type <- ifelse(college$Private == "Yes", "Private", "Public")
school_type <- as.factor(school_type)

plot(college$Outstate ~ factor(school_type),
     col = "white",
     xlab = "",
     ylab = "Outstate")

stripchart(college$Outstate ~ factor(school_type),
           method = "jitter",
           pch = 19,
           col = 2:4,
           vertical = TRUE,
           add = TRUE)
```

::: {.cell-output-display}
![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}
:::

```{.r .cell-code}
par(mfrow = c(2,2))

bin_values <- c(10, 20, 30, 50)
lapply(bin_values, function(n) {
  hist(log10(college$Enroll), 
       breaks = n,
       col = 4,
       main = paste("Bins =", n),
       xlab = "log(Enroll)")
})
```

::: {.cell-output-display}
![](index_files/figure-html/unnamed-chunk-7-3.png){width=672}
:::
:::


#### Question 9

This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.


::: {.cell}

```{.r .cell-code}
data <- read.table("Auto.data", header = TRUE, na.strings = "?")
data <- na.omit(data)

data$cylinders <- as.factor(data$cylinders)
data$origin <- as.factor(data$origin)
data$year <- as.factor(data$year)
str(data)
```

::: {.cell-output .cell-output-stdout}

```
'data.frame':   392 obs. of  9 variables:
 $ mpg         : num  18 15 18 16 17 15 14 14 14 15 ...
 $ cylinders   : Factor w/ 5 levels "3","4","5","6",..: 5 5 5 5 5 5 5 5 5 5 ...
 $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...
 $ horsepower  : num  130 165 150 150 140 198 220 215 225 190 ...
 $ weight      : num  3504 3693 3436 3433 3449 ...
 $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
 $ year        : Factor w/ 13 levels "70","71","72",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ origin      : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
 $ name        : chr  "chevrolet chevelle malibu" "buick skylark 320" "plymouth satellite" "amc rebel sst" ...
 - attr(*, "na.action")= 'omit' Named int [1:5] 33 127 331 337 355
  ..- attr(*, "names")= chr [1:5] "33" "127" "331" "337" ...
```


:::
:::


a.  Which of the predictors are quantitative, and which are qualitative?

> Quantitative: mpg, cylinders, displacement, horsepower, weight, acceleration.
>
> Qualitative: name, year, origin.

b.  What is the range of each quantitative predictor? You can answer this using the `range()` function.


::: {.cell}

```{.r .cell-code}
sapply(data[, c(1,3,4,5,6)], range)
```

::: {.cell-output .cell-output-stdout}

```
      mpg displacement horsepower weight acceleration
[1,]  9.0           68         46   1613          8.0
[2,] 46.6          455        230   5140         24.8
```


:::
:::


c.  What is the mean and standard deviation of each quantitative predictor?


::: {.cell}

```{.r .cell-code}
data[, c(1,3,4,5,6)] |>
  pivot_longer(everything()) |>
  group_by(name) |>
  summarise(
    Mean = mean(value),
    SD = sd(value)
  ) |>
  knitr::kable()
```

::: {.cell-output-display}


|name         |       Mean|         SD|
|:------------|----------:|----------:|
|acceleration |   15.54133|   2.758864|
|displacement |  194.41199| 104.644004|
|horsepower   |  104.46939|  38.491160|
|mpg          |   23.44592|   7.805008|
|weight       | 2977.58418| 849.402560|


:::
:::


d.  Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?


::: {.cell}

```{.r .cell-code}
data[-(10:85), c(1,3,4,5,6)] |>
  pivot_longer(everything()) |>
  group_by(name) |>
  summarise(
    Range = diff(range(value)),
    Mean = mean(value),
    SD = sd(value)
  ) |>
  knitr::kable()
```

::: {.cell-output-display}


|name         |  Range|       Mean|         SD|
|:------------|------:|----------:|----------:|
|acceleration |   16.3|   15.72690|   2.693721|
|displacement |  387.0|  187.24051|  99.678367|
|horsepower   |  184.0|  100.72152|  35.708853|
|mpg          |   35.6|   24.40443|   7.867283|
|weight       | 3348.0| 2935.97152| 811.300208|


:::
:::


e.  Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.


::: {.cell}

```{.r .cell-code}
pairs.panels(data[1:8], ellipses = FALSE)
```

::: {.cell-output-display}
![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}
:::
:::


> There are both positive and negative correlation between the variables.

f.  Suppose that we wish to predict gas mileage (`mpg`) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting `mpg`? Justify your answer.

> Yes, since other variables are correlated. However, horsepower, weight and displacement are highly related.

#### Question 10

This exercise involves the `Boston` housing data set.

a.  To begin, load in the `Boston` data set. The `Boston` data set is part of the `ISLR2` library in R.

Now the data set is contained in the object `Boston`.


::: {.cell}

```{.r .cell-code}
View(Boston)
```
:::


Read about the data set:


::: {.cell}

```{.r .cell-code}
?Boston
```
:::


How many rows are in this data set? How many columns? What do the rows and columns represent?


::: {.cell}

```{.r .cell-code}
dim(Boston)
```

::: {.cell-output .cell-output-stdout}

```
[1] 506  13
```


:::
:::


b.  Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.


::: {.cell}

```{.r .cell-code}
pairs.panels(Boston[1:7], ellipses = FALSE, cex.labels = 1)
```

::: {.cell-output-display}
![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}
:::
:::


c.  Are any of the predictors associated with per capita crime rate? If so, explain the relationship.


::: {.cell}

```{.r .cell-code}
cor.plot(Boston, xlas = 2, main = "Correlation plot")
```

::: {.cell-output-display}
![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}
:::
:::


> Per capita crime shows a moderately positive correlation with both `rad` and `tax`.

d.  Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.


::: {.cell}

```{.r .cell-code}
# top 5% of crime,tax,and pupil-teacher ratios
cat("High crime:", sum(Boston$crim > quantile(Boston$crim, 0.95)), "\n") 
```

::: {.cell-output .cell-output-stdout}

```
High crime: 26 
```


:::

```{.r .cell-code}
cat("High tax:", sum(Boston$tax > quantile(Boston$tax, 0.95)), "\n")
```

::: {.cell-output .cell-output-stdout}

```
High tax: 5 
```


:::

```{.r .cell-code}
cat("Pupil-teacher ratio:", sum(Boston$ptratio > quantile(Boston$ptratio, 0.95)),"\n")
```

::: {.cell-output .cell-output-stdout}

```
Pupil-teacher ratio: 18 
```


:::
:::


e.  How many of the census tracts in this data set bound the Charles river?


::: {.cell}

```{.r .cell-code}
sum(Boston$chas==1)
```

::: {.cell-output .cell-output-stdout}

```
[1] 35
```


:::
:::


f.  What is the median pupil-teacher ratio among the towns in this data set?


::: {.cell}

```{.r .cell-code}
median_ptratio <- median(Boston$ptratio)
cat("Median pupil-teacher ratio:", median_ptratio, "\n")
```

::: {.cell-output .cell-output-stdout}

```
Median pupil-teacher ratio: 19.05 
```


:::
:::


g.  Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.


::: {.cell}

```{.r .cell-code}
# whichsuburbs that have the lowest median property values.
which(Boston$medv == min(Boston$medv))

# Values of other predictors for suburb 406
Boston[406,]

range(Boston$crim)
```
:::


> There are two suburbs that have the lowest median property values. crime is negatively correlated with median property value.

h.  In this data set, how many of the census tract average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.


::: {.cell}

```{.r .cell-code}
cat("More than 7 rooms:",sum(Boston$rm > 7),"\n")
```

::: {.cell-output .cell-output-stdout}

```
More than 7 rooms: 64 
```


:::

```{.r .cell-code}
cat("More than 8 rooms:", sum(Boston$rm > 8), "\n")
```

::: {.cell-output .cell-output-stdout}

```
More than 8 rooms: 13 
```


:::
:::


Making summaries:


::: {.cell}

```{.r .cell-code}
tracts_more_than_8 <- Boston[Boston$rm > 8, ]
# Reshape data to long format using pivot_longer()
data_long <- tracts_more_than_8 |>
  pivot_longer(cols = -rm, names_to = "Predictor", values_to = "Value")

# Create boxplots with facet_wrap
ggplot(data_long, aes(x = Predictor, y = Value, fill = Predictor)) +
  geom_boxplot(outlier.color = "red", outlier.size = 1.5) +
  facet_wrap(~ Predictor, scales = "free", ncol = 4) +  # Each plot gets its own scale
  theme_minimal() +
  labs(title = "Boxplots for Predictors with More Than 8 Rooms",
       x = "Predictors",
       y = "Values") +
  theme(axis.text.x = element_blank(),  # Remove x-axis text (not meaningful here)
        axis.ticks.x = element_blank(),
        strip.text = element_text(size = 10, face = "bold"))
```

::: {.cell-output-display}
![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}
:::
:::


# Linear Regression

## Exercises

### Conceptual

### Applied


<!-- -->
<section id="classification" class="level1">
<h1>Classification</h1>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section id="conceptual" class="level3">
<h3 class="anchored" data-anchor-id="conceptual">Conceptual</h3>
<ol type="1">
<li>Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.</li>
</ol>
<p>The logistic function is define as</p>
<p><span id="eq-4.2"><span class="math display">\[
p(X) = \frac{e^{\beta_o + \beta_1X}}{1\;+ e^{\beta_o + \beta_1X}}\\
\tag{1}\]</span></span></p>
<p>Rearranging terms, <span class="math display">\[p(X) = \frac{e^{(β₀ + β₁X)}} {1 + e^{(β₀ + β₁X)}}\]</span> Let <span class="math inline">\(L = e^{(β₀ + β₁X)}\)</span>. Then <span class="math inline">\(p(X) = \frac{L}{1 + L}\)</span>.</p>
<p><span class="math display">\[\begin{gather*}

p(X) \cdot{} (1 + L) = L\\
p(X) + p(X) \cdot{} L = L\\
p(X) = L - p(X) \cdot{} L\\
p(X) = L \cdot{} (1 - p(X))\\
\frac{p(X)}{(1 - p(X)} = L\\

\end{gather*}\]</span></p>
<p>Substituting <span class="math inline">\(L\)</span> back gives the logit representation:</p>
<p><span id="eq-4.3"><span class="math display">\[
\frac{p(X)}{1 - p(X)} = e^{\beta_o + \beta_1X}
\tag{2}\]</span></span></p>
<ol start="2" type="1">
<li>It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(µk,σ2) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.</li>
</ol>
<p><span id="eq-4.17"><span class="math display">\[
p_k(x) = \frac{\pi_k\frac{1}{\sqrt{2\pi\sigma}} \cdot \exp(-\frac{1}{2\sigma^2}(x - \mu_k)^2)}
              {\sum_{l=1}^k \pi_l\frac{1}{\sqrt{2\pi\sigma}} \cdot \exp(-\frac{1}{2\sigma^2}(x - \mu_l)^2)}
\tag{3}\]</span></span></p>
<p>The discriminant function is</p>
<p><span id="eq-4.18"><span class="math display">\[
\delta_k(x) = x.\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma_2} + \log(\pi_k)
\tag{4}\]</span></span></p>
<p>To show <a href="#eq-4.17" class="quarto-xref">Equation&nbsp;3</a> is equals to <a href="#eq-4.18" class="quarto-xref">Equation&nbsp;4</a>, we first assume <span class="math inline">\(\sigma_1^2\;=\;...\;=\sigma_k^2\)</span>. Hence,</p>
<p><span class="math display">\[
p_k(x) = \frac{\pi_k \exp\left(-\frac{1}{2\sigma^2}(x - \mu_k)^2\right)}
              {\sum_{l=1}^k \pi_l \exp\left(-\frac{1}{2\sigma^2}(x - \mu_l)^2\right)}
\]</span></p>
<p>Next, we take the <span class="math inline">\(\log(p_K(X))\)</span> to linearized the function. <span class="math display">\[
\log(p_k(x)) = \log(\frac{\pi_k \exp\left(-\frac{1}{2\sigma^2}(x - \mu_k)^2\right)}{\sum_{l=1}^k \pi_l \exp\left(-\frac{1}{2\sigma^2}(x - \mu_l)^2\right)})
\]</span> Rearranging terms</p>
<p><span class="math display">\[
\log(p_k(x)) = \log(\pi_k) - \frac{1}{2\sigma^2}(x - \mu_k)^2 -
              \log\left(\sum_{l=1}^k \pi_l \exp\left(-\frac{1}{2\sigma^2}(x - \mu_l)^2\right)\right)
\]</span></p>
<p>To maximize over <span class="math inline">\(k\)</span>, any term that’s independent of <span class="math inline">\(k\)</span> is ignored</p>
<p><span class="math display">\[\begin{align}
f &amp;= \log(\pi_k) - \frac{1}{2\sigma^2} (x^2 - 2x\mu_k + \mu_k^2) \\
  &amp;= \log(\pi_k) - \frac{x^2}{2\sigma^2} + \frac{x\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} \\
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\frac{x^2}{2\sigma^2}\)</span> is also independent of <span class="math inline">\(k\)</span>, it is ignored in order to maximize over <span class="math inline">\(k\)</span></p>
<p><span class="math display">\[
\log(\pi_k) + \frac{x\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2}
\]</span></p>
<ol start="3" type="1">
<li>This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e.&nbsp;there is only one feature.</li>
</ol>
<p>Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.</p>
<p>To classify an observation, the Bayes classifier assigns it to the class <span class="math inline">\(k\)</span> for which the posterior probability <span class="math inline">\(p_k(x)\)</span> is largest. This is equivalent to assigning the observation to the class that maximizes the discriminant function, <span class="math inline">\(δ_k(x)\)</span>. For a one-dimensional QDA model, we do not assume the variances are equal across classes.</p>
<p>The discriminant function for class <span class="math inline">\(k\)</span> is derived from the posterior probability: <span class="math display">\[δ_k(x) = log(p_k(x)) = log(π_k \cdot{} f_k(x)) - log(C)\]</span> where <span class="math inline">\(f_k(x)\)</span> is the normal density <span class="math inline">\(N(μ_k, σ_k^2)\)</span> and <span class="math inline">\(C\)</span> is the denominator <span class="math inline">\(Σ P(Y=l)f_l(x)\)</span>, which doesn’t depend on <span class="math inline">\(k\)</span>. To maximize over <span class="math inline">\(k\)</span>, we can ignore <span class="math inline">\(C\)</span>.</p>
<p>Let’s expand the function: <span class="math display">\[δ_k(x) ∝ log(π_k) + log(f_k(x))\]</span> <span class="math display">\[δ_k(x) = log(π_k) - log(√(2πσ_k^2)) - (\frac{1}{2σ_k^2}) \cdot{} (x - μ_k)^2\]</span></p>
<p>Expanding the squared term: <span class="math display">\[δ_k(x) = log(π_k) - (\frac{1}{2})\;log\;(2πσ_k^2) - (\frac{1}{2σ_k^2}) \cdot{} (x^2 - 2xμ_k + μ_k^2)\]</span></p>
<p>Rearranging the terms to see the structure as a function of <span class="math inline">\(x\)</span>: <span class="math display">\[δ_k(x) = - (\frac{1}{2σ_k^2})x^2 + (\frac{μ_k}{σ_k^2})x + [log(π_k) - (\frac{1}{2})log(2πσ_k^2) - (\frac{μ_k^2}{2σ_k^2})]\]</span></p>
<p>This discriminant function <span class="math inline">\(δ_k(x)\)</span> is a quadratic function of <span class="math inline">\(x\)</span>. The coefficient of the <span class="math inline">\(x^2\)</span> term is <span class="math inline">\(\frac{1}{2σ_k^2}\)</span>. Since QDA assumes that <span class="math inline">\(σ_k^2\)</span> is different for each class <span class="math inline">\(k\)</span>, this quadratic term does not cancel out when comparing the discriminant functions of two different classes (i.e., when finding the decision boundary where <span class="math inline">\(δ_k(x) = δ_j(x))\)</span>.</p>
<p>Therefore, the resulting decision boundary is quadratic, and the Bayes classifier is not linear.</p>
<ol start="4" type="1">
<li>When the number of features <span class="math inline">\(p\)</span> is large, there tends to be a deterioration in the performance of KNN and other local approaches… This phenomenon is known as the curse of dimensionality… We will now investigate this curse.</li>
</ol>
<!-- -->
<ol type="a">
<li>Suppose that we have a set of observations, each with measurements on <span class="math inline">\(p = 1\)</span> feature, <span class="math inline">\(X\)</span>… On average, what fraction of the available observations will we use to make the prediction?</li>
</ol>
<p>Since <span class="math inline">\(X\)</span> is uniformly distributed on <span class="math inline">\([0,1]\)</span>, its range is <span class="math inline">\(1\)</span>. The prediction for a test observation <span class="math inline">\(X=0.6\)</span> uses training observations in the range <span class="math inline">\([0.55, 0.65]\)</span>. The length of this interval is <span class="math inline">\(0.65 - 0.55 = 0.1\)</span>. Because the data is uniform, the fraction of observations that fall into an interval is equal to the length of that interval. Therefore, on average, we will use <span class="math inline">\(10\%\)</span> of the available observations.</p>
<ol start="2" type="a">
<li>Now suppose that we have… <span class="math inline">\(p = 2\)</span> features… On average, what fraction of the available observations will we use?</li>
</ol>
<p>We are now in two dimensions, and we form a neighborhood by taking a <span class="math inline">\(10\%\)</span> range for each feature. This creates a square (a 2-D hypercube) centered on the test point. The side length for this square is 0.1 for each dimension. Since the features are uniformly distributed on [0,1] x [0,1], the fraction of observations we use is the area of this square, which is <span class="math inline">\(0.1 \cdot{} 0.1 = 0.01\)</span>. So, on average, we will use only <span class="math inline">\(1%\)</span> of the observations.</p>
<ol start="3" type="a">
<li>Now suppose that we have… <span class="math inline">\(p = 100\)</span> features… What fraction of the available observations will we use?</li>
</ol>
<p>Following the pattern, for <span class="math inline">\(p=100\)</span> dimensions, our neighborhood is a 100-dimensional hypercube where each side has length <span class="math inline">\(0.1\)</span>. The volume of this hypercube is <span class="math inline">\(0.1^{100}\)</span>. This is an infinitesimally small number, meaning we expect to use virtually zero fraction of the available observations.</p>
<ol start="4" type="a">
<li>Using your answers… argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.</li>
</ol>
<p>The answers to parts (a)-(c) show that the fraction of training data available in a local neighborhood of a fixed size (<span class="math inline">\(10\%\)</span> of the range of each feature) decreases exponentially as the number of dimensions p increases <span class="math inline">\((0.1^p)\)</span>. For a large <span class="math inline">\(p\)</span>, this fraction is so small that the neighborhood is effectively empty. This means there are no “nearby” neighbors to average for a prediction, which undermines the entire principle of KNN and other local methods. This is the curse of dimensionality.</p>
<ol start="5" type="a">
<li>Now suppose that we wish to make a prediction… by creating a p-dimensional hypercube… that contains, on average, <span class="math inline">\(10\%\)</span> of the training observations. For <span class="math inline">\(p\)</span> = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer.</li>
</ol>
<p>Let <span class="math inline">\(l\)</span> be the side length of the p-dimensional hypercube. The volume is <span class="math inline">\(l^p\)</span>. We want this volume to be <span class="math inline">\(0.1\)</span> (to capture <span class="math inline">\(10\%\)</span> of the uniform data). So, we solve <span class="math inline">\(l^p = 0.1\)</span> for <span class="math inline">\(l\)</span>, which gives <span class="math inline">\(l = (0.1)^{(1/p)}\)</span>.</p>
<ul>
<li>For <span class="math inline">\(p = 1: l = (0.1)^{(1/1)} = 0.1\)</span></li>
<li>For <span class="math inline">\(p = 2: l = (0.1)^{(1/2)} ≈ 0.316\)</span></li>
<li>For <span class="math inline">\(p = 100: l = (0.1)^{(1/100)} ≈ 0.977\)</span></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As the dimension <span class="math inline">\(p\)</span> increases, the side length <span class="math inline">\(l\)</span> of the hypercube needed to capture just <span class="math inline">\(10\%\)</span> of the data rapidly approaches <span class="math inline">\(1\)</span>. For <span class="math inline">\(p=100\)</span>, each side of the “local” neighborhood must span over <span class="math inline">\(97\%\)</span> of the total range of its corresponding feature. This means the points within this hypercube are no longer truly “local” or “near” the test observation. The method loses its local character, and the predictions are based on points that are far away, which leads to poor performance.</p>
</div>
</div>
<ol start="5" type="1">
<li>We now examine the differences between LDA and QDA.</li>
</ol>
<!-- -->
<ol type="a">
<li>If the bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?</li>
</ol>
<blockquote class="blockquote">
<p>QDA been the more flexible model will perform better on the training set, but worse on the test set. On the test set, QDA will overfit the data because the true decision boundary is linear.</p>
</blockquote>
<ol start="2" type="a">
<li>If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?</li>
</ol>
<blockquote class="blockquote">
<p>In this situation since the decision boundary is non-linear, QDA will perform better in both data sets. A linear model will underfit in this case.</p>
</blockquote>
<ol start="3" type="a">
<li>In general, as the sample size <span class="math inline">\(n\)</span> increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?</li>
</ol>
<blockquote class="blockquote">
<p>As sample size increase, QDA will improve because there is more data to fit and the low bias will offset increase in variance.</p>
</blockquote>
<ol start="4" type="a">
<li>True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.</li>
</ol>
<blockquote class="blockquote">
<p>False. In this case, QDA will overfit the data.</p>
</blockquote>
<ol start="6" type="1">
<li>Suppose we collect data for a group of students in a statistics class with variables <span class="math inline">\(X_1 =\)</span> hours studied, <span class="math inline">\(X_2 =\)</span> undergrad GPA, and <span class="math inline">\(Y =\)</span> receive an A. We fit a logistic regression and produce estimated coefficient, <span class="math inline">\(\hat\beta_0 = -6\)</span>, <span class="math inline">\(\hat\beta_1 = 0.05\)</span>, <span class="math inline">\(\hat\beta_2 = 1\)</span>.</li>
</ol>
<p>(a). Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.</p>
<p><span class="math display">\[
p(X) = \frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}}
\]</span></p>
<p>when <span class="math inline">\(X_1 = 40\)</span> and <span class="math inline">\(X_2 = 3.5\)</span>, <span class="math inline">\(p(X) = 0.38\)</span></p>
<ol start="2" type="a">
<li>How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?</li>
</ol>
<p><span class="math display">\[\begin{gather}

\log\left(\frac{p(X)}{1-p(x)}\right) = -6 + 0.05X_1 + X_2 \\

\log\left(\frac{0.5}{1-0.5}\right) = -6 + 0.05X_1 + 3.5 \\

\end{gather}\]</span></p>
<p>Therefore, solving the equation <span class="math inline">\(0 = −6 + 0.05X_1 + 3.5\)</span>, <span class="math inline">\(X_1 = 50\)</span> hours.</p>
<ol start="7" type="1">
<li>Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on <span class="math inline">\(X\)</span>, last year’s percent profit. We examine a large number of companies and discover that the mean value of <span class="math inline">\(X\)</span> for companies that issued a dividend was <span class="math inline">\(\bar{X} = 10\)</span>, while the mean for those that didn’t was <span class="math inline">\(\bar{X} = 0\)</span>. In addition, the variance of <span class="math inline">\(X\)</span> for these two sets of companies was <span class="math inline">\(\hat{\sigma}^2 = 36\)</span>. Finally, 80% of companies issued dividends. Assuming that <span class="math inline">\(X\)</span> follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was <span class="math inline">\(X = 4\)</span> last year.</li>
</ol>
<p><em>Hint: Recall that the density function for a normal random variable is</em> <span class="math inline">\(f(x) =\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\)</span>. <em>You will need to use Bayes’ theorem.</em></p>
<p>Value of companies issuing a dividend (D) = <span class="math inline">\(D \sim \mathcal{N}(10, 36)\)</span>.</p>
<p>Value <span class="math inline">\(v\)</span> for companies not issuing a dividend <span class="math inline">\((D^c)\)</span> = <span class="math inline">\(D^c \sim \mathcal{N}(0, 36)\)</span> and <span class="math inline">\(p(D) = 0.8\)</span>.</p>
<p>Bayes theorem:</p>
<p><span class="math display">\[\begin{align}
P(D|X) &amp;= \frac{P(D) \cdot{} P(X|D)}{P(D) \cdot{} P(X|D) + P(D^c) \cdot{} P(X|D^c)} \\
\end{align}\]</span></p>
<p>Substitute the Gaussian likelihoods into Bayes theorem</p>
<p><span class="math display">\[\begin{align}
P(D|X) &amp;= \frac{P(D) \cdot{} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu_D)^2/2\sigma^2}}
               {P(D) \cdot{} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu_D)^2/2\sigma^2} +
                P(D^c) \cdot{} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu_{D^c})^2/2\sigma^2}} \\
\end{align}\]</span></p>
<p>Factor out <span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}}\)</span> from the numerator and denominator, which cancels out:</p>
<p><span class="math display">\[\begin{align}
P(D|X)  &amp;= \frac{P(D) \cdot{} e^{-(x-\mu_D)^2/2\sigma^2}}
               {P(D) \cdot{} e^{-(x-\mu_D)^2/2\sigma^2} +
                P(D^c) \cdot{} e^{-(x-\mu_{D^c})^2/2\sigma^2}} \\
\end{align}\]</span></p>
<p>Substitute the given probabilities and means:</p>
<p><span class="math display">\[\begin{align}
P(D|X)  &amp;= \frac{0.8 \times e^{-(4-10)^2/(2 \times 36)}}
               {0.8 \times e^{-(4-10)^2/(2 \times 36)} + 0.2 \times e^{-(4-0)^2/(2 \times 36)}} \\
       &amp;= \frac{0.8 \cdot e^{-1/2}}{0.8 \cdot e^{-1/2} + 0.2 \cdot e^{-2/9}} \\
\end{align}\]</span></p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">*</span> <span class="fl">0.8</span> <span class="sc">/</span> (<span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">*</span> <span class="fl">0.8</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">/</span><span class="dv">9</span>) <span class="sc">*</span> <span class="fl">0.2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7518525</code></pre>
</div>
</div>
<ol start="8" type="1">
<li>Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e.&nbsp;<span class="math inline">\(K = 1\)</span>) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?</li>
</ol>
<blockquote class="blockquote">
<p>Logistic regression will perform better because it has a lower test error rate. For <span class="math inline">\(K = 1\)</span>, the training error rate is always zero because the closest point is always the training point itself, so the model will never make a mistake on the training set. Given that the average error rate for 1-NN is 18%, this implies a test error rate of 36%. Logistic regression, with a test error rate of 30%, is therefore the better choice.</p>
</blockquote>
<ol start="9" type="1">
<li>This problem has to do with <em>odds</em>.</li>
</ol>
<!-- -->
<ol type="a">
<li>On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?</li>
</ol>
<p><span class="math display">\[
\frac{p(x)}{1 - P(x)} = odd\\
\]</span> <span class="math display">\[\begin{equation}
p(x) = \frac{odd}{1 + odd}\\
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
p(x)  = \frac{0.37}{1 + 0.37} = 0.27
\end{equation}\]</span></p>
<ol start="2" type="a">
<li>Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?</li>
</ol>
<p><span class="math display">\[\frac{0.16}{1 - 0.16}  = 0.19\]</span></p>
<ol start="10" type="1">
<li>In the setting with p = 1, (4.32) takes a simpler form…repeat the calculation in (4.32), and provide expressions for <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_{kj}\)</span> in terms of <span class="math inline">\(πk\)</span>, <span class="math inline">\(πK\)</span>, <span class="math inline">\(μk\)</span>, <span class="math inline">\(μK\)</span>, and <span class="math inline">\(σ2\)</span>.</li>
</ol>
<p>The goal is to find the log-odds between class <span class="math inline">\(k\)</span> and a baseline class <span class="math inline">\(K\)</span> for an LDA model with <span class="math inline">\(p=1\)</span>. This is given by <span class="math inline">\(log(\frac{P(Y=k|X=x)}{P(Y=K|X=x)})\)</span>. This is equivalent to <span class="math inline">\(δ_k(x) - δ_K(x)\)</span>.</p>
<p>The discriminant function for LDA is: <span class="math display">\[δ_k(x) = x \cdot{} (\frac{μ_k}{σ^2}) - (\frac{μ_k^2}{2σ^2}) + log(π_k)\]</span></p>
<p>The log-odds are: <span class="math display">\[δ_k(x) - δ_K(x) = [x \cdot{} (\frac{μ_k}{σ^2}) - (\frac{μ_k^2}{2σ^2}) + log(π_k)] - [x \cdot{} (\frac{μ_K}{σ^2}) - (\frac{μ_K^2}{2σ^2}) + log(π_K)]\]</span></p>
<p>Now, group the terms that are constant and those that are coefficients of <span class="math inline">\(x\)</span>: <span class="math display">\[= x \cdot{} (\frac{μ_k}{σ^2} - \frac{μ_K}{σ^2}) + [log(π_k) - log(π_K) - \frac{μ_k^2}{2σ^2} + \frac{μ_K^2}{2σ^2}]\]</span></p>
<p>This has the form <span class="math inline">\(a_k + b_{k1} \cdot{} x\)</span>. The coefficients are: <span class="math display">\[a_k = log(\frac{π_k}{π_K}) - \frac{μ_k^2 - μ_K^2}{2σ^2}\]</span> <span class="math display">\[b_k1 = \frac{μ_k - μ_K}{σ^2}\]</span> 11. Work out the detailed forms of <span class="math inline">\(a_k\)</span>, <span class="math inline">\(b_{kj}\)</span>, and <span class="math inline">\(b_{kjl}\)</span> in (4.33). Your answer should involve <span class="math inline">\(πk\)</span>, <span class="math inline">\(μk\)</span>, <span class="math inline">\(Σk\)</span>. (Assuming (4.33) refers to the general quadratic form of the QDA discriminant function).</p>
<p>The QDA discriminant function is: <span class="math display">\[δ_k(x) = -\frac{1}{2} \cdot{} log(det(Σ_k)) -\frac{1}{2} \cdot{} (x-μ_k)^T \cdot{} Σ_k^{-1} \cdot{} (x-μ_k) + log(π_k)\]</span> Expanding the quadratic term <span class="math inline">\((x-μ_k)^T \cdot{} Σ_k^{-1} \cdot{} (x-μ_k)\)</span>: <span class="math display">\[= x^TΣ_k^{-1}x - x^TΣ_k^{-1}μ_k - μ_k^TΣ_k^{-1}x + μ_k^TΣ_k^{-1}μ_k\]</span> Since <span class="math inline">\(x^TΣ_k^{-1}μ_k\)</span> is a scalar, it’s equal to its transpose <span class="math display">\[μ_k^TΣ_k^{-1T}x = μ_k^TΣ_k^{-1}x\]</span>.</p>
<p>So the expression is <span class="math display">\[x^TΣ_k^{-1}x - 2x^TΣ_k^{A-1}μ_k + μ_k^TΣ_k^{-1}μ_k\]</span>.</p>
<p>Substituting this back into <span class="math inline">\(δ_k(x)\)</span>: <span class="math display">\[δ_k(x) = -\frac{1}{2} \cdot{} x^TΣ_k^{-1}x + x^TΣ_k^{-1}μ_k - \frac{1}{2} \cdot{} μ_k^TΣ_k^{-1}μ_k - \frac{1}{2} \cdot{} log(det(Σ_k)) + log(π_k)\]</span> This is a quadratic function of <span class="math inline">\(x\)</span>. We can identify the constant, linear, and quadratic parts.</p>
<p>The constant term is <span class="math inline">\(a_k\)</span>: <span class="math display">\[a_k = log(π_k) - \frac{1}{2} \cdot{} log(det(Σ_k)) - \frac{1}{2} \cdot{} μ_k^TΣ_k^{-1}μ_k\]</span> The linear part is <span class="math inline">\(x^T(Σ_k^{-1}μ_k)\)</span>. The vector of coefficients for the linear terms is <span class="math inline">\(b_k = Σ_k^{-1}μ_k\)</span>. The coefficient for the j-th variable, <span class="math inline">\(x_j\)</span>, is: <span class="math inline">\(b_{kj} = (Σ_k^{-1}μ_k)_j\)</span></p>
<p>The quadratic part is <span class="math inline">\(-\frac{1}{2} \cdot{} x^TΣ_k^{-1}x\)</span>. The matrix of coefficients for the quadratic terms <span class="math inline">\(x_j \cdot{} x_l\)</span> is <span class="math inline">\(-\frac{1}{2} \cdot{} Σ_k^{-1}\)</span>. The coefficient <span class="math inline">\(b_{kjl}\)</span> is: <span class="math display">\[b_{kjl} = (\frac{1}{2} \cdot Σ_k^{-1})_{jl}\]</span></p>
<ol start="12" type="1">
<li>Suppose that you wish to classify an observation <span class="math inline">\(X \in \mathbb{R}\)</span> into <code>apples</code> and <code>oranges</code>. You fit a logistic regression model and find that</li>
</ol>
<p><span class="math display">\[
\hat{Pr}(Y=orange|X=x) =
\frac{\exp(\hat\beta_0 + \hat\beta_1x)}{1 + \exp(\hat\beta_0 + \hat\beta_1x)}
\]</span></p>
<p>Your friend fits a logistic regression model to the same data using the <em>softmax</em> formulation in (4.13), and finds that</p>
<p><span class="math display">\[
\hat{Pr}(Y=orange|X=x) =
\frac{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x)}
{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x) + \exp(\hat\alpha_{apple0} + \hat\alpha_{apple1}x)}
\]</span></p>
<ol type="a">
<li>What is the log odds of <code>orange</code> versus <code>apple</code> in your model?</li>
</ol>
<blockquote class="blockquote">
<p>The log odds (<span class="math inline">\(\frac{p(x)}{1 - P(x)}\)</span>) in my model will be <span class="math inline">\(\hat\beta_0 + \hat\beta_1x\)</span></p>
</blockquote>
<ol start="2" type="a">
<li>What is the log odds of <code>orange</code> versus <code>apple</code> in your friend’s model?</li>
</ol>
<p>Log odds of our friend model:</p>
<p><span class="math display">\[
(\hat\alpha_{orange0} - \hat\alpha_{apple0}) + (\hat\alpha_{orange1} - \hat\alpha_{apple1})x
\]</span></p>
<ol start="3" type="a">
<li>Suppose that in your model, <span class="math inline">\(\hat\beta_0 = 2\)</span> and <span class="math inline">\(\hat\beta = −1\)</span>. What are the coefficient estimates in your friend’s model? Be as specific as possible.</li>
</ol>
<p>The coefficient estimate in my friend’s model:</p>
<ul>
<li><span class="math inline">\(\hat\alpha_{orange0} -\hat\alpha_{apple0} = 2\)</span></li>
<li><span class="math inline">\(\hat\alpha_{orange1} - \hat\alpha_{apple1} = -1\)</span>.</li>
</ul>
<ol start="4" type="a">
<li>Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates <span class="math inline">\(\hat\alpha_{orange0} = 1.2\)</span>, <span class="math inline">\(\hat\alpha_{orange1} = −2\)</span>, <span class="math inline">\(\hat\alpha_{apple0} = 3\)</span>, <span class="math inline">\(\hat\alpha_{apple1} = 0.6\)</span>. What are the coefficient estimates in your model?</li>
</ol>
<p>The coefficients in my model would be <span class="math inline">\(\hat\beta_0 = 1.2 - 3 = -1.8\)</span> and <span class="math inline">\(\hat\beta_1 = -2 - 0.6 = -2.6\)</span></p>
<ol start="5" type="a">
<li>Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.</li>
</ol>
<p>They should agree everytime (i.e, pridictions and log odd between any pair of classes will remain the same, regardless of coding). The cofficient will be different because of the choice of baseline.</p>
</section>
<section id="applied" class="level3">
<h3 class="anchored" data-anchor-id="applied">Applied</h3>
<ol start="13" type="1">
<li>This question should be answered using the <code>Weekly</code> data set, which is part of the <code>ISLR2</code> package. This data is similar in nature to the <code>Smarket</code> data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.</li>
</ol>
<!-- -->
<ol type="a">
<li>Produce some numerical and graphical summaries of the <code>Weekly</code> data. Do there appear to be any patterns?</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2) <span class="co"># data sets</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(psych) <span class="co"># for correlation plots</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS) <span class="co"># for lda() and qda()</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class) <span class="co"># for knn()</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071) <span class="co"># for naiveBayes()</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"Weekly"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Weekly)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Weekly)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>'data.frame':   1089 obs. of  9 variables:
 $ Year     : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...
 $ Lag1     : num  0.816 -0.27 -2.576 3.514 0.712 ...
 $ Lag2     : num  1.572 0.816 -0.27 -2.576 3.514 ...
 $ Lag3     : num  -3.936 1.572 0.816 -0.27 -2.576 ...
 $ Lag4     : num  -0.229 -3.936 1.572 0.816 -0.27 ...
 $ Lag5     : num  -3.484 -0.229 -3.936 1.572 0.816 ...
 $ Volume   : num  0.155 0.149 0.16 0.162 0.154 ...
 $ Today    : num  -0.27 -2.576 3.514 0.712 1.178 ...
 $ Direction: Factor w/ 2 levels "Down","Up": 1 1 2 2 2 1 2 2 2 1 ...</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Weekly<span class="sc">$</span>Year <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Weekly<span class="sc">$</span>Year)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs.panels</span>(Weekly[<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>], <span class="at">cex.labels =</span> <span class="dv">1</span>, <span class="at">ellipses =</span> <span class="cn">FALSE</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># total number of times the market had a positive or negative return</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(Weekly<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Down   Up 
 484  605 </code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>There is a strong positive correlation between the volume of shares traded and the year. From 2005 to 2010, the volume of shares began increasing exponentially. Additionally, between 1990 and 2010, the market had a total of 605 positive returns and 484 negative returns, respectively.</p>
</blockquote>
<ol start="2" type="a">
<li>Use the full data set to perform a logistic regression with <code>Direction</code> as the response and the five lag variables plus <code>Volume</code> as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4 <span class="sc">+</span> Lag5 <span class="sc">+</span> Volume,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">family =</span> binomial)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm.fit)<span class="sc">$</span>coef</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>               Estimate Std. Error    z value    Pr(&gt;|z|)
(Intercept)  0.26686414 0.08592961  3.1056134 0.001898848
Lag1        -0.04126894 0.02641026 -1.5626099 0.118144368
Lag2         0.05844168 0.02686499  2.1753839 0.029601361
Lag3        -0.01606114 0.02666299 -0.6023760 0.546923890
Lag4        -0.02779021 0.02646332 -1.0501409 0.293653342
Lag5        -0.01447206 0.02638478 -0.5485006 0.583348244
Volume      -0.02274153 0.03689812 -0.6163330 0.537674762</code></pre>
</div>
</div>
<p>Among all the predictors, only <code>Lag2</code> is statistically significant.</p>
<ol start="3" type="a">
<li>Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>glm.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fit, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(glm.prob <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction
glm.pred Down  Up
    Down   54  48
    Up    430 557</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># training error rate:</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(glm.pred <span class="sc">!=</span> Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4389348</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>The 43.8% error rate is the&nbsp;training error. This is likely an optimistic estimate of how the model would perform on new, unseen data, as the model was evaluated on the same data it was trained on.</p>
</blockquote>
<ol start="4" type="a">
<li>Now fit the logistic regression model using a training data period from 1990 to 2008, with <code>Lag2</code> as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>train.data <span class="ot">&lt;-</span> Weekly[Year <span class="sc">&lt;</span> <span class="dv">2009</span>, ]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>test.data <span class="ot">&lt;-</span> Weekly[Year <span class="sc">&gt;</span> <span class="dv">2008</span>, ]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag2, <span class="at">family =</span> binomial,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> train.data)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm.fit)<span class="sc">$</span>coef</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Estimate Std. Error  z value   Pr(&gt;|z|)
(Intercept) 0.20325743 0.06428036 3.162046 0.00156665
Lag2        0.05809527 0.02870446 2.023911 0.04297934</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>glm.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fit, <span class="at">newdata =</span> test.data, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(glm.prob <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>        
glm.pred Down Up
    Down    9  5
    Up     34 56</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(glm.pred <span class="sc">==</span> test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.625</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test error rate</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(glm.pred <span class="sc">!=</span> test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.375</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>The model improved with <code>Lag2</code> as the only the predictor. Test error rate is 37.5% which is better than random guessing.</p>
</blockquote>
<ol start="5" type="a">
<li>Repeat (d) using LDA.</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># lda() is part of the MASS library</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>lda.fit <span class="ot">&lt;-</span> <span class="fu">lda</span>(Direction <span class="sc">~</span> Lag2, <span class="at">data =</span> train.data)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda.fit)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>lda.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda.fit, <span class="at">newdata =</span> test.data)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(lda.pred)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "class"     "posterior" "x"        </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(lda.pred<span class="sc">$</span>class, test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>      
       Down Up
  Down    9  5
  Up     34 56</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test error rate</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(lda.pred<span class="sc">$</span>class <span class="sc">!=</span> test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.375</code></pre>
</div>
</div>
<ol start="6" type="a">
<li>Repeat (d) using QDA.</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>qda.fit <span class="ot">&lt;-</span> <span class="fu">qda</span>(Direction <span class="sc">~</span> Lag2, <span class="at">data =</span> train.data)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>qda.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(qda.fit, <span class="at">newdata =</span> test.data)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(qda.pred<span class="sc">$</span>class, test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>      
       Down Up
  Down    0  0
  Up     43 61</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test error rate</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(qda.pred<span class="sc">$</span>class <span class="sc">!=</span> test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4134615</code></pre>
</div>
</div>
<ol start="7" type="a">
<li>Repeat (d) using KNN with <span class="math inline">\(K = 1\)</span>.</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> (Year <span class="sc">&lt;</span> <span class="dv">2009</span>) </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>train.Direction <span class="ot">&lt;-</span> Weekly<span class="sc">$</span>Direction[train] <span class="co"># vector for the train class labels</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>test.Direction <span class="ot">&lt;-</span> Weekly<span class="sc">$</span>Direction[<span class="sc">!</span>train] </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># knn() is part of the 'Class' library</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(Weekly[train, <span class="st">"Lag2"</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>                Weekly[<span class="sc">!</span>train, <span class="st">"Lag2"</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>], </span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>                train.Direction, <span class="at">k =</span> <span class="dv">1</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(knn.pred, test.Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Direction
knn.pred Down Up
    Down   21 30
    Up     22 31</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test error rate</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(knn.pred <span class="sc">!=</span> test.Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5</code></pre>
</div>
</div>
<ol start="8" type="a">
<li>Repeat (d) using naive Bayes.</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>nb.fit <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(Direction <span class="sc">~</span> Lag2, <span class="at">data =</span> train.data)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>nb.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.fit, test.data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(nb.pred, test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>       
nb.pred Down Up
   Down    0  0
   Up     43 61</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test error rate</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(nb.pred <span class="sc">!=</span> test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4134615</code></pre>
</div>
</div>
<ol type="i">
<li>Which of these methods appears to provide the best results on this data?</li>
</ol>
<blockquote class="blockquote">
<p>Logistic regression and linear discriminant analysis methods provide the best results on this data.</p>
</blockquote>
<ol start="10" type="a">
<li>Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for <span class="math inline">\(K\)</span> in the KNN classifier.</li>
</ol>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Logistic Regression</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4,</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> train.data,</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">family =</span> binomial)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>glm.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fit, <span class="at">newdata =</span> test.data, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(glm.prob <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>        
glm.pred Down Up
    Down    8  5
    Up     35 56</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Logistic error rate: "</span>, <span class="fu">mean</span>(glm.pred <span class="sc">!=</span> test.data<span class="sc">$</span>Direction))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Logistic error rate:  0.3846154</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Discreminant analysis</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>lda.fit <span class="ot">&lt;-</span> <span class="fu">lda</span>(Direction <span class="sc">~</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4, <span class="at">data =</span> train.data)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>lda.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda.fit, <span class="at">newdata =</span> test.data)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(lda.pred<span class="sc">$</span>class, test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>      
       Down Up
  Down    8  5
  Up     35 56</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"LDA error rate: "</span>, <span class="fu">mean</span>(lda.pred<span class="sc">$</span>class <span class="sc">!=</span> test.data<span class="sc">$</span>Direction))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>LDA error rate:  0.3846154</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Naiv Bayes</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>nb.fit <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(Direction <span class="sc">~</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4, <span class="at">data =</span> train.data)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>nb.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.fit, test.data)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(nb.pred, test.data<span class="sc">$</span>Direction)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>       
nb.pred Down Up
   Down    7 10
   Up     36 51</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Naive Bayes error rate: "</span>, <span class="fu">mean</span>(nb.pred <span class="sc">!=</span> test.data<span class="sc">$</span>Direction))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Naive Bayes error rate:  0.4423077</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># KNN</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate error rates for each k</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>error_rates <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="cf">function</span>(k) {</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>  train <span class="ot">&lt;-</span> (Year <span class="sc">&lt;</span> <span class="dv">2009</span>)</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>  knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(Weekly[train, <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>                  Weekly[<span class="sc">!</span>train, <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>], </span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>                  Weekly<span class="sc">$</span>Direction[train], <span class="at">k =</span> k)</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>  error_rate <span class="ot">&lt;-</span> <span class="fu">mean</span>(knn.pred <span class="sc">!=</span> test.Direction)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(error_rate)</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the k with the minimum error rate</span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>min_error_rate <span class="ot">&lt;-</span> <span class="fu">min</span>(error_rates)</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>best_k <span class="ot">&lt;-</span> <span class="fu">which.min</span>(error_rates)</span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(knn.pred, Weekly<span class="sc">$</span>Direction[<span class="sc">!</span>train])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>        
knn.pred Down Up
    Down   21 30
    Up     22 31</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"The best k is:"</span>, best_k, <span class="st">"with an error rate of:"</span>, min_error_rate, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The best k is: 26 with an error rate of: 0.3365385 </code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>After experimenting with several models and predictor combinations, the methods that provided the best results were&nbsp;logistic regression, LDA, and Naive Bayes using only <code>Lag2</code> as a predictor. These models all achieved a test error rate of <span class="math inline">\(37.5\%\)</span>. The more complex models, including those with more predictors or the optimized KNN classifier, did not outperform this simpler baseline.</p>
</blockquote>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 Abdoulie Jallow</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is build with <a href="https://github.com/jallow-code/jallow-code.github.io" target="_blank"><i class="fa-brands fa-github" title="GitHub octocat logo" aria-label="github"></i></a>, <a href="https://www.r-project.org/about.html" target="_blank"><i class="fa-brands fa-r-project" title="R Project" aria-label="r-project"></i></a> and <a href="https://quarto.org/" target="_blank">Quarto</a></p>
</div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>