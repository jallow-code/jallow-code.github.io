[
  {
    "objectID": "posts/jku-thesis-template/index.html",
    "href": "posts/jku-thesis-template/index.html",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "",
    "text": "Writing a Bachelor’s or Diploma thesis at JKU involves not only research but also formatting requirements. Managing citations, ensuring consistent layout, and integrating code or data analysis can quickly become overwhelming, especially using traditional tools like Microsoft Word.\nTo help streamline this process, I’ve adapted a Quarto thesis template specifically tailored to meet JKU’s formal criteria for diploma and bachelor theses. This template lets you focus purely on your research and writing, while Quarto handles the complexities of formatting, citation management, and document structure.\nThis template builds upon the excellent Masters/Doctoral Thesis, LaTeX Template, Version 2.5, which was first adapted for Quarto by Eli Holmes.\nWhy use Quarto for your thesis?\n\nReproducibility: Seamlessly integrate R or Python code and data analysis directly into your document.\nConsistency: Ensure uniform formatting for headings, figures, tables, and citations throughout.\nVersion Control: Works perfectly with tools like Git, making collaboration and tracking changes easy.\nFocus: Spend less time wrestling with layout and more time on your content."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#introduction",
    "href": "posts/jku-thesis-template/index.html#introduction",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "",
    "text": "Writing a Bachelor’s or Diploma thesis at JKU involves not only research but also formatting requirements. Managing citations, ensuring consistent layout, and integrating code or data analysis can quickly become overwhelming, especially using traditional tools like Microsoft Word.\nTo help streamline this process, I’ve adapted a Quarto thesis template specifically tailored to meet JKU’s formal criteria for diploma and bachelor theses. This template lets you focus purely on your research and writing, while Quarto handles the complexities of formatting, citation management, and document structure.\nThis template builds upon the excellent Masters/Doctoral Thesis, LaTeX Template, Version 2.5, which was first adapted for Quarto by Eli Holmes.\nWhy use Quarto for your thesis?\n\nReproducibility: Seamlessly integrate R or Python code and data analysis directly into your document.\nConsistency: Ensure uniform formatting for headings, figures, tables, and citations throughout.\nVersion Control: Works perfectly with tools like Git, making collaboration and tracking changes easy.\nFocus: Spend less time wrestling with layout and more time on your content."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#getting-started",
    "href": "posts/jku-thesis-template/index.html#getting-started",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Getting Started",
    "text": "Getting Started\nReady to use the template? You’ll need Quarto installed first. Then, follow these steps:\n\nOpen your Terminal or Command Prompt. Navigate to the directory where you want to create your thesis project folder.\nUse the Quarto Template: Run the following command. Quarto will download the template and ask you to provide a name for your new thesis directory.\n\n\nquarto use template jallow-code/jku-quarto-thesis\n\n\nEnter Your Project Directory: Once the download is complete, change into the newly created directory:\n\n\ncd &lt;your-new-thesis-directory-name&gt;\n\n\nRender the Example: To generate the sample PDF document and check if everything is working, run:\n\n\nquarto render\n\nThis will create a PDF file based on the example content included in the template."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#troubleshooting-common-rendering-errors",
    "href": "posts/jku-thesis-template/index.html#troubleshooting-common-rendering-errors",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Troubleshooting Common Rendering Errors",
    "text": "Troubleshooting Common Rendering Errors\nIf you encounter an error message like this during rendering:\nERROR:\ncompilation failed - missing packages (automatic install disabled)\nLaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H &lt;return&gt;  for immediate help.\n ...\nl.872 \\end{CSLReferences}\nThis usually indicates issues with your LaTeX distribution or Quarto version. Try these steps:\n\nUpdate TinyTeX: Ensure your LaTeX distribution is up-to-date. If using TinyTeX (common with RStudio/Quarto), update it via your R console (tinytex::tlmgr_update()) or follow instructions on the TinyTeX website.\nUpdate Quarto: Make sure you have the latest version of Quarto installed from the official Quarto website."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#using-the-template-with-an-existing-quarto-project",
    "href": "posts/jku-thesis-template/index.html#using-the-template-with-an-existing-quarto-project",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Using the Template with an Existing Quarto Project",
    "text": "Using the Template with an Existing Quarto Project\nIf you’ve already started writing your thesis in a Quarto document (.qmd) and want to apply this template’s formatting and structure:\n\nEnsure you are inside your existing project directory in the terminal.\nRun the following command to add the template’s resources (like LaTeX settings and front matter files) to your project:\n\n\nquarto install extension jallow-code/jku-quarto-thesis\n\n\nYou will likely need to manually copy or adjust settings from the template’s _quarto.yml file into your own project’s _quarto.yml file."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#customizing-your-thesis",
    "href": "posts/jku-thesis-template/index.html#customizing-your-thesis",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Customizing Your Thesis",
    "text": "Customizing Your Thesis\nThe template is designed to be easily customized through the _quarto.yml configuration file located in the main directory."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#thesis-details-_quarto.yml",
    "href": "posts/jku-thesis-template/index.html#thesis-details-_quarto.yml",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Thesis Details (_quarto.yml)",
    "text": "Thesis Details (_quarto.yml)\nOpen _quarto.yml in a text editor. You’ll find a thesis: block where you can set crucial information:\n\nthesis:\n  type: Bachelor Thesis # Change to \"Master Thesis\", \"Diploma Thesis\", etc.\n  degree-name: Bachelor of Science # e.g., Master of Science, Diplom-Ingenieur\n  group: Biological Chemistry # Your specific JKU study program\n  supervisor:\n    primary: \"Assoz. Univ.-Prof. Dr. Ian Teasdale\" # Primary supervisor's full title and name\n    co: \"Dipl.-Ing. Michael Kneidinger\" # Co-supervisor (optional, remove if not needed)\n  university: Johannes Kepler Universität Linz\n  department: Institute of Polymer Chemistry # Your institute\n  faculty: Faculty of Engineering and Natural Sciences # Your faculty (TNF, SOWI, MED)"
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#front-matter-customization",
    "href": "posts/jku-thesis-template/index.html#front-matter-customization",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Front Matter Customization",
    "text": "Front Matter Customization\nThe _quarto.yml file also links to LaTeX files (.tex) located in the Frontmatter/ sub-directory for sections like the Abstract, Acknowledgements, and Dedication.\n\nTo modify content: Edit the corresponding .tex file (e.g., Frontmatter/acknowledgements.tex).\nTo exclude a section: Comment out the line in _quarto.yml by adding a # at the beginning. For example, to remove the dedication:\n\n\n# dedication: \"Frontmatter/dedication.tex\""
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#adding-lists-of-symbols-or-abbreviations",
    "href": "posts/jku-thesis-template/index.html#adding-lists-of-symbols-or-abbreviations",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Adding Lists of Symbols or Abbreviations",
    "text": "Adding Lists of Symbols or Abbreviations\nThe Frontmatter/ directory contains templates for abbreviations (abbreviations.tex) and symbols (symbols.tex).\n\nEdit these files directly using standard LaTeX syntax.\nIf you’re unsure about specific LaTeX commands (e.g., for mathematical symbols), searching online for “LaTeX [your need]” (e.g., “LaTeX degree symbol”, “LaTeX align equations”) is usually very helpful."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#citation-styling",
    "href": "posts/jku-thesis-template/index.html#citation-styling",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Citation Styling",
    "text": "Citation Styling\nThe template defaults to the nature-chemistry.csl citation style. To use a different style (e.g., APA, IEEE, or another chemistry journal):\n\nFind and download the desired .csl file from the Zotero Style Repository.\nPlace the downloaded .csl file in the same directory as your _quarto.yml file.\nUpdate the _quarto.yml file to point to your new style:\n\nbibliography: [references.bib] # Make sure this points to your bibliography file\ncsl: your-preferred-style.csl # Update this line"
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#additional-resources-final-tips",
    "href": "posts/jku-thesis-template/index.html#additional-resources-final-tips",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Additional Resources & Final Tips",
    "text": "Additional Resources & Final Tips\n\nThe best way to understand the structure is to look at the included example .qmd files and the rendered thesis_template.pdf (linked at the bottom of this post or generated when you run quarto render).\nCheck out Eli Holmes’s Youtube channel for practical data science and Quarto tutorials.\nAlways double-check the latest official JKU guidelines for thesis formatting to ensure this template still meets all requirements."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#additional-resources",
    "href": "posts/jku-thesis-template/index.html#additional-resources",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Additional Resources",
    "text": "Additional Resources\nI hope this template proves useful for your thesis writing journey at JKU! Feel free to raise issues or suggest improvements on the GitHub repository."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#sample-document",
    "href": "posts/jku-thesis-template/index.html#sample-document",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Sample Document",
    "text": "Sample Document\n\n\n\n\n  \n     Download thesis sample"
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html",
    "href": "posts/2024-11-14-statistical-learning/index.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\nThe logistic function is define as\n\\[\np(X) = \\frac{e^{\\beta_o + \\beta_1X}}{1\\;+ e^{\\beta_o + \\beta_1X}}\\\\\n\\tag{1}\\]\nRearranging terms, \\[p(X) = \\frac{e^{(β₀ + β₁X)}} {1 + e^{(β₀ + β₁X)}}\\] Let \\(L = e^{(β₀ + β₁X)}\\). Then \\(p(X) = \\frac{L}{1 + L}\\).\n\\[\\begin{gather*}\n\np(X) \\cdot{} (1 + L) = L\\\\\np(X) + p(X) \\cdot{} L = L\\\\\np(X) = L - p(X) \\cdot{} L\\\\\np(X) = L \\cdot{} (1 - p(X))\\\\\n\\frac{p(X)}{(1 - p(X)} = L\\\\\n\n\\end{gather*}\\]\nSubstituting \\(L\\) back gives the logit representation:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_o + \\beta_1X}\n\\tag{2}\\]\n\nIt was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(µk,σ2) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n\n\\[\np_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2)}\n              {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2)}\n\\tag{3}\\]\nThe discriminant function is\n\\[\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma_2} + \\log(\\pi_k)\n\\tag{4}\\]\nTo show Equation 3 is equals to Equation 4, we first assume \\(\\sigma_1^2\\;=\\;...\\;=\\sigma_k^2\\). Hence,\n\\[\np_k(x) = \\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}\n              {\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)}\n\\]\nNext, we take the \\(\\log(p_K(X))\\) to linearized the function. \\[\n\\log(p_k(x)) = \\log(\\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}{\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)})\n\\] Rearranging terms\n\\[\n\\log(p_k(x)) = \\log(\\pi_k) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 -\n              \\log\\left(\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)\\right)\n\\]\nTo maximize over \\(k\\), any term that’s independent of \\(k\\) is ignored\n\\[\\begin{align}\nf &= \\log(\\pi_k) - \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\\\\n  &= \\log(\\pi_k) - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\\\\n\\end{align}\\]\nSince \\(\\frac{x^2}{2\\sigma^2}\\) is also independent of \\(k\\), it is ignored in order to maximize over \\(k\\)\n\\[\n\\log(\\pi_k) + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}\n\\]\n\nThis problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature.\n\nProve that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\nTo classify an observation, the Bayes classifier assigns it to the class \\(k\\) for which the posterior probability \\(p_k(x)\\) is largest. This is equivalent to assigning the observation to the class that maximizes the discriminant function, \\(δ_k(x)\\). For a one-dimensional QDA model, we do not assume the variances are equal across classes.\nThe discriminant function for class \\(k\\) is derived from the posterior probability: \\[δ_k(x) = log(p_k(x)) = log(π_k \\cdot{} f_k(x)) - log(C)\\] where \\(f_k(x)\\) is the normal density \\(N(μ_k, σ_k^2)\\) and \\(C\\) is the denominator \\(Σ P(Y=l)f_l(x)\\), which doesn’t depend on \\(k\\). To maximize over \\(k\\), we can ignore \\(C\\).\nLet’s expand the function: \\[δ_k(x) ∝ log(π_k) + log(f_k(x))\\] \\[δ_k(x) = log(π_k) - log(√(2πσ_k^2)) - (\\frac{1}{2σ_k^2}) \\cdot{} (x - μ_k)^2\\]\nExpanding the squared term: \\[δ_k(x) = log(π_k) - (\\frac{1}{2})\\;log\\;(2πσ_k^2) - (\\frac{1}{2σ_k^2}) \\cdot{} (x^2 - 2xμ_k + μ_k^2)\\]\nRearranging the terms to see the structure as a function of \\(x\\): \\[δ_k(x) = - (\\frac{1}{2σ_k^2})x^2 + (\\frac{μ_k}{σ_k^2})x + [log(π_k) - (\\frac{1}{2})log(2πσ_k^2) - (\\frac{μ_k^2}{2σ_k^2})]\\]\nThis discriminant function \\(δ_k(x)\\) is a quadratic function of \\(x\\). The coefficient of the \\(x^2\\) term is \\(\\frac{1}{2σ_k^2}\\). Since QDA assumes that \\(σ_k^2\\) is different for each class \\(k\\), this quadratic term does not cancel out when comparing the discriminant functions of two different classes (i.e., when finding the decision boundary where \\(δ_k(x) = δ_j(x))\\).\nTherefore, the resulting decision boundary is quadratic, and the Bayes classifier is not linear.\n\nWhen the number of features \\(p\\) is large, there tends to be a deterioration in the performance of KNN and other local approaches… This phenomenon is known as the curse of dimensionality… We will now investigate this curse.\n\n\n\nSuppose that we have a set of observations, each with measurements on \\(p = 1\\) feature, \\(X\\)… On average, what fraction of the available observations will we use to make the prediction?\n\nSince \\(X\\) is uniformly distributed on \\([0,1]\\), its range is \\(1\\). The prediction for a test observation \\(X=0.6\\) uses training observations in the range \\([0.55, 0.65]\\). The length of this interval is \\(0.65 - 0.55 = 0.1\\). Because the data is uniform, the fraction of observations that fall into an interval is equal to the length of that interval. Therefore, on average, we will use \\(10\\%\\) of the available observations.\n\nNow suppose that we have… \\(p = 2\\) features… On average, what fraction of the available observations will we use?\n\nWe are now in two dimensions, and we form a neighborhood by taking a \\(10\\%\\) range for each feature. This creates a square (a 2-D hypercube) centered on the test point. The side length for this square is 0.1 for each dimension. Since the features are uniformly distributed on [0,1] x [0,1], the fraction of observations we use is the area of this square, which is \\(0.1 \\cdot{} 0.1 = 0.01\\). So, on average, we will use only \\(1%\\) of the observations.\n\nNow suppose that we have… \\(p = 100\\) features… What fraction of the available observations will we use?\n\nFollowing the pattern, for \\(p=100\\) dimensions, our neighborhood is a 100-dimensional hypercube where each side has length \\(0.1\\). The volume of this hypercube is \\(0.1^{100}\\). This is an infinitesimally small number, meaning we expect to use virtually zero fraction of the available observations.\n\nUsing your answers… argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.\n\nThe answers to parts (a)-(c) show that the fraction of training data available in a local neighborhood of a fixed size (\\(10\\%\\) of the range of each feature) decreases exponentially as the number of dimensions p increases \\((0.1^p)\\). For a large \\(p\\), this fraction is so small that the neighborhood is effectively empty. This means there are no “nearby” neighbors to average for a prediction, which undermines the entire principle of KNN and other local methods. This is the curse of dimensionality.\n\nNow suppose that we wish to make a prediction… by creating a p-dimensional hypercube… that contains, on average, \\(10\\%\\) of the training observations. For \\(p\\) = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer.\n\nLet \\(l\\) be the side length of the p-dimensional hypercube. The volume is \\(l^p\\). We want this volume to be \\(0.1\\) (to capture \\(10\\%\\) of the uniform data). So, we solve \\(l^p = 0.1\\) for \\(l\\), which gives \\(l = (0.1)^{(1/p)}\\).\n\nFor \\(p = 1: l = (0.1)^{(1/1)} = 0.1\\)\nFor \\(p = 2: l = (0.1)^{(1/2)} ≈ 0.316\\)\nFor \\(p = 100: l = (0.1)^{(1/100)} ≈ 0.977\\)\n\n\n\n\n\n\n\nNote\n\n\n\nAs the dimension \\(p\\) increases, the side length \\(l\\) of the hypercube needed to capture just \\(10\\%\\) of the data rapidly approaches \\(1\\). For \\(p=100\\), each side of the “local” neighborhood must span over \\(97\\%\\) of the total range of its corresponding feature. This means the points within this hypercube are no longer truly “local” or “near” the test observation. The method loses its local character, and the predictions are based on points that are far away, which leads to poor performance.\n\n\n\nWe now examine the differences between LDA and QDA.\n\n\n\nIf the bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nQDA been the more flexible model will perform better on the training set, but worse on the test set. On the test set, QDA will overfit the data because the true decision boundary is linear.\n\n\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nIn this situation since the decision boundary is non-linear, QDA will perform better in both data sets. A linear model will underfit in this case.\n\n\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n\n\nAs sample size increase, QDA will improve because there is more data to fit and the low bias will offset increase in variance.\n\n\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n\n\nFalse. In this case, QDA will overfit the data.\n\n\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat\\beta_0 = -6\\), \\(\\hat\\beta_1 = 0.05\\), \\(\\hat\\beta_2 = 1\\).\n\n(a). Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.\n\\[\np(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}}\n\\]\nwhen \\(X_1 = 40\\) and \\(X_2 = 3.5\\), \\(p(X) = 0.38\\)\n\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n\n\\[\\begin{gather}\n\n\\log\\left(\\frac{p(X)}{1-p(x)}\\right) = -6 + 0.05X_1 + X_2 \\\\\n\n\\log\\left(\\frac{0.5}{1-0.5}\\right) = -6 + 0.05X_1 + 3.5 \\\\\n\n\\end{gather}\\]\nTherefore, solving the equation \\(0 = −6 + 0.05X_1 + 3.5\\), \\(X_1 = 50\\) hours.\n\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of \\(X\\) for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year.\n\nHint: Recall that the density function for a normal random variable is \\(f(x) =\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\\). You will need to use Bayes’ theorem.\nValue of companies issuing a dividend (D) = \\(D \\sim \\mathcal{N}(10, 36)\\).\nValue \\(v\\) for companies not issuing a dividend \\((D^c)\\) = \\(D^c \\sim \\mathcal{N}(0, 36)\\) and \\(p(D) = 0.8\\).\nBayes theorem:\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} P(X|D)}{P(D) \\cdot{} P(X|D) + P(D^c) \\cdot{} P(X|D^c)} \\\\\n\\end{align}\\]\nSubstitute the Gaussian likelihoods into Bayes theorem\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nFactor out \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) from the numerator and denominator, which cancels out:\n\\[\\begin{align}\nP(D|X)  &= \\frac{P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nSubstitute the given probabilities and means:\n\\[\\begin{align}\nP(D|X)  &= \\frac{0.8 \\times e^{-(4-10)^2/(2 \\times 36)}}\n               {0.8 \\times e^{-(4-10)^2/(2 \\times 36)} + 0.2 \\times e^{-(4-0)^2/(2 \\times 36)}} \\\\\n       &= \\frac{0.8 \\cdot e^{-1/2}}{0.8 \\cdot e^{-1/2} + 0.2 \\cdot e^{-2/9}} \\\\\n\\end{align}\\]\n\nexp(-0.5) * 0.8 / (exp(-0.5) * 0.8 + exp(-2/9) * 0.2)\n\n[1] 0.7518525\n\n\n\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\n\n\nLogistic regression will perform better because it has a lower test error rate. For \\(K = 1\\), the training error rate is always zero because the closest point is always the training point itself, so the model will never make a mistake on the training set. Given that the average error rate for 1-NN is 18%, this implies a test error rate of 36%. Logistic regression, with a test error rate of 30%, is therefore the better choice.\n\n\nThis problem has to do with odds.\n\n\n\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\n\\[\n\\frac{p(x)}{1 - P(x)} = odd\\\\\n\\] \\[\\begin{equation}\np(x) = \\frac{odd}{1 + odd}\\\\\n\\end{equation}\\]\n\\[\\begin{equation}\np(x)  = \\frac{0.37}{1 + 0.37} = 0.27\n\\end{equation}\\]\n\nSuppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?\n\n\\[\\frac{0.16}{1 - 0.16}  = 0.19\\]\n\nIn the setting with p = 1, (4.32) takes a simpler form…repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(πk\\), \\(πK\\), \\(μk\\), \\(μK\\), and \\(σ2\\).\n\nThe goal is to find the log-odds between class \\(k\\) and a baseline class \\(K\\) for an LDA model with \\(p=1\\). This is given by \\(log(\\frac{P(Y=k|X=x)}{P(Y=K|X=x)})\\). This is equivalent to \\(δ_k(x) - δ_K(x)\\).\nThe discriminant function for LDA is: \\[δ_k(x) = x \\cdot{} (\\frac{μ_k}{σ^2}) - (\\frac{μ_k^2}{2σ^2}) + log(π_k)\\]\nThe log-odds are: \\[δ_k(x) - δ_K(x) = [x \\cdot{} (\\frac{μ_k}{σ^2}) - (\\frac{μ_k^2}{2σ^2}) + log(π_k)] - [x \\cdot{} (\\frac{μ_K}{σ^2}) - (\\frac{μ_K^2}{2σ^2}) + log(π_K)]\\]\nNow, group the terms that are constant and those that are coefficients of \\(x\\): \\[= x \\cdot{} (\\frac{μ_k}{σ^2} - \\frac{μ_K}{σ^2}) + [log(π_k) - log(π_K) - \\frac{μ_k^2}{2σ^2} + \\frac{μ_K^2}{2σ^2}]\\]\nThis has the form \\(a_k + b_{k1} \\cdot{} x\\). The coefficients are: \\[a_k = log(\\frac{π_k}{π_K}) - \\frac{μ_k^2 - μ_K^2}{2σ^2}\\] \\[b_k1 = \\frac{μ_k - μ_K}{σ^2}\\] 11. Work out the detailed forms of \\(a_k\\), \\(b_{kj}\\), and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(πk\\), \\(μk\\), \\(Σk\\). (Assuming (4.33) refers to the general quadratic form of the QDA discriminant function).\nThe QDA discriminant function is: \\[δ_k(x) = -\\frac{1}{2} \\cdot{} log(det(Σ_k)) -\\frac{1}{2} \\cdot{} (x-μ_k)^T \\cdot{} Σ_k^{-1} \\cdot{} (x-μ_k) + log(π_k)\\] Expanding the quadratic term \\((x-μ_k)^T \\cdot{} Σ_k^{-1} \\cdot{} (x-μ_k)\\): \\[= x^TΣ_k^{-1}x - x^TΣ_k^{-1}μ_k - μ_k^TΣ_k^{-1}x + μ_k^TΣ_k^{-1}μ_k\\] Since \\(x^TΣ_k^{-1}μ_k\\) is a scalar, it’s equal to its transpose \\[μ_k^TΣ_k^{-1T}x = μ_k^TΣ_k^{-1}x\\].\nSo the expression is \\[x^TΣ_k^{-1}x - 2x^TΣ_k^{A-1}μ_k + μ_k^TΣ_k^{-1}μ_k\\].\nSubstituting this back into \\(δ_k(x)\\): \\[δ_k(x) = -\\frac{1}{2} \\cdot{} x^TΣ_k^{-1}x + x^TΣ_k^{-1}μ_k - \\frac{1}{2} \\cdot{} μ_k^TΣ_k^{-1}μ_k - \\frac{1}{2} \\cdot{} log(det(Σ_k)) + log(π_k)\\] This is a quadratic function of \\(x\\). We can identify the constant, linear, and quadratic parts.\nThe constant term is \\(a_k\\): \\[a_k = log(π_k) - \\frac{1}{2} \\cdot{} log(det(Σ_k)) - \\frac{1}{2} \\cdot{} μ_k^TΣ_k^{-1}μ_k\\] The linear part is \\(x^T(Σ_k^{-1}μ_k)\\). The vector of coefficients for the linear terms is \\(b_k = Σ_k^{-1}μ_k\\). The coefficient for the j-th variable, \\(x_j\\), is: \\(b_{kj} = (Σ_k^{-1}μ_k)_j\\)\nThe quadratic part is \\(-\\frac{1}{2} \\cdot{} x^TΣ_k^{-1}x\\). The matrix of coefficients for the quadratic terms \\(x_j \\cdot{} x_l\\) is \\(-\\frac{1}{2} \\cdot{} Σ_k^{-1}\\). The coefficient \\(b_{kjl}\\) is: \\[b_{kjl} = (\\frac{1}{2} \\cdot Σ_k^{-1})_{jl}\\]\n\nSuppose that you wish to classify an observation \\(X \\in \\mathbb{R}\\) into apples and oranges. You fit a logistic regression model and find that\n\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\beta_0 + \\hat\\beta_1x)}{1 + \\exp(\\hat\\beta_0 + \\hat\\beta_1x)}\n\\]\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x)}\n{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x) + \\exp(\\hat\\alpha_{apple0} + \\hat\\alpha_{apple1}x)}\n\\]\n\nWhat is the log odds of orange versus apple in your model?\n\n\nThe log odds (\\(\\frac{p(x)}{1 - P(x)}\\)) in my model will be \\(\\hat\\beta_0 + \\hat\\beta_1x\\)\n\n\nWhat is the log odds of orange versus apple in your friend’s model?\n\nLog odds of our friend model:\n\\[\n(\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x\n\\]\n\nSuppose that in your model, \\(\\hat\\beta_0 = 2\\) and \\(\\hat\\beta = −1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\nThe coefficient estimate in my friend’s model:\n\n\\(\\hat\\alpha_{orange0} -\\hat\\alpha_{apple0} = 2\\)\n\\(\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1} = -1\\).\n\n\nNow suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat\\alpha_{orange0} = 1.2\\), \\(\\hat\\alpha_{orange1} = −2\\), \\(\\hat\\alpha_{apple0} = 3\\), \\(\\hat\\alpha_{apple1} = 0.6\\). What are the coefficient estimates in your model?\n\nThe coefficients in my model would be \\(\\hat\\beta_0 = 1.2 - 3 = -1.8\\) and \\(\\hat\\beta_1 = -2 - 0.6 = -2.6\\)\n\nFinally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\n\nThey should agree everytime (i.e, pridictions and log odd between any pair of classes will remain the same, regardless of coding). The cofficient will be different because of the choice of baseline.\n\n\n\n\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n\n\n\nProduce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n\n\nlibrary(ISLR2) # data sets\nlibrary(tidyverse)\nlibrary(psych) # for correlation plots\nlibrary(MASS) # for lda() and qda()\nlibrary(class) # for knn()\nlibrary(e1071) # for naiveBayes()\ndata(\"Weekly\")\nattach(Weekly)\n\n\nstr(Weekly)\n\n'data.frame':   1089 obs. of  9 variables:\n $ Year     : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...\n $ Lag1     : num  0.816 -0.27 -2.576 3.514 0.712 ...\n $ Lag2     : num  1.572 0.816 -0.27 -2.576 3.514 ...\n $ Lag3     : num  -3.936 1.572 0.816 -0.27 -2.576 ...\n $ Lag4     : num  -0.229 -3.936 1.572 0.816 -0.27 ...\n $ Lag5     : num  -3.484 -0.229 -3.936 1.572 0.816 ...\n $ Volume   : num  0.155 0.149 0.16 0.162 0.154 ...\n $ Today    : num  -0.27 -2.576 3.514 0.712 1.178 ...\n $ Direction: Factor w/ 2 levels \"Down\",\"Up\": 1 1 2 2 2 1 2 2 2 1 ...\n\nWeekly$Year &lt;- as.factor(Weekly$Year)\n\npairs.panels(Weekly[1:8], cex.labels = 1, ellipses = FALSE)\n\n\n\n\n\n\n\n# total number of times the market had a positive or negative return\ntable(Weekly$Direction)\n\n\nDown   Up \n 484  605 \n\n\n\nThere is a strong positive correlation between the volume of shares traded and the year. From 2005 to 2010, the volume of shares began increasing exponentially. Additionally, between 1990 and 2010, the market had a total of 605 positive returns and 484 negative returns, respectively.\n\n\nUse the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\n\nglm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n               family = binomial)\n\nsummary(glm.fit)$coef\n\n               Estimate Std. Error    z value    Pr(&gt;|z|)\n(Intercept)  0.26686414 0.08592961  3.1056134 0.001898848\nLag1        -0.04126894 0.02641026 -1.5626099 0.118144368\nLag2         0.05844168 0.02686499  2.1753839 0.029601361\nLag3        -0.01606114 0.02666299 -0.6023760 0.546923890\nLag4        -0.02779021 0.02646332 -1.0501409 0.293653342\nLag5        -0.01447206 0.02638478 -0.5485006 0.583348244\nVolume      -0.02274153 0.03689812 -0.6163330 0.537674762\n\n\nAmong all the predictors, only Lag2 is statistically significant.\n\nCompute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n\n\nglm.prob &lt;- predict(glm.fit, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down   54  48\n    Up    430 557\n\n# training error rate:\nmean(glm.pred != Direction)\n\n[1] 0.4389348\n\n\n\nThe 43.8% error rate is the training error. This is likely an optimistic estimate of how the model would perform on new, unseen data, as the model was evaluated on the same data it was trained on.\n\n\nNow fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n\n\ntrain.data &lt;- Weekly[Year &lt; 2009, ]\ntest.data &lt;- Weekly[Year &gt; 2008, ]\n\nglm.fit &lt;- glm(Direction ~ Lag2, family = binomial,\n               data = train.data)\n\nsummary(glm.fit)$coef\n\n              Estimate Std. Error  z value   Pr(&gt;|z|)\n(Intercept) 0.20325743 0.06428036 3.162046 0.00156665\nLag2        0.05809527 0.02870446 2.023911 0.04297934\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    9  5\n    Up     34 56\n\nmean(glm.pred == test.data$Direction)\n\n[1] 0.625\n\n# Test error rate\nmean(glm.pred != test.data$Direction)\n\n[1] 0.375\n\n\n\nThe model improved with Lag2 as the only the predictor. Test error rate is 37.5% which is better than random guessing.\n\n\nRepeat (d) using LDA.\n\n\n# lda() is part of the MASS library\nlda.fit &lt;- lda(Direction ~ Lag2, data = train.data)\n\nplot(lda.fit)\n\n\n\n\n\n\n\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    9  5\n  Up     34 56\n\n# test error rate\nmean(lda.pred$class != test.data$Direction)\n\n[1] 0.375\n\n\n\nRepeat (d) using QDA.\n\n\nqda.fit &lt;- qda(Direction ~ Lag2, data = train.data)\n\nqda.pred &lt;- predict(qda.fit, newdata = test.data)\n\ntable(qda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    0  0\n  Up     43 61\n\n# test error rate\nmean(qda.pred$class != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nRepeat (d) using KNN with \\(K = 1\\).\n\n\ntrain &lt;- (Year &lt; 2009) \n\ntrain.Direction &lt;- Weekly$Direction[train] # vector for the train class labels\ntest.Direction &lt;- Weekly$Direction[!train] \n\n# knn() is part of the 'Class' library\nset.seed(1)\nknn.pred &lt;- knn(Weekly[train, \"Lag2\", drop = FALSE],\n                Weekly[!train, \"Lag2\", drop = FALSE], \n                train.Direction, k = 1)\n\n\ntable(knn.pred, test.Direction)\n\n        test.Direction\nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# test error rate\nmean(knn.pred != test.Direction)\n\n[1] 0.5\n\n\n\nRepeat (d) using naive Bayes.\n\n\nnb.fit &lt;- naiveBayes(Direction ~ Lag2, data = train.data)\n\nnb.pred &lt;- predict(nb.fit, test.data)\n\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    0  0\n   Up     43 61\n\n# test error rate\nmean(nb.pred != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nWhich of these methods appears to provide the best results on this data?\n\n\nLogistic regression and linear discriminant analysis methods provide the best results on this data.\n\n\nExperiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for \\(K\\) in the KNN classifier.\n\n\n# Logistic Regression\nglm.fit &lt;- glm(Direction ~ Lag2 + Lag3 + Lag4,\n               data = train.data,\n               family = binomial)\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    8  5\n    Up     35 56\n\ncat(\"Logistic error rate: \", mean(glm.pred != test.data$Direction))\n\nLogistic error rate:  0.3846154\n\n# Linear Discreminant analysis\nlda.fit &lt;- lda(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    8  5\n  Up     35 56\n\ncat(\"LDA error rate: \", mean(lda.pred$class != test.data$Direction))\n\nLDA error rate:  0.3846154\n\n# Naiv Bayes\nnb.fit &lt;- naiveBayes(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nnb.pred &lt;- predict(nb.fit, test.data)\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    7 10\n   Up     36 51\n\ncat(\"Naive Bayes error rate: \", mean(nb.pred != test.data$Direction))\n\nNaive Bayes error rate:  0.4423077\n\n# KNN\n\n# Calculate error rates for each k\nerror_rates &lt;- sapply(1:100, function(k) {\n  train &lt;- (Year &lt; 2009)\n  set.seed(1)\n  \n  knn.pred &lt;- knn(Weekly[train, 2:4, drop = FALSE],\n                  Weekly[!train, 2:4, drop = FALSE], \n                  Weekly$Direction[train], k = k)\n  \n  error_rate &lt;- mean(knn.pred != test.Direction)\n  return(error_rate)\n})\n\n# Find the k with the minimum error rate\nmin_error_rate &lt;- min(error_rates)\nbest_k &lt;- which.min(error_rates)\ntable(knn.pred, Weekly$Direction[!train])\n\n        \nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# Print the result\ncat(\"The best k is:\", best_k, \"with an error rate of:\", min_error_rate, \"\\n\")\n\nThe best k is: 26 with an error rate of: 0.3365385 \n\n\n\nAfter experimenting with several models and predictor combinations, the methods that provided the best results were logistic regression, LDA, and Naive Bayes using only Lag2 as a predictor. These models all achieved a test error rate of \\(37.5\\%\\). The more complex models, including those with more predictors or the optimized KNN classifier, did not outperform this simpler baseline."
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html#exercises",
    "href": "posts/2024-11-14-statistical-learning/index.html#exercises",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\nThe logistic function is define as\n\\[\np(X) = \\frac{e^{\\beta_o + \\beta_1X}}{1\\;+ e^{\\beta_o + \\beta_1X}}\\\\\n\\tag{1}\\]\nRearranging terms, \\[p(X) = \\frac{e^{(β₀ + β₁X)}} {1 + e^{(β₀ + β₁X)}}\\] Let \\(L = e^{(β₀ + β₁X)}\\). Then \\(p(X) = \\frac{L}{1 + L}\\).\n\\[\\begin{gather*}\n\np(X) \\cdot{} (1 + L) = L\\\\\np(X) + p(X) \\cdot{} L = L\\\\\np(X) = L - p(X) \\cdot{} L\\\\\np(X) = L \\cdot{} (1 - p(X))\\\\\n\\frac{p(X)}{(1 - p(X)} = L\\\\\n\n\\end{gather*}\\]\nSubstituting \\(L\\) back gives the logit representation:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_o + \\beta_1X}\n\\tag{2}\\]\n\nIt was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(µk,σ2) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n\n\\[\np_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2)}\n              {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2)}\n\\tag{3}\\]\nThe discriminant function is\n\\[\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma_2} + \\log(\\pi_k)\n\\tag{4}\\]\nTo show Equation 3 is equals to Equation 4, we first assume \\(\\sigma_1^2\\;=\\;...\\;=\\sigma_k^2\\). Hence,\n\\[\np_k(x) = \\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}\n              {\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)}\n\\]\nNext, we take the \\(\\log(p_K(X))\\) to linearized the function. \\[\n\\log(p_k(x)) = \\log(\\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}{\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)})\n\\] Rearranging terms\n\\[\n\\log(p_k(x)) = \\log(\\pi_k) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 -\n              \\log\\left(\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)\\right)\n\\]\nTo maximize over \\(k\\), any term that’s independent of \\(k\\) is ignored\n\\[\\begin{align}\nf &= \\log(\\pi_k) - \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\\\\n  &= \\log(\\pi_k) - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\\\\n\\end{align}\\]\nSince \\(\\frac{x^2}{2\\sigma^2}\\) is also independent of \\(k\\), it is ignored in order to maximize over \\(k\\)\n\\[\n\\log(\\pi_k) + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}\n\\]\n\nThis problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature.\n\nProve that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\nTo classify an observation, the Bayes classifier assigns it to the class \\(k\\) for which the posterior probability \\(p_k(x)\\) is largest. This is equivalent to assigning the observation to the class that maximizes the discriminant function, \\(δ_k(x)\\). For a one-dimensional QDA model, we do not assume the variances are equal across classes.\nThe discriminant function for class \\(k\\) is derived from the posterior probability: \\[δ_k(x) = log(p_k(x)) = log(π_k \\cdot{} f_k(x)) - log(C)\\] where \\(f_k(x)\\) is the normal density \\(N(μ_k, σ_k^2)\\) and \\(C\\) is the denominator \\(Σ P(Y=l)f_l(x)\\), which doesn’t depend on \\(k\\). To maximize over \\(k\\), we can ignore \\(C\\).\nLet’s expand the function: \\[δ_k(x) ∝ log(π_k) + log(f_k(x))\\] \\[δ_k(x) = log(π_k) - log(√(2πσ_k^2)) - (\\frac{1}{2σ_k^2}) \\cdot{} (x - μ_k)^2\\]\nExpanding the squared term: \\[δ_k(x) = log(π_k) - (\\frac{1}{2})\\;log\\;(2πσ_k^2) - (\\frac{1}{2σ_k^2}) \\cdot{} (x^2 - 2xμ_k + μ_k^2)\\]\nRearranging the terms to see the structure as a function of \\(x\\): \\[δ_k(x) = - (\\frac{1}{2σ_k^2})x^2 + (\\frac{μ_k}{σ_k^2})x + [log(π_k) - (\\frac{1}{2})log(2πσ_k^2) - (\\frac{μ_k^2}{2σ_k^2})]\\]\nThis discriminant function \\(δ_k(x)\\) is a quadratic function of \\(x\\). The coefficient of the \\(x^2\\) term is \\(\\frac{1}{2σ_k^2}\\). Since QDA assumes that \\(σ_k^2\\) is different for each class \\(k\\), this quadratic term does not cancel out when comparing the discriminant functions of two different classes (i.e., when finding the decision boundary where \\(δ_k(x) = δ_j(x))\\).\nTherefore, the resulting decision boundary is quadratic, and the Bayes classifier is not linear.\n\nWhen the number of features \\(p\\) is large, there tends to be a deterioration in the performance of KNN and other local approaches… This phenomenon is known as the curse of dimensionality… We will now investigate this curse.\n\n\n\nSuppose that we have a set of observations, each with measurements on \\(p = 1\\) feature, \\(X\\)… On average, what fraction of the available observations will we use to make the prediction?\n\nSince \\(X\\) is uniformly distributed on \\([0,1]\\), its range is \\(1\\). The prediction for a test observation \\(X=0.6\\) uses training observations in the range \\([0.55, 0.65]\\). The length of this interval is \\(0.65 - 0.55 = 0.1\\). Because the data is uniform, the fraction of observations that fall into an interval is equal to the length of that interval. Therefore, on average, we will use \\(10\\%\\) of the available observations.\n\nNow suppose that we have… \\(p = 2\\) features… On average, what fraction of the available observations will we use?\n\nWe are now in two dimensions, and we form a neighborhood by taking a \\(10\\%\\) range for each feature. This creates a square (a 2-D hypercube) centered on the test point. The side length for this square is 0.1 for each dimension. Since the features are uniformly distributed on [0,1] x [0,1], the fraction of observations we use is the area of this square, which is \\(0.1 \\cdot{} 0.1 = 0.01\\). So, on average, we will use only \\(1%\\) of the observations.\n\nNow suppose that we have… \\(p = 100\\) features… What fraction of the available observations will we use?\n\nFollowing the pattern, for \\(p=100\\) dimensions, our neighborhood is a 100-dimensional hypercube where each side has length \\(0.1\\). The volume of this hypercube is \\(0.1^{100}\\). This is an infinitesimally small number, meaning we expect to use virtually zero fraction of the available observations.\n\nUsing your answers… argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.\n\nThe answers to parts (a)-(c) show that the fraction of training data available in a local neighborhood of a fixed size (\\(10\\%\\) of the range of each feature) decreases exponentially as the number of dimensions p increases \\((0.1^p)\\). For a large \\(p\\), this fraction is so small that the neighborhood is effectively empty. This means there are no “nearby” neighbors to average for a prediction, which undermines the entire principle of KNN and other local methods. This is the curse of dimensionality.\n\nNow suppose that we wish to make a prediction… by creating a p-dimensional hypercube… that contains, on average, \\(10\\%\\) of the training observations. For \\(p\\) = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer.\n\nLet \\(l\\) be the side length of the p-dimensional hypercube. The volume is \\(l^p\\). We want this volume to be \\(0.1\\) (to capture \\(10\\%\\) of the uniform data). So, we solve \\(l^p = 0.1\\) for \\(l\\), which gives \\(l = (0.1)^{(1/p)}\\).\n\nFor \\(p = 1: l = (0.1)^{(1/1)} = 0.1\\)\nFor \\(p = 2: l = (0.1)^{(1/2)} ≈ 0.316\\)\nFor \\(p = 100: l = (0.1)^{(1/100)} ≈ 0.977\\)\n\n\n\n\n\n\n\nNote\n\n\n\nAs the dimension \\(p\\) increases, the side length \\(l\\) of the hypercube needed to capture just \\(10\\%\\) of the data rapidly approaches \\(1\\). For \\(p=100\\), each side of the “local” neighborhood must span over \\(97\\%\\) of the total range of its corresponding feature. This means the points within this hypercube are no longer truly “local” or “near” the test observation. The method loses its local character, and the predictions are based on points that are far away, which leads to poor performance.\n\n\n\nWe now examine the differences between LDA and QDA.\n\n\n\nIf the bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nQDA been the more flexible model will perform better on the training set, but worse on the test set. On the test set, QDA will overfit the data because the true decision boundary is linear.\n\n\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nIn this situation since the decision boundary is non-linear, QDA will perform better in both data sets. A linear model will underfit in this case.\n\n\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n\n\nAs sample size increase, QDA will improve because there is more data to fit and the low bias will offset increase in variance.\n\n\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n\n\nFalse. In this case, QDA will overfit the data.\n\n\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat\\beta_0 = -6\\), \\(\\hat\\beta_1 = 0.05\\), \\(\\hat\\beta_2 = 1\\).\n\n(a). Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.\n\\[\np(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}}\n\\]\nwhen \\(X_1 = 40\\) and \\(X_2 = 3.5\\), \\(p(X) = 0.38\\)\n\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n\n\\[\\begin{gather}\n\n\\log\\left(\\frac{p(X)}{1-p(x)}\\right) = -6 + 0.05X_1 + X_2 \\\\\n\n\\log\\left(\\frac{0.5}{1-0.5}\\right) = -6 + 0.05X_1 + 3.5 \\\\\n\n\\end{gather}\\]\nTherefore, solving the equation \\(0 = −6 + 0.05X_1 + 3.5\\), \\(X_1 = 50\\) hours.\n\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of \\(X\\) for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year.\n\nHint: Recall that the density function for a normal random variable is \\(f(x) =\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\\). You will need to use Bayes’ theorem.\nValue of companies issuing a dividend (D) = \\(D \\sim \\mathcal{N}(10, 36)\\).\nValue \\(v\\) for companies not issuing a dividend \\((D^c)\\) = \\(D^c \\sim \\mathcal{N}(0, 36)\\) and \\(p(D) = 0.8\\).\nBayes theorem:\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} P(X|D)}{P(D) \\cdot{} P(X|D) + P(D^c) \\cdot{} P(X|D^c)} \\\\\n\\end{align}\\]\nSubstitute the Gaussian likelihoods into Bayes theorem\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nFactor out \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) from the numerator and denominator, which cancels out:\n\\[\\begin{align}\nP(D|X)  &= \\frac{P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nSubstitute the given probabilities and means:\n\\[\\begin{align}\nP(D|X)  &= \\frac{0.8 \\times e^{-(4-10)^2/(2 \\times 36)}}\n               {0.8 \\times e^{-(4-10)^2/(2 \\times 36)} + 0.2 \\times e^{-(4-0)^2/(2 \\times 36)}} \\\\\n       &= \\frac{0.8 \\cdot e^{-1/2}}{0.8 \\cdot e^{-1/2} + 0.2 \\cdot e^{-2/9}} \\\\\n\\end{align}\\]\n\nexp(-0.5) * 0.8 / (exp(-0.5) * 0.8 + exp(-2/9) * 0.2)\n\n[1] 0.7518525\n\n\n\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\n\n\nLogistic regression will perform better because it has a lower test error rate. For \\(K = 1\\), the training error rate is always zero because the closest point is always the training point itself, so the model will never make a mistake on the training set. Given that the average error rate for 1-NN is 18%, this implies a test error rate of 36%. Logistic regression, with a test error rate of 30%, is therefore the better choice.\n\n\nThis problem has to do with odds.\n\n\n\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\n\\[\n\\frac{p(x)}{1 - P(x)} = odd\\\\\n\\] \\[\\begin{equation}\np(x) = \\frac{odd}{1 + odd}\\\\\n\\end{equation}\\]\n\\[\\begin{equation}\np(x)  = \\frac{0.37}{1 + 0.37} = 0.27\n\\end{equation}\\]\n\nSuppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?\n\n\\[\\frac{0.16}{1 - 0.16}  = 0.19\\]\n\nIn the setting with p = 1, (4.32) takes a simpler form…repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(πk\\), \\(πK\\), \\(μk\\), \\(μK\\), and \\(σ2\\).\n\nThe goal is to find the log-odds between class \\(k\\) and a baseline class \\(K\\) for an LDA model with \\(p=1\\). This is given by \\(log(\\frac{P(Y=k|X=x)}{P(Y=K|X=x)})\\). This is equivalent to \\(δ_k(x) - δ_K(x)\\).\nThe discriminant function for LDA is: \\[δ_k(x) = x \\cdot{} (\\frac{μ_k}{σ^2}) - (\\frac{μ_k^2}{2σ^2}) + log(π_k)\\]\nThe log-odds are: \\[δ_k(x) - δ_K(x) = [x \\cdot{} (\\frac{μ_k}{σ^2}) - (\\frac{μ_k^2}{2σ^2}) + log(π_k)] - [x \\cdot{} (\\frac{μ_K}{σ^2}) - (\\frac{μ_K^2}{2σ^2}) + log(π_K)]\\]\nNow, group the terms that are constant and those that are coefficients of \\(x\\): \\[= x \\cdot{} (\\frac{μ_k}{σ^2} - \\frac{μ_K}{σ^2}) + [log(π_k) - log(π_K) - \\frac{μ_k^2}{2σ^2} + \\frac{μ_K^2}{2σ^2}]\\]\nThis has the form \\(a_k + b_{k1} \\cdot{} x\\). The coefficients are: \\[a_k = log(\\frac{π_k}{π_K}) - \\frac{μ_k^2 - μ_K^2}{2σ^2}\\] \\[b_k1 = \\frac{μ_k - μ_K}{σ^2}\\] 11. Work out the detailed forms of \\(a_k\\), \\(b_{kj}\\), and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(πk\\), \\(μk\\), \\(Σk\\). (Assuming (4.33) refers to the general quadratic form of the QDA discriminant function).\nThe QDA discriminant function is: \\[δ_k(x) = -\\frac{1}{2} \\cdot{} log(det(Σ_k)) -\\frac{1}{2} \\cdot{} (x-μ_k)^T \\cdot{} Σ_k^{-1} \\cdot{} (x-μ_k) + log(π_k)\\] Expanding the quadratic term \\((x-μ_k)^T \\cdot{} Σ_k^{-1} \\cdot{} (x-μ_k)\\): \\[= x^TΣ_k^{-1}x - x^TΣ_k^{-1}μ_k - μ_k^TΣ_k^{-1}x + μ_k^TΣ_k^{-1}μ_k\\] Since \\(x^TΣ_k^{-1}μ_k\\) is a scalar, it’s equal to its transpose \\[μ_k^TΣ_k^{-1T}x = μ_k^TΣ_k^{-1}x\\].\nSo the expression is \\[x^TΣ_k^{-1}x - 2x^TΣ_k^{A-1}μ_k + μ_k^TΣ_k^{-1}μ_k\\].\nSubstituting this back into \\(δ_k(x)\\): \\[δ_k(x) = -\\frac{1}{2} \\cdot{} x^TΣ_k^{-1}x + x^TΣ_k^{-1}μ_k - \\frac{1}{2} \\cdot{} μ_k^TΣ_k^{-1}μ_k - \\frac{1}{2} \\cdot{} log(det(Σ_k)) + log(π_k)\\] This is a quadratic function of \\(x\\). We can identify the constant, linear, and quadratic parts.\nThe constant term is \\(a_k\\): \\[a_k = log(π_k) - \\frac{1}{2} \\cdot{} log(det(Σ_k)) - \\frac{1}{2} \\cdot{} μ_k^TΣ_k^{-1}μ_k\\] The linear part is \\(x^T(Σ_k^{-1}μ_k)\\). The vector of coefficients for the linear terms is \\(b_k = Σ_k^{-1}μ_k\\). The coefficient for the j-th variable, \\(x_j\\), is: \\(b_{kj} = (Σ_k^{-1}μ_k)_j\\)\nThe quadratic part is \\(-\\frac{1}{2} \\cdot{} x^TΣ_k^{-1}x\\). The matrix of coefficients for the quadratic terms \\(x_j \\cdot{} x_l\\) is \\(-\\frac{1}{2} \\cdot{} Σ_k^{-1}\\). The coefficient \\(b_{kjl}\\) is: \\[b_{kjl} = (\\frac{1}{2} \\cdot Σ_k^{-1})_{jl}\\]\n\nSuppose that you wish to classify an observation \\(X \\in \\mathbb{R}\\) into apples and oranges. You fit a logistic regression model and find that\n\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\beta_0 + \\hat\\beta_1x)}{1 + \\exp(\\hat\\beta_0 + \\hat\\beta_1x)}\n\\]\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x)}\n{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x) + \\exp(\\hat\\alpha_{apple0} + \\hat\\alpha_{apple1}x)}\n\\]\n\nWhat is the log odds of orange versus apple in your model?\n\n\nThe log odds (\\(\\frac{p(x)}{1 - P(x)}\\)) in my model will be \\(\\hat\\beta_0 + \\hat\\beta_1x\\)\n\n\nWhat is the log odds of orange versus apple in your friend’s model?\n\nLog odds of our friend model:\n\\[\n(\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x\n\\]\n\nSuppose that in your model, \\(\\hat\\beta_0 = 2\\) and \\(\\hat\\beta = −1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\nThe coefficient estimate in my friend’s model:\n\n\\(\\hat\\alpha_{orange0} -\\hat\\alpha_{apple0} = 2\\)\n\\(\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1} = -1\\).\n\n\nNow suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat\\alpha_{orange0} = 1.2\\), \\(\\hat\\alpha_{orange1} = −2\\), \\(\\hat\\alpha_{apple0} = 3\\), \\(\\hat\\alpha_{apple1} = 0.6\\). What are the coefficient estimates in your model?\n\nThe coefficients in my model would be \\(\\hat\\beta_0 = 1.2 - 3 = -1.8\\) and \\(\\hat\\beta_1 = -2 - 0.6 = -2.6\\)\n\nFinally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\n\nThey should agree everytime (i.e, pridictions and log odd between any pair of classes will remain the same, regardless of coding). The cofficient will be different because of the choice of baseline.\n\n\n\n\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n\n\n\nProduce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n\n\nlibrary(ISLR2) # data sets\nlibrary(tidyverse)\nlibrary(psych) # for correlation plots\nlibrary(MASS) # for lda() and qda()\nlibrary(class) # for knn()\nlibrary(e1071) # for naiveBayes()\ndata(\"Weekly\")\nattach(Weekly)\n\n\nstr(Weekly)\n\n'data.frame':   1089 obs. of  9 variables:\n $ Year     : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...\n $ Lag1     : num  0.816 -0.27 -2.576 3.514 0.712 ...\n $ Lag2     : num  1.572 0.816 -0.27 -2.576 3.514 ...\n $ Lag3     : num  -3.936 1.572 0.816 -0.27 -2.576 ...\n $ Lag4     : num  -0.229 -3.936 1.572 0.816 -0.27 ...\n $ Lag5     : num  -3.484 -0.229 -3.936 1.572 0.816 ...\n $ Volume   : num  0.155 0.149 0.16 0.162 0.154 ...\n $ Today    : num  -0.27 -2.576 3.514 0.712 1.178 ...\n $ Direction: Factor w/ 2 levels \"Down\",\"Up\": 1 1 2 2 2 1 2 2 2 1 ...\n\nWeekly$Year &lt;- as.factor(Weekly$Year)\n\npairs.panels(Weekly[1:8], cex.labels = 1, ellipses = FALSE)\n\n\n\n\n\n\n\n# total number of times the market had a positive or negative return\ntable(Weekly$Direction)\n\n\nDown   Up \n 484  605 \n\n\n\nThere is a strong positive correlation between the volume of shares traded and the year. From 2005 to 2010, the volume of shares began increasing exponentially. Additionally, between 1990 and 2010, the market had a total of 605 positive returns and 484 negative returns, respectively.\n\n\nUse the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\n\nglm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n               family = binomial)\n\nsummary(glm.fit)$coef\n\n               Estimate Std. Error    z value    Pr(&gt;|z|)\n(Intercept)  0.26686414 0.08592961  3.1056134 0.001898848\nLag1        -0.04126894 0.02641026 -1.5626099 0.118144368\nLag2         0.05844168 0.02686499  2.1753839 0.029601361\nLag3        -0.01606114 0.02666299 -0.6023760 0.546923890\nLag4        -0.02779021 0.02646332 -1.0501409 0.293653342\nLag5        -0.01447206 0.02638478 -0.5485006 0.583348244\nVolume      -0.02274153 0.03689812 -0.6163330 0.537674762\n\n\nAmong all the predictors, only Lag2 is statistically significant.\n\nCompute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n\n\nglm.prob &lt;- predict(glm.fit, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down   54  48\n    Up    430 557\n\n# training error rate:\nmean(glm.pred != Direction)\n\n[1] 0.4389348\n\n\n\nThe 43.8% error rate is the training error. This is likely an optimistic estimate of how the model would perform on new, unseen data, as the model was evaluated on the same data it was trained on.\n\n\nNow fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n\n\ntrain.data &lt;- Weekly[Year &lt; 2009, ]\ntest.data &lt;- Weekly[Year &gt; 2008, ]\n\nglm.fit &lt;- glm(Direction ~ Lag2, family = binomial,\n               data = train.data)\n\nsummary(glm.fit)$coef\n\n              Estimate Std. Error  z value   Pr(&gt;|z|)\n(Intercept) 0.20325743 0.06428036 3.162046 0.00156665\nLag2        0.05809527 0.02870446 2.023911 0.04297934\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    9  5\n    Up     34 56\n\nmean(glm.pred == test.data$Direction)\n\n[1] 0.625\n\n# Test error rate\nmean(glm.pred != test.data$Direction)\n\n[1] 0.375\n\n\n\nThe model improved with Lag2 as the only the predictor. Test error rate is 37.5% which is better than random guessing.\n\n\nRepeat (d) using LDA.\n\n\n# lda() is part of the MASS library\nlda.fit &lt;- lda(Direction ~ Lag2, data = train.data)\n\nplot(lda.fit)\n\n\n\n\n\n\n\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    9  5\n  Up     34 56\n\n# test error rate\nmean(lda.pred$class != test.data$Direction)\n\n[1] 0.375\n\n\n\nRepeat (d) using QDA.\n\n\nqda.fit &lt;- qda(Direction ~ Lag2, data = train.data)\n\nqda.pred &lt;- predict(qda.fit, newdata = test.data)\n\ntable(qda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    0  0\n  Up     43 61\n\n# test error rate\nmean(qda.pred$class != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nRepeat (d) using KNN with \\(K = 1\\).\n\n\ntrain &lt;- (Year &lt; 2009) \n\ntrain.Direction &lt;- Weekly$Direction[train] # vector for the train class labels\ntest.Direction &lt;- Weekly$Direction[!train] \n\n# knn() is part of the 'Class' library\nset.seed(1)\nknn.pred &lt;- knn(Weekly[train, \"Lag2\", drop = FALSE],\n                Weekly[!train, \"Lag2\", drop = FALSE], \n                train.Direction, k = 1)\n\n\ntable(knn.pred, test.Direction)\n\n        test.Direction\nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# test error rate\nmean(knn.pred != test.Direction)\n\n[1] 0.5\n\n\n\nRepeat (d) using naive Bayes.\n\n\nnb.fit &lt;- naiveBayes(Direction ~ Lag2, data = train.data)\n\nnb.pred &lt;- predict(nb.fit, test.data)\n\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    0  0\n   Up     43 61\n\n# test error rate\nmean(nb.pred != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nWhich of these methods appears to provide the best results on this data?\n\n\nLogistic regression and linear discriminant analysis methods provide the best results on this data.\n\n\nExperiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for \\(K\\) in the KNN classifier.\n\n\n# Logistic Regression\nglm.fit &lt;- glm(Direction ~ Lag2 + Lag3 + Lag4,\n               data = train.data,\n               family = binomial)\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    8  5\n    Up     35 56\n\ncat(\"Logistic error rate: \", mean(glm.pred != test.data$Direction))\n\nLogistic error rate:  0.3846154\n\n# Linear Discreminant analysis\nlda.fit &lt;- lda(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    8  5\n  Up     35 56\n\ncat(\"LDA error rate: \", mean(lda.pred$class != test.data$Direction))\n\nLDA error rate:  0.3846154\n\n# Naiv Bayes\nnb.fit &lt;- naiveBayes(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nnb.pred &lt;- predict(nb.fit, test.data)\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    7 10\n   Up     36 51\n\ncat(\"Naive Bayes error rate: \", mean(nb.pred != test.data$Direction))\n\nNaive Bayes error rate:  0.4423077\n\n# KNN\n\n# Calculate error rates for each k\nerror_rates &lt;- sapply(1:100, function(k) {\n  train &lt;- (Year &lt; 2009)\n  set.seed(1)\n  \n  knn.pred &lt;- knn(Weekly[train, 2:4, drop = FALSE],\n                  Weekly[!train, 2:4, drop = FALSE], \n                  Weekly$Direction[train], k = k)\n  \n  error_rate &lt;- mean(knn.pred != test.Direction)\n  return(error_rate)\n})\n\n# Find the k with the minimum error rate\nmin_error_rate &lt;- min(error_rates)\nbest_k &lt;- which.min(error_rates)\ntable(knn.pred, Weekly$Direction[!train])\n\n        \nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# Print the result\ncat(\"The best k is:\", best_k, \"with an error rate of:\", min_error_rate, \"\\n\")\n\nThe best k is: 26 with an error rate of: 0.3365385 \n\n\n\nAfter experimenting with several models and predictor combinations, the methods that provided the best results were logistic regression, LDA, and Naive Bayes using only Lag2 as a predictor. These models all achieved a test error rate of \\(37.5\\%\\). The more complex models, including those with more predictors or the optimized KNN classifier, did not outperform this simpler baseline."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hello! I’m Jallow",
    "section": "",
    "text": "I hold a bachelor’s degree in biological chemistry. I am passionate about science and epistemology. I created this website to share what I’m learning and topics that captivate me. Welcome to my learning journey!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Predicting Breast Cancer with R and Machine Learning\n\n\n\nLearn\n\nR\n\nMachine Learning\n\nClassification\n\nHealthcare Data\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nAbdoulie Jallow\n\n\n\n\n\n\n\n\n\n\n\n\nA Quarto Thesis Template for Bachelor and Diploma Projects at JKU\n\n\n\nResources\n\nQuarto\n\nJKU\n\nThesis\n\n\n\nSimplify your JKU Bachelor’s or Diploma thesis writing with this customized Quarto template. Focus on your content, let the template handle the formatting.\n\n\n\n\n\nNov 17, 2024\n\n\nAbdoulie Jallow\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\n\nLearn\n\nR\n\n\n\nI’m continuing my journey through the classic textbook An Introduction to Statistical Learning. In this post, I’ll walk through my solutions for the Chapter 4 exercises, which cover the topic of Classification. This book is the work of Gareth M. James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n\n\n\n\n\nNov 14, 2024\n\n\nAbdoulie Jallow\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html",
    "href": "posts/Breast_cancer/index_cancer.html",
    "title": "Predicting Breast Cancer with R and Machine Learning",
    "section": "",
    "text": "This blog post walks through a practical exercise in building and evaluating machine learning classification models using R. We’ll use the well-known Breast Cancer Wisconsin (Original) dataset from the UCI Machine Learning Repository to classify tumors as either benign (non-cancerous) or malignant (cancerous) based on microscopic characteristics of cell nuclei.\nWe will cover the following steps:\n\nLoading and exploring the dataset.\nPreprocessing the data (handling missing values, encoding variables).\nAnalyzing feature relationships and addressing collinearity.\nSplitting the dataset into training and testing sets.\nTraining various classification algorithms (Logistic Regression, KNN, LDA, QDA, Naive Bayes).\nEvaluating model performance using metrics such as accuracy, AUC-ROC, and precision-recall curves.\n\nLet’s dive in!"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#introduction",
    "href": "posts/Breast_cancer/index_cancer.html#introduction",
    "title": "Predicting Breast Cancer with R and Machine Learning",
    "section": "",
    "text": "This blog post walks through a practical exercise in building and evaluating machine learning classification models using R. We’ll use the well-known Breast Cancer Wisconsin (Original) dataset from the UCI Machine Learning Repository to classify tumors as either benign (non-cancerous) or malignant (cancerous) based on microscopic characteristics of cell nuclei.\nWe will cover the following steps:\n\nLoading and exploring the dataset.\nPreprocessing the data (handling missing values, encoding variables).\nAnalyzing feature relationships and addressing collinearity.\nSplitting the dataset into training and testing sets.\nTraining various classification algorithms (Logistic Regression, KNN, LDA, QDA, Naive Bayes).\nEvaluating model performance using metrics such as accuracy, AUC-ROC, and precision-recall curves.\n\nLet’s dive in!"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#exploratory-analysis-and-preprocessing",
    "href": "posts/Breast_cancer/index_cancer.html#exploratory-analysis-and-preprocessing",
    "title": "Predicting Breast Cancer with R and Machine Learning",
    "section": "Exploratory Analysis and Preprocessing",
    "text": "Exploratory Analysis and Preprocessing\nBefore building any models, understanding and preparing the data is essential. This involves inspecting the data structure, identifying and handling missing values, correcting data types, and exploring relationships between variables.\n\nUnderstanding the Data Source\nThe first step is always to consult the dataset’s documentation. It provides vital information about the variables, their meaning, potential issues like missing value codes, and the number of instances. For this dataset, the documentation tells us about the features measured from digitized images of fine needle aspirates (FNAs) of breast masses.\n\n\nImporting the Dataset\nWe load the data, which is provided as a comma-separated file without headers. Based on the documentation, we assign meaningful column names\n\n# Define the path to the dataset\npath &lt;- \"dataset/breast-cancer-wisconsin.data\"\n\n# Read the data using read.table as it's a simple CSV without headers\ndata &lt;- read.table(path, sep = \",\")\n\n# Assign column names based on the dataset documentation\ncolnames(data) &lt;- c(\"Sample_code_number\", \"Clump_thickness\",\n                    \"Uniformity_of_cell_size\", \"Uniformity_of_cell_shape\",\n                    \"Marginal_adhesion\", \"Single_epithelial_cell_size\",\n                    \"Bare_nuclei\", \"Bland_chromatin\", \"Normal_nucleoli\", \n                    \"Mitoses\", \"Class\")\n\n# Display the structure of the data\nstr(data)\n\n'data.frame':   699 obs. of  11 variables:\n $ Sample_code_number         : int  1000025 1002945 1015425 1016277 1017023 1017122 1018099 1018561 1033078 1033078 ...\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Uniformity_of_cell_shape   : int  1 4 1 8 1 10 1 2 1 1 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : chr  \"1\" \"10\" \"2\" \"4\" ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Normal_nucleoli            : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Mitoses                    : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : int  2 2 2 2 2 4 2 2 2 2 ...\n\n# Display the first few rows\nhead(data)\n\n  Sample_code_number Clump_thickness Uniformity_of_cell_size\n1            1000025               5                       1\n2            1002945               5                       4\n3            1015425               3                       1\n4            1016277               6                       8\n5            1017023               4                       1\n6            1017122               8                      10\n  Uniformity_of_cell_shape Marginal_adhesion Single_epithelial_cell_size\n1                        1                 1                           2\n2                        4                 5                           7\n3                        1                 1                           2\n4                        8                 1                           3\n5                        1                 3                           2\n6                       10                 8                           7\n  Bare_nuclei Bland_chromatin Normal_nucleoli Mitoses Class\n1           1               3               1       1     2\n2          10               3               2       1     2\n3           2               3               1       1     2\n4           4               3               7       1     2\n5           1               3               1       1     2\n6          10               9               7       1     4\n\n\n\n\nMissing Values and Variable Encoding\nThe documentation reveals that missing values are coded as \"?\". We need to replace these with R’s standard NA representation. We also need to handle variables encoded incorrectly\n\n# Replace \"?\" with NA\ndata[data == \"?\"] &lt;- NA\n\n# Check how many missing values per column\ncolSums(is.na(data))\n\n         Sample_code_number             Clump_thickness \n                          0                           0 \n    Uniformity_of_cell_size    Uniformity_of_cell_shape \n                          0                           0 \n          Marginal_adhesion Single_epithelial_cell_size \n                          0                           0 \n                Bare_nuclei             Bland_chromatin \n                         16                           0 \n            Normal_nucleoli                     Mitoses \n                          0                           0 \n                      Class \n                          0 \n\n# For simplicity in this example, we remove rows with any missing values.\n# In a real-world scenario, imputation might be considered.\ndata &lt;- na.omit(data)\ncat(\"Dimensions after removing NAs:\", dim(data), \"\\n\")\n\nDimensions after removing NAs: 683 11 \n\n# The 'Sample_code_number' is an identifier and not useful for classification.\ndata$Sample_code_number &lt;- NULL\n\n# The 'Class' variable is coded as 2 for benign and 4 for malignant.\n# Convert it to a factor with meaningful labels.\ndata$Class &lt;- factor(ifelse(data$Class == 2, \"benign\", \"malignant\"),\n                     levels = c(\"benign\", \"malignant\")) # Explicitly set levels\n\n# The 'Bare_nuclei' column was read as character due to the \"?\" values.\n# Now that NAs are handled, convert it to integer.\ndata$Bare_nuclei &lt;- as.integer(data$Bare_nuclei)\n\n# Verify the structure again\nstr(data)\n\n'data.frame':   683 obs. of  10 variables:\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Uniformity_of_cell_shape   : int  1 4 1 8 1 10 1 2 1 1 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : int  1 10 2 4 1 10 10 1 1 1 ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Normal_nucleoli            : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Mitoses                    : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n\n# Check the distribution of the target variable\nsummary(data$Class)\n\n   benign malignant \n      444       239 \n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways check the documentation! As seen here, understanding how missing values (\"?\") and the target variable (Class) were encoded was crucial for correct preprocessing. Failing to do this can lead to errors or incorrect model results.\n\n\n\n\nExploring Variable Distributions and Relationships\nLet’s examine the predictor variables. Since they are all numerical (ratings from 1-10), we can look at their distributions and correlations.\n\n# Summary statistics for numerical predictors\nsummary(data[, -which(names(data) == \"Class\")]) # Exclude the Class variable\n\n Clump_thickness  Uniformity_of_cell_size Uniformity_of_cell_shape\n Min.   : 1.000   Min.   : 1.000          Min.   : 1.000          \n 1st Qu.: 2.000   1st Qu.: 1.000          1st Qu.: 1.000          \n Median : 4.000   Median : 1.000          Median : 1.000          \n Mean   : 4.442   Mean   : 3.151          Mean   : 3.215          \n 3rd Qu.: 6.000   3rd Qu.: 5.000          3rd Qu.: 5.000          \n Max.   :10.000   Max.   :10.000          Max.   :10.000          \n Marginal_adhesion Single_epithelial_cell_size  Bare_nuclei    \n Min.   : 1.00     Min.   : 1.000              Min.   : 1.000  \n 1st Qu.: 1.00     1st Qu.: 2.000              1st Qu.: 1.000  \n Median : 1.00     Median : 2.000              Median : 1.000  \n Mean   : 2.83     Mean   : 3.234              Mean   : 3.545  \n 3rd Qu.: 4.00     3rd Qu.: 4.000              3rd Qu.: 6.000  \n Max.   :10.00     Max.   :10.000              Max.   :10.000  \n Bland_chromatin  Normal_nucleoli    Mitoses      \n Min.   : 1.000   Min.   : 1.00   Min.   : 1.000  \n 1st Qu.: 2.000   1st Qu.: 1.00   1st Qu.: 1.000  \n Median : 3.000   Median : 1.00   Median : 1.000  \n Mean   : 3.445   Mean   : 2.87   Mean   : 1.603  \n 3rd Qu.: 5.000   3rd Qu.: 4.00   3rd Qu.: 1.000  \n Max.   :10.000   Max.   :10.00   Max.   :10.000  \n\n# Visualize distributions\n data %&gt;%\n  select(-Class) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  facet_wrap(~variable, scales = \"free_y\") +\n  labs(title = \"Distribution of Predictor Variables\", x = \"Value (1-10)\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Analyze correlations among continuous variables\n# Select only numeric predictors for correlation analysis\nnumeric_predictors &lt;- data %&gt;% select(-Class) %&gt;% names()\ndata_corr &lt;- cor(data[, numeric_predictors])\n\n# Visualize the correlation matrix\ncorrplot(data_corr, method = \"circle\", type = \"upper\", tl.col = \"black\", tl.srt = 45, order = \"hclust\")\n\n\n\n\n\n\n\n# Use pairs.panels for scatter plots and correlation values (optional, can be busy)\n# pairs.panels(data[, numeric_predictors], cex.labels = 0.7, ellipses = FALSE, lm=TRUE)\n\nWe observe strong positive correlations between several variables, particularly Uniformity_of_cell_size and Uniformity_of_cell_shape (correlation coefficient often &gt; 0.9). High collinearity can sometimes destabilize model coefficients (especially in linear models) and make interpretation harder.\nA common strategy is to remove one of the highly correlated variables. Let’s remove Uniformity_of_cell_shape as it’s highly correlated with Uniformity_of_cell_size.\n\n# Remove 'Uniformity_of_cell_shape' due to high correlation with 'Uniformity_of_cell_size'\ndata$Uniformity_of_cell_shape &lt;- NULL\n\n# Check the structure again\nstr(data)\n\n'data.frame':   683 obs. of  9 variables:\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : int  1 10 2 4 1 10 10 1 1 1 ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Normal_nucleoli            : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Mitoses                    : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n\n\n\n\nChecking for Class Imbalance\nClass imbalance occurs when one class is much more frequent than the other. This can bias models towards the majority class. Let’s check our target variable Class.\n\n# Calculate class proportions\nimbalance_summary &lt;- data |&gt;\n  count(Class) |&gt;\n  mutate(Percentage = round((n / sum(n)) * 100, 1)) # Use sum(n) instead of nrow(data) just in case\n\nprint(imbalance_summary)\n\n      Class   n Percentage\n1    benign 444         65\n2 malignant 239         35\n\n# Visualize class distribution\nggplot(imbalance_summary, aes(x = Class, y = n, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(n, \"\\n(\", Percentage, \"%)\"))) +\n  labs(title = \"Distribution of Target Variable (Class)\", x = \"Tumor Class\", y = \"Number of Samples\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Paired\")\n\n\n\n\n\n\n\n\nThe dataset shows a moderate imbalance: approximately 65% benign and 35% malignant. While not extreme, this is something to keep in mind during evaluation. Metrics like the Area Under the Precision-Recall Curve (AUC-PR) can be more informative than ROC-AUC in such cases. For this analysis, we’ll proceed without specific resampling techniques (like SMOTE, upsampling, or downsampling), but they could be considered if performance on the minority class (malignant) is poor.\n\n\nSplitting the Dataset\nTo evaluate our models’ generalization ability, we split the data into a training set (used to build the models) and a test set (used for final, unbiased evaluation). We’ll use a 70% / 30% split, stratified by the Class variable to maintain the class proportions in both sets.\nWe will also set up cross-validation (CV) within the training process. CV provides a more robust estimate of performance by repeatedly splitting the training data into smaller train/validation folds.\n\n# Create a stratified split (70% train, 30% test)\ntrainIndex &lt;- createDataPartition(data$Class, p = 0.7, list = FALSE, times = 1)\n\ndata.trn &lt;- data[trainIndex, ]\ndata.tst &lt;- data[-trainIndex, ]\n\ncat(\"Training set dimensions:\", dim(data.trn), \"\\n\")\n\nTraining set dimensions: 479 9 \n\ncat(\"Test set dimensions:\", dim(data.tst), \"\\n\")\n\nTest set dimensions: 204 9 \n\ncat(\"Training set class proportions:\\n\")\n\nTraining set class proportions:\n\nprint(prop.table(table(data.trn$Class)))\n\n\n   benign malignant \n0.6492693 0.3507307 \n\ncat(\"Test set class proportions:\\n\")\n\nTest set class proportions:\n\nprint(prop.table(table(data.tst$Class)))\n\n\n   benign malignant \n0.6519608 0.3480392 \n\n# Set up control parameters for cross-validation within caret::train\n# We use 10-fold cross-validation\nctrl &lt;- trainControl(method = \"cv\",\n                     number = 10,\n                     classProbs = TRUE, # Calculate class probabilities needed for ROC/PR\n                     savePredictions = \"final\", # Save predictions from CV folds\n                     summaryFunction = twoClassSummary) # Use metrics like ROC AUC"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#modeling",
    "href": "posts/Breast_cancer/index_cancer.html#modeling",
    "title": "Predicting Breast Cancer with R and Machine Learning",
    "section": "Modeling",
    "text": "Modeling\nNow, we’ll train several different classification algorithms on the training data. caret provides a unified interface for this. We’ll evaluate them using the cross-validation strategy defined above and then assess their final performance on the held-out test set.\nOur goal is to predict the Class variable based on the other features.\n\nLogistic regression\nLogistic regression is one of the simplest and most interpretable classification algorithms. It models the probability of an instance belonging to a class.\n\n# Train the Logistic Regression model\nglm.fit &lt;- train(Class ~ .,\n                 data = data.trn,\n                 method = \"glm\",          # Generalized Linear Model\n                 family = \"binomial\",     # Specifies logistic regression\n                 trControl = ctrl,\n                 metric = \"ROC\")          # Optimize based on ROC AUC during CV\n\n# Print the best model summary (based on CV)\nprint(glm.fit)\n\nGeneralized Linear Model \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 432, 431, 432, 430, 431, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.9945209  0.9839718  0.9529412\n\n# Variable Importance for Logistic Regression\nvarImp(glm.fit)\n\nglm variable importance\n\n                            Overall\nBare_nuclei                  100.00\nClump_thickness               72.20\nMarginal_adhesion             50.85\nNormal_nucleoli               48.85\nBland_chromatin               48.19\nMitoses                       46.49\nUniformity_of_cell_size       21.39\nSingle_epithelial_cell_size    0.00\n\n# --- Evaluation on Test Set ---\n# Predict probabilities on the test set\nglm.pred.prob &lt;- predict(glm.fit, newdata = data.tst, type = \"prob\")\n\n# Predict class based on a 0.5 probability threshold\nglm.pred.class &lt;- ifelse(glm.pred.prob[, \"malignant\"] &gt; 0.5, \"malignant\", \"benign\")\nglm.pred.class &lt;- factor(glm.pred.class, levels = levels(data.tst$Class)) # Ensure factor levels match\n\n# Create confusion matrix\nlogistic_cm &lt;- confusionMatrix(data = glm.pred.class,\n                               reference = data.tst$Class,\n                               positive = \"malignant\") # Specify the \"positive\" class\nprint(logistic_cm)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       129         6\n  malignant      4        65\n                                          \n               Accuracy : 0.951           \n                 95% CI : (0.9117, 0.9762)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8913          \n                                          \n Mcnemar's Test P-Value : 0.7518          \n                                          \n            Sensitivity : 0.9155          \n            Specificity : 0.9699          \n         Pos Pred Value : 0.9420          \n         Neg Pred Value : 0.9556          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3186          \n   Detection Prevalence : 0.3382          \n      Balanced Accuracy : 0.9427          \n                                          \n       'Positive' Class : malignant       \n                                          \n\n\n\n\nK-Nearest Neighbors (KNN)\nKNN is a non-parametric, instance-based learner. It classifies a new sample based on the majority class of its 'K' nearest neighbors in the feature space. KNN requires features to be scaled. caret handles this automatically using the preProcess argument. We will also tune the hyperparameter k (number of neighbors).\n\n# Train the KNN model\n# caret will automatically test different values of 'k' specified in tuneGrid\nknn.fit &lt;- train(Class ~ .,\n                 data = data.trn,\n                 method = \"knn\",\n                 trControl = ctrl,\n                 preProcess = c(\"center\", \"scale\"), # Crucial for KNN\n                 tuneGrid = data.frame(k = seq(3, 21, by = 2)), # Tune k (odd values often preferred)\n                 metric = \"ROC\")\n\n# Print the best tuning parameter found during CV\nprint(knn.fit)\n\nk-Nearest Neighbors \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 431, 430, 431, 431, 431, ... \nResampling results across tuning parameters:\n\n  k   ROC        Sens       Spec     \n   3  0.9815999  0.9806452  0.9647059\n   5  0.9911023  0.9775202  0.9529412\n   7  0.9915856  0.9807460  0.9588235\n   9  0.9916716  0.9807460  0.9529412\n  11  0.9914819  0.9807460  0.9529412\n  13  0.9918643  0.9775202  0.9529412\n  15  0.9914848  0.9775202  0.9529412\n  17  0.9914848  0.9775202  0.9529412\n  19  0.9914848  0.9775202  0.9470588\n  21  0.9922438  0.9775202  0.9470588\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was k = 21.\n\n# Plot the tuning results (ROC vs. k)\nplot(knn.fit)\n\n\n\n\n\n\n\n# --- Evaluation on Test Set ---\n# Predict classes directly on the test set (KNN in caret often defaults to class)\nknn.pred.class &lt;- predict(knn.fit, newdata = data.tst)\n\n# Create confusion matrix\nknn_cm &lt;- confusionMatrix(data = knn.pred.class,\n                          reference = data.tst$Class,\n                          positive = \"malignant\")\nprint(knn_cm)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       129         4\n  malignant      4        67\n                                          \n               Accuracy : 0.9608          \n                 95% CI : (0.9242, 0.9829)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9136          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9437          \n            Specificity : 0.9699          \n         Pos Pred Value : 0.9437          \n         Neg Pred Value : 0.9699          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3284          \n   Detection Prevalence : 0.3480          \n      Balanced Accuracy : 0.9568          \n                                          \n       'Positive' Class : malignant       \n                                          \n\n\n\n\nLinear Discriminant Analysis\nLDA is a linear classifier that assumes features are normally distributed within each class and have equal covariance matrices. It finds a linear combination of features that maximizes separability between classes.\n\n# Train the LDA model\nlda.fit &lt;- train(Class ~ .,\n                 data = data.trn,\n                 method = \"lda\",\n                 trControl = ctrl,\n                 metric = \"ROC\")\n\nprint(lda.fit) # LDA doesn't have hyperparameters to tune in this basic form\n\nLinear Discriminant Analysis \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 431, 432, 431, 431, 431, ... \nResampling results:\n\n  ROC        Sens      Spec     \n  0.9946758  0.983871  0.9165441\n\n# --- Evaluation on Test Set ---\n# Predict probabilities\nlda.pred.prob &lt;- predict(lda.fit, newdata = data.tst, type = \"prob\")\n# Predict class\nlda.pred.class &lt;- ifelse(lda.pred.prob[, \"malignant\"] &gt; 0.5, \"malignant\", \"benign\")\nlda.pred.class &lt;- factor(lda.pred.class, levels = levels(data.tst$Class))\n\n# Create confusion matrix\nlda_cm &lt;- confusionMatrix(data = lda.pred.class,\n                          reference = data.tst$Class,\n                          positive = \"malignant\")\nprint(lda_cm)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       130         7\n  malignant      3        64\n                                          \n               Accuracy : 0.951           \n                 95% CI : (0.9117, 0.9762)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8905          \n                                          \n Mcnemar's Test P-Value : 0.3428          \n                                          \n            Sensitivity : 0.9014          \n            Specificity : 0.9774          \n         Pos Pred Value : 0.9552          \n         Neg Pred Value : 0.9489          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3137          \n   Detection Prevalence : 0.3284          \n      Balanced Accuracy : 0.9394          \n                                          \n       'Positive' Class : malignant       \n                                          \n\n\n\n\nQuadradic Discriminant Analysis\nQDA is similar to LDA but more flexible. It does not assume equal covariance matrices across classes, allowing for quadratic decision boundaries.\n\n# Train the QDA model\nqda.fit &lt;- train(Class ~ .,\n                 data = data.trn,\n                 method = \"qda\",\n                 trControl = ctrl,\n                 metric = \"ROC\")\n\nprint(qda.fit) # QDA also has no hyperparameters to tune here\n\nQuadratic Discriminant Analysis \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 431, 432, 431, 430, 431, 432, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.9896407  0.9517137  0.9819853\n\n# --- Evaluation on Test Set ---\n# Predict probabilities\nqda.pred.prob &lt;- predict(qda.fit, newdata = data.tst, type = \"prob\")\n# Predict class\nqda.pred.class &lt;- ifelse(qda.pred.prob[, \"malignant\"] &gt; 0.5, \"malignant\", \"benign\")\nqda.pred.class &lt;- factor(qda.pred.class, levels = levels(data.tst$Class))\n\n# Create confusion matrix\nqda_cm &lt;- confusionMatrix(data = qda.pred.class,\n                          reference = data.tst$Class,\n                          positive = \"malignant\")\nprint(qda_cm)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       123         3\n  malignant     10        68\n                                          \n               Accuracy : 0.9363          \n                 95% CI : (0.8935, 0.9656)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8627          \n                                          \n Mcnemar's Test P-Value : 0.09609         \n                                          \n            Sensitivity : 0.9577          \n            Specificity : 0.9248          \n         Pos Pred Value : 0.8718          \n         Neg Pred Value : 0.9762          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3333          \n   Detection Prevalence : 0.3824          \n      Balanced Accuracy : 0.9413          \n                                          \n       'Positive' Class : malignant       \n                                          \n\n\n\n\nNaive Bayes\nNaive Bayes is a probabilistic classifier based on Bayes’ theorem. It makes a “naive” assumption that all predictor variables are independent given the class. Despite this simplification, it often works well in practice.\n\n# Train the Naive Bayes model\n# Note: Need to install e1071 or klaR package if not already installed for method='naive_bayes'\n# install.packages(\"e1071\")\nnb.fit &lt;- train(Class ~ .,\n                data = data.trn,\n                method = \"naive_bayes\", # Uses e1071::naiveBayes implementation\n                trControl = ctrl,\n                metric = \"ROC\")\n\nprint(nb.fit) # Might show tuning parameters if the implementation supports them (e.g., Laplace correction)\n\nNaive Bayes \n\n479 samples\n  8 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 432, 431, 431, 430, 431, 431, ... \nResampling results across tuning parameters:\n\n  usekernel  ROC        Sens       Spec     \n  FALSE      0.9920956  0.9549395  0.9878676\n   TRUE      0.9936136  0.9742944  0.9819853\n\nTuning parameter 'laplace' was held constant at a value of 0\nTuning\n parameter 'adjust' was held constant at a value of 1\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were laplace = 0, usekernel = TRUE\n and adjust = 1.\n\n# --- Evaluation on Test Set ---\n# Predict probabilities\nnb.pred.prob &lt;- predict(nb.fit, newdata = data.tst, type = \"prob\")\n# Predict class\nnb.pred.class &lt;- ifelse(nb.pred.prob[, \"malignant\"] &gt; 0.5, \"malignant\", \"benign\")\nnb.pred.class &lt;- factor(nb.pred.class, levels = levels(data.tst$Class))\n\n# Create confusion matrix\nnb_cm &lt;- confusionMatrix(data = nb.pred.class,\n                         reference = data.tst$Class,\n                         positive = \"malignant\")\nprint(nb_cm)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       126         4\n  malignant      7        67\n                                          \n               Accuracy : 0.9461          \n                 95% CI : (0.9056, 0.9728)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8823          \n                                          \n Mcnemar's Test P-Value : 0.5465          \n                                          \n            Sensitivity : 0.9437          \n            Specificity : 0.9474          \n         Pos Pred Value : 0.9054          \n         Neg Pred Value : 0.9692          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3284          \n   Detection Prevalence : 0.3627          \n      Balanced Accuracy : 0.9455          \n                                          \n       'Positive' Class : malignant"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#performance-comparison",
    "href": "posts/Breast_cancer/index_cancer.html#performance-comparison",
    "title": "Predicting Breast Cancer with R and Machine Learning",
    "section": "Performance Comparison",
    "text": "Performance Comparison\nWe’ve trained five different models. Let’s compare their performance on the test set using standard evaluation metrics and visualizations. We’ll focus on ROC curves, Precision-Recall curves, and overall Accuracy.\n\nAUC-ROC Curve Analysis\nThe Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) at various classification thresholds. The Area Under the Curve (AUC-ROC) summarizes the model’s ability to discriminate between the positive (malignant) and negative (benign) classes across all thresholds. An AUC of 1 indicates perfect discrimination, while 0.5 suggests random guessing.\n\n\nCode\n# Function to calculate ROC curve using pROC\ncalculate_roc &lt;- function(model, test_data, response_var, positive_class) {\n  pred_probs &lt;- predict(model, newdata = test_data, type = \"prob\")\n  response &lt;- test_data[[response_var]]\n  roc_curve &lt;- roc(response = response,\n                   predictor = pred_probs[, positive_class],\n                   levels = levels(response),\n                   direction = \"&lt;\", # Ensure correct direction based on probability meaning\n                   quiet = TRUE) # Suppress messages\n  return(roc_curve)\n}\n\n# Calculate ROC for each model\nroc_glm &lt;- calculate_roc(glm.fit, data.tst, \"Class\", \"malignant\")\nroc_knn &lt;- calculate_roc(knn.fit, data.tst, \"Class\", \"malignant\")\nroc_lda &lt;- calculate_roc(lda.fit, data.tst, \"Class\", \"malignant\")\nroc_qda &lt;- calculate_roc(qda.fit, data.tst, \"Class\", \"malignant\")\nroc_nb &lt;- calculate_roc(nb.fit, data.tst, \"Class\", \"malignant\")\n\n# Extract AUC values\nauc_values &lt;- c(\n  Logistic = auc(roc_glm),\n  KNN = auc(roc_knn),\n  LDA = auc(roc_lda),\n  QDA = auc(roc_qda),\n  `Naive Bayes` = auc(roc_nb)\n)\n\n# Plot ROC Curves using base R plotting and pROC integration\npar(pty = \"s\") # Set square aspect ratio\n\nplot(roc_glm, col = \"#1B9E77\", lwd = 2, print.auc = FALSE, legacy.axes = TRUE, main = \"Comparison of ROC Curves\") # Start plot\nplot(roc_knn, col = \"#D95F02\", lwd = 2, add = TRUE, print.auc = FALSE)\nplot(roc_lda, col = \"#7570B3\", lwd = 2, add = TRUE, print.auc = FALSE)\nplot(roc_qda, col = \"#E7298A\", lwd = 2, add = TRUE, print.auc = FALSE)\nplot(roc_nb, col = \"#66A61E\", lwd = 2, add = TRUE, print.auc = FALSE)\n\n# Add Legend\nlegend(\"bottomright\",\n       legend = paste(names(auc_values), \"(AUC =\", round(auc_values, 3), \")\"),\n       col = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\"),\n       lwd = 2, cex = 0.8)\n\n\n\n\n\nROC Curves for all models on the test set. Higher curves (closer to top-left) and larger AUC values indicate better discrimination.\n\n\n\n\nAll models achieve extremely high AUC-ROC values (close to 1.0) on the test set. This suggests that the classes in this dataset are highly separable using the given features. While excellent performance is encouraging, AUCs this high can sometimes warrant double-checking for potential data leakage or indicate that the dataset might not fully represent the complexity of real-world scenarios. However, given the standard preprocessing steps, it likely reflects the relatively clear separation in this classic dataset. Logistic Regression, KNN, and LDA show near-perfect discrimination.\n\n\nPrecision Recall Curve\nThe Precision-Recall (PR) curve plots Precision (Positive Predictive Value) against Recall (Sensitivity or True Positive Rate). It is particularly informative when dealing with imbalanced datasets, as it focuses on the performance for the positive class (malignant, in our case) without involving True Negatives. The Area Under the PR Curve (AUC-PR) summarizes this performance.\n\n\nCode\n# Function to calculate PR curve data using PRROC\ncalculate_pr &lt;- function(model, test_data, true_labels, positive_class) {\n  pred_probs &lt;- predict(model, newdata = test_data, type = \"prob\")[, positive_class]\n  true_binary &lt;- ifelse(true_labels == positive_class, 1, 0)\n  pr &lt;- pr.curve(scores.class0 = pred_probs, weights.class0 = true_binary, curve = TRUE)\n  return(pr)\n}\n\n# Calculate PR curves\npr_glm &lt;- calculate_pr(glm.fit, data.tst, data.tst$Class, \"malignant\")\npr_knn &lt;- calculate_pr(knn.fit, data.tst, data.tst$Class, \"malignant\")\npr_lda &lt;- calculate_pr(lda.fit, data.tst, data.tst$Class, \"malignant\")\npr_qda &lt;- calculate_pr(qda.fit, data.tst, data.tst$Class, \"malignant\")\npr_nb &lt;- calculate_pr(nb.fit, data.tst, data.tst$Class, \"malignant\")\n\n# Extract AUC-PR values\nauc_pr_values &lt;- c(\n  Logistic = pr_glm$auc.integral,\n  KNN = pr_knn$auc.integral,\n  LDA = pr_lda$auc.integral,\n  QDA = pr_qda$auc.integral,\n  `Naive Bayes` = pr_nb$auc.integral\n)\n\n# Plot PR curves using PRROC's plot function\nplot(pr_glm, main = \"Comparison of Precision-Recall Curves\", col = \"#1B9E77\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_knn, add = TRUE, col = \"#D95F02\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_lda, add = TRUE, col = \"#7570B3\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_qda, add = TRUE, col = \"#E7298A\", lwd = 2, auc.main = FALSE, legend = FALSE)\nplot(pr_nb, add = TRUE, col = \"#66A61E\", lwd = 2, auc.main = FALSE, legend = FALSE)\n\n# Add Legend\nlegend(\"bottomleft\",\n       legend = paste(names(auc_pr_values), \"(AUC-PR =\", round(auc_pr_values, 3), \")\"),\n       col = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\"),\n       lwd = 2, cex = 0.8, bty = \"n\") # bty=\"n\" removes legend box\n\n\n\n\n\nPrecision-Recall Curves for all models on the test set. Curves closer to the top-right indicate better performance.\n\n\n\n\nSimilar to the ROC curves, the AUC-PR values are very high for all models, indicating excellent precision and recall for identifying malignant cases. Logistic Regression, KNN, and LDA again show top-tier performance with AUC-PR values very close to 1.0.\n\n\nAccuracy\nAccuracy is the overall proportion of correctly classified instances. While useful, it can be misleading on imbalanced datasets if a model simply predicts the majority class well. However, since both Sensitivity and Specificity were high for most models (as seen in the confusion matrices), accuracy here provides a reasonable summary.\n\n\nCode\n# Consolidate model performance metrics from confusion matrices\nmodel_performance &lt;- data.frame(\n  Model = c(\"Logistic\", \"KNN\", \"LDA\", \"QDA\", \"Naive Bayes\"),\n  Accuracy = c(logistic_cm$overall[\"Accuracy\"],\n               knn_cm$overall[\"Accuracy\"],\n               lda_cm$overall[\"Accuracy\"],\n               qda_cm$overall[\"Accuracy\"],\n               nb_cm$overall[\"Accuracy\"]),\n  LowerCI = c(logistic_cm$overall[\"AccuracyLower\"],\n              knn_cm$overall[\"AccuracyLower\"],\n              lda_cm$overall[\"AccuracyLower\"],\n              qda_cm$overall[\"AccuracyLower\"],\n              nb_cm$overall[\"AccuracyLower\"]),\n  UpperCI = c(logistic_cm$overall[\"AccuracyUpper\"],\n              knn_cm$overall[\"AccuracyUpper\"],\n              lda_cm$overall[\"AccuracyUpper\"],\n              qda_cm$overall[\"AccuracyUpper\"],\n              nb_cm$overall[\"AccuracyUpper\"])\n)\n\n# Plot using ggplot2\nggplot(model_performance, aes(x = Accuracy, y = reorder(Model, Accuracy))) + # Reorder models by accuracy\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_errorbarh(aes(xmin = LowerCI, xmax = UpperCI), height = 0.2, color = \"steelblue\") +\n  labs(title = \"Model Accuracy Comparison (with 95% CI)\",\n       x = \"Accuracy\",\n       y = \"Model\") +\n  scale_x_continuous(limits = c(0.85, 1.0)) + # Adjust limits based on observed values\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\nWarning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\nAccuracy of models on the test set with 95% confidence intervals.\n\n\n\n\nThe accuracy plot confirms the high performance across all models, with most achieving accuracy above 95%. Logistic Regression, KNN, and LDA appear slightly superior, consistent with the AUC metrics. The confidence intervals are relatively tight, indicating stable performance estimates."
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#conclusion",
    "href": "posts/Breast_cancer/index_cancer.html#conclusion",
    "title": "Predicting Breast Cancer with R and Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThe uniform high performance across diverse classifiers indicates that the Wisconsin (Original) dataset exhibits strong class separability. Therefore, model choice has limited impact on discrimination performance, and even simple linear classifiers will achieve near-optimal results."
  }
]