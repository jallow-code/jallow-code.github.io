[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU\n\n\n\nResources\n\n\n\nI’m sharing a Quarto template I customized for my bachelor’s thesis at Johannes Kepler University Linz.\n\n\n\nAbdoulie Jallow\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\n\nLearn\n\n\nR\n\n\n\nIn this blog post, I’ll share my solutions to exercises from An Introduction to Statistical Learning by Gareth M. James, Daniela Witten, Trevor Hastie, and Robert…\n\n\n\nAbdoulie Jallow\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html",
    "href": "posts/2024-11-14-statistical-learning/index.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "The association between predictors (\\(X = X_1, X_2,...,X_p\\)) and a quantitative response (Y) can be expressed as \\(Y = f(X) + \\epsilon\\). Where \\(f\\) is some fixed unknown function, and \\(\\epsilon\\) is a random error term with a zero mean. Statistical learning refers to a large collection of tools for estimating \\(f\\). These tools can be classified either supervised or unsupervised. Supervised learning involves estimating an output based on one or more inputs. In contrast, unsupervised learning involves inputs but no supervising outputs. There are two primary reasons for predicting \\(f\\):\n\nPrediction: This applies to situations were inputs (X) are readily available, but the output (Y) can not be easily obtained. \\(Y\\) can be estimated using \\(\\hat{Y} = \\hat{f}(X)\\) where \\(\\hat{f}\\) is our estimate for the true function \\(f\\). In this setting, one is not typically concerned with the exact form of \\(\\hat{f}\\) given it yields accurate predictions for \\(Y\\). The accuracy of \\(\\hat{f}\\) depends on two quantities known as reducible error and Irreducible error. The reducible error can be improved by using an appropriate statistical learning method to estimate \\(f\\). The Irreducible error is the variability associated with the error term (\\(Var(\\epsilon)\\)). Since \\(Y\\) also depends on \\(\\epsilon\\), it is impossible to achieve a perfect estimate of \\(Y\\). The irreducible error will always be greater than zero, as it may include unmeasured variables or unmeasurable variations influencing \\(Y\\).\nInference: In this setting we’re interested in the association between \\(Y\\) and \\(X\\) hence the exact form of \\(\\hat{f}\\) is very important. One may be interested in answering the following questions: Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\n\\(f\\) can be estimated using either parametric or non-parametric methods. parametric methods involve first assuming the shape of \\(f\\) (e.g., linear) and then using training data to fit or train the model. These methods are less flexible but more interpretable. Non-parametric methods do not make assumptions about the shape of \\(f\\), allowing for a wider range of possible forms. However, they tend to be less interpretable. Variables can be characterized as either quantitative or qualitative (also known as categorical). Problems with a quantitative response are often referred to as regression problems, while those involving a qualitative response are often referred to as classification problems. In regression settings the measure for the accuracy of a model is given by the mean squared error (MSE) . We choose a statistical learning method that gives the lowest test MSE. The test MSE depends on variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\), and var(\\(\\epsilon\\)). To achieve a low test MSE, one needs to choose a model that simultaneously results in low variance and low bias. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases.\nIn classification settings, model accuracy is evaluated by the proportion of incorrect predictions made when applying the estimated function \\(\\hat{f}\\) to a set of test observations. This proportion is referred to as the test error rate. A good classifier minimizes the test error rate, achieving the highest possible accuracy on unseen data. The Bayes classifier is an approach in classification that minimizes the test error rate by assigning each observation to the class with the highest conditional probability, \\(\\Pr(Y = j | X = x_0)\\). The decision boundary separating classes is called the Bayes decision boundary. This classifier achieves the lowest possible error rate, known as the Bayes error rate, which arises due to overlapping classes in the data. However, because the true conditional probabilities are generally unknown, the Bayes classifier serves as a theoretical benchmark. In practice, methods like k-Nearest Neighbors (k-NN) estimate these probabilities by considering the closest \\(K\\) training points to a test observation. k-NN assigns the class with the majority vote among these neighbors. The flexibility of k-NN depends on \\(K\\); small \\(K\\) values result in high variance but low bias, while large \\(K\\) values lead to high bias but low variance. The right balance is important, as overly flexible models overfit the training data, and less flexible ones underfit. The tradeoff is illustrated by the characteristic U-shape of the test error as flexibility increases."
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html#summary",
    "href": "posts/2024-11-14-statistical-learning/index.html#summary",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "The association between predictors (\\(X = X_1, X_2,...,X_p\\)) and a quantitative response (Y) can be expressed as \\(Y = f(X) + \\epsilon\\). Where \\(f\\) is some fixed unknown function, and \\(\\epsilon\\) is a random error term with a zero mean. Statistical learning refers to a large collection of tools for estimating \\(f\\). These tools can be classified either supervised or unsupervised. Supervised learning involves estimating an output based on one or more inputs. In contrast, unsupervised learning involves inputs but no supervising outputs. There are two primary reasons for predicting \\(f\\):\n\nPrediction: This applies to situations were inputs (X) are readily available, but the output (Y) can not be easily obtained. \\(Y\\) can be estimated using \\(\\hat{Y} = \\hat{f}(X)\\) where \\(\\hat{f}\\) is our estimate for the true function \\(f\\). In this setting, one is not typically concerned with the exact form of \\(\\hat{f}\\) given it yields accurate predictions for \\(Y\\). The accuracy of \\(\\hat{f}\\) depends on two quantities known as reducible error and Irreducible error. The reducible error can be improved by using an appropriate statistical learning method to estimate \\(f\\). The Irreducible error is the variability associated with the error term (\\(Var(\\epsilon)\\)). Since \\(Y\\) also depends on \\(\\epsilon\\), it is impossible to achieve a perfect estimate of \\(Y\\). The irreducible error will always be greater than zero, as it may include unmeasured variables or unmeasurable variations influencing \\(Y\\).\nInference: In this setting we’re interested in the association between \\(Y\\) and \\(X\\) hence the exact form of \\(\\hat{f}\\) is very important. One may be interested in answering the following questions: Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\n\\(f\\) can be estimated using either parametric or non-parametric methods. parametric methods involve first assuming the shape of \\(f\\) (e.g., linear) and then using training data to fit or train the model. These methods are less flexible but more interpretable. Non-parametric methods do not make assumptions about the shape of \\(f\\), allowing for a wider range of possible forms. However, they tend to be less interpretable. Variables can be characterized as either quantitative or qualitative (also known as categorical). Problems with a quantitative response are often referred to as regression problems, while those involving a qualitative response are often referred to as classification problems. In regression settings the measure for the accuracy of a model is given by the mean squared error (MSE) . We choose a statistical learning method that gives the lowest test MSE. The test MSE depends on variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\), and var(\\(\\epsilon\\)). To achieve a low test MSE, one needs to choose a model that simultaneously results in low variance and low bias. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases.\nIn classification settings, model accuracy is evaluated by the proportion of incorrect predictions made when applying the estimated function \\(\\hat{f}\\) to a set of test observations. This proportion is referred to as the test error rate. A good classifier minimizes the test error rate, achieving the highest possible accuracy on unseen data. The Bayes classifier is an approach in classification that minimizes the test error rate by assigning each observation to the class with the highest conditional probability, \\(\\Pr(Y = j | X = x_0)\\). The decision boundary separating classes is called the Bayes decision boundary. This classifier achieves the lowest possible error rate, known as the Bayes error rate, which arises due to overlapping classes in the data. However, because the true conditional probabilities are generally unknown, the Bayes classifier serves as a theoretical benchmark. In practice, methods like k-Nearest Neighbors (k-NN) estimate these probabilities by considering the closest \\(K\\) training points to a test observation. k-NN assigns the class with the majority vote among these neighbors. The flexibility of k-NN depends on \\(K\\); small \\(K\\) values result in high variance but low bias, while large \\(K\\) values lead to high bias but low variance. The right balance is important, as overly flexible models overfit the training data, and less flexible ones underfit. The tradeoff is illustrated by the characteristic U-shape of the test error as flexibility increases."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)), interval = \"confidence\") #  This function can be used to produce confidence and prediction intervals. Here, the numeric values are for prediction\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)), interval = \"prediction\")\nplot(lstat, medv)\nabline(lm.fit, lwd = 3) # lwd increases the width of the regression line by a factor of three (3). This works for plot() and lines() function too.\nabline(lm.fit, col = \"red\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abdoulie Jallow",
    "section": "",
    "text": "Hello! I’m Abdoulie, although most people call me Jallow. I have a bachelor’s degree in biological chemistry and am deeply interested in science, epistemology, and lifelong learning. Some of my favorite thinkers include Karl Popper, David Deutsch, and Nick Lane. I created this website to document and share my learning journey. Feel free to explore and see what I’m working on!"
  },
  {
    "objectID": "posts/jku-thesis-template/index.html",
    "href": "posts/jku-thesis-template/index.html",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "",
    "text": "This template is based on the Masters/Doctoral Thesis, LaTeX Template, Version 2.5 (27 Aug 2017), which Eli Holmes updated for Quarto. I further modified the Quarto template to fulfill JKU’s criteria for diploma theses and doctoral dissertations. Using Quarto was motivated by the difficulties I had when writing my first bachelor thesis in Microsoft Word. These include ensuring consistent citation formatting, text alignment, and maintaining a tidy version-controlled workflow. Quarto tackles these issues and also allows data analysis with R and documentation to be done on a single platform, hence improving reproducibility."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#clone-the-template",
    "href": "posts/jku-thesis-template/index.html#clone-the-template",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "1. Clone the Template",
    "text": "1. Clone the Template\nNavigate to the directory where you want to create your thesis files. Run the following command in a terminal to download the template and its associated folders:\n\nquarto use template jallow-code/jku-quarto-thesis\n\nYou will be prompted to specify an empty directory name for the template files. Provide a name for the new directory, which will store your thesis files."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#render-the-template",
    "href": "posts/jku-thesis-template/index.html#render-the-template",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "2. Render the Template",
    "text": "2. Render the Template\nOnce the template is installed, change into the new directory:\n\ncd &lt;new-directory-name&gt;\n\nTo generate the document, use the command:\n\nquarto render\n\n\nTroubleshooting Errors\nIf you encounter the following error during rendering:\n\nERROR: \ncompilation failed - missing packages (automatic install disabled)\nLaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H &lt;return&gt;  for immediate help.\n ...\nl.872 \\end{CSLReferences}\nThis error is likely due to outdated versions of Quarto or TinyTeX. To resolve it:\n\nUpdate your TinyTex installation.\nEnsure you are using the latest version of Quarto."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#thesis-type-and-frontmatter",
    "href": "posts/jku-thesis-template/index.html#thesis-type-and-frontmatter",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Thesis Type and Frontmatter",
    "text": "Thesis Type and Frontmatter\nThe template is configured for a Bachelor’s thesis by default. To change this, edit the _quarto.yml file. For example:\n\nthesis:\n  type: Bachelor Thesis # Or \"Master Thesis\", \"Doctoral Dissertation\", etc.\n  degree-name: Bachelor of Science\n  group: Biological Chemistry # name of your study program\n  supervisor:\n    primary: \"Assoz. Univ.-Prof. Dr. Ian Teasdale\"\n    co: \"Dipl.-Ing. Michael Kneidinger\"\n  university: Johannes Kepler Universität Linz\n  department: Institute of Polymer Chemistry\n  faculty: Faculty of Engineering and Natural Sciences\nYou can also customize elements of the front matter (e.g., acknowledgments, dedication) by commenting out or modifying the respective sections in_quarto.yml. For instance, to exclude the dedication page:\n\n# dedication: \"Frontmatter/dedication.tex\""
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#adding-symbols-or-abbreviations",
    "href": "posts/jku-thesis-template/index.html#adding-symbols-or-abbreviations",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Adding Symbols or Abbreviations",
    "text": "Adding Symbols or Abbreviations\nThe Frontmatter folder contains LaTeX files for abbreviations, symbols, and other sections. If needed, search online for LaTeX code examples to add symbols or format abbreviations."
  }
]