[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Exploring the Breast Cancer Dataset\n\n\n\nLearn\n\n\nR\n\n\n\n\n\n\n\nAbdoulie Jallow\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Quarto Thesis Template for Bachelor and Diploma Projects at JKU\n\n\n\nResources\n\n\n\nI’m sharing a Quarto template I customized for my bachelor’s thesis at Johannes Kepler University Linz.\n\n\n\nAbdoulie Jallow\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\n\nLearn\n\n\nR\n\n\n\nIn this blog post, I’ll share my solutions to exercises from An Introduction to Statistical Learning by Gareth M. James, Daniela Witten, Trevor Hastie, and Robert…\n\n\n\nAbdoulie Jallow\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html",
    "href": "posts/Breast_cancer/index_cancer.html",
    "title": "Exploring the Breast Cancer Dataset",
    "section": "",
    "text": "library(data.table)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(pROC)\nlibrary(PRROC)\nlibrary(RColorBrewer)\n\nset.seed(12)"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#introduction",
    "href": "posts/Breast_cancer/index_cancer.html#introduction",
    "title": "Exploring the Breast Cancer Dataset",
    "section": "Introduction",
    "text": "Introduction\nThis blog is an exercise on classification methods using the Breast Cancer dataset which is sourced from the UCI Machine Learning Repository. This dataset contains different variables used to classify tumors as either benign or malignant."
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#exploratory-analysis-and-preprocessing",
    "href": "posts/Breast_cancer/index_cancer.html#exploratory-analysis-and-preprocessing",
    "title": "Exploring the Breast Cancer Dataset",
    "section": "Exploratory Analysis and Preprocessing",
    "text": "Exploratory Analysis and Preprocessing\nWe will start by reading the documentation that comes with the dataset. This is crucial as it will allow us to understand its structure. For example, it may provide information on the number of variables or observations, how to interpret certain variables, how missing values are encoded, etc. Next, we will check for possible outliers, class imbalance, and the presence of redundant continuous variables.\n\nImporting the Dataset\nThe first step is to load the dataset and assign appropriate column names to make the data easier to interpret.\n\npath &lt;- \"dataset/breast-cancer-wisconsin.data\"\n\ndata &lt;- read.table(path, sep = \",\")\n\ncolnames(data) &lt;- c(\"Sample_code_number\", \"Clump_thickness\",\n                    \"Uniformity_of_cell_size\", \"Uniformity_of_cell_shape\",\n                    \"Marginal_adhesion\", \"Single_epithelial_cell_size\",\n                    \"Bare_nuclei\", \"Bland_chromatin\", \"Mitoses\",\n                    \"Normal_nucleoli\", \"Class\")\n\nstr(data)\n\n'data.frame':   699 obs. of  11 variables:\n $ Sample_code_number         : int  1000025 1002945 1015425 1016277 1017023 1017122 1018099 1018561 1033078 1033078 ...\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Uniformity_of_cell_shape   : int  1 4 1 8 1 10 1 2 1 1 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : chr  \"1\" \"10\" \"2\" \"4\" ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Mitoses                    : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Normal_nucleoli            : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : int  2 2 2 2 2 4 2 2 2 2 ...\n\n\n\n\nMissing Values and Variable Encoding\nThis dataset uses \"?\" to represent missing values. These need to be replaced with NA for proper handling in R. We then remove rows containing missing values. Additionally, some variables are encoded incorrectly and need to be transformed into their appropriate data types.\n\ndata[data == \"?\"] &lt;- NA \ndata &lt;- na.omit(data)   \n\ndata$Sample_code_number &lt;- NULL\n\ndata$Class &lt;- as.factor(ifelse(data$Class == 2, \"benign\", \"malignant\"))\n\ndata$Bare_nuclei &lt;- as.integer(data$Bare_nuclei)\n\n\nstr(data)\n\n'data.frame':   683 obs. of  10 variables:\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Uniformity_of_cell_size    : int  1 4 1 8 1 10 1 1 1 2 ...\n $ Uniformity_of_cell_shape   : int  1 4 1 8 1 10 1 2 1 1 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : int  1 10 2 4 1 10 10 1 1 1 ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Mitoses                    : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Normal_nucleoli            : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n\n\n\n\n\n\n\n\nTip\n\n\n\nReading the dataset documentation is important for understanding how missing values and variables are encoded. For example, missing values in this dataset are not labeled as NA by default, and variables such as Bare_nuclei and Class needed to be converted to the appropriate data types (integer and factor, respectively).\n\n\n\n\nAnalyze the continuous variables\nTo identify redundant features, we analyze correlations among continuous variables. High collinearity can negatively impact model performance, so we address this by combining highly correlated features.\n\ndata_corr &lt;- cor(data[, -10]) \n\npairs.panels(data_corr, cex.labels = 0.6, ellipses = FALSE)\n\n\n\n\n\n\n\n\nThe variables Uniformity_of_cell_size and Uniformity_of_cell_shape are highly correlated. To reduce redundancy, we create a new variable, Cell_morphology, by randomly sampling values from these two columns row-wise.\n\ndata$Cell_Morphology &lt;- apply(data[, c(\"Uniformity_of_cell_size\", \"Uniformity_of_cell_shape\")],\n                              MARGIN = 1, \n                              FUN = sample,\n                              size = 1)\n\ndata$Uniformity_of_cell_size &lt;- NULL\ndata$Uniformity_of_cell_shape &lt;- NULL\n\n\nstr(data)\n\n'data.frame':   683 obs. of  9 variables:\n $ Clump_thickness            : int  5 5 3 6 4 8 1 2 2 4 ...\n $ Marginal_adhesion          : int  1 5 1 1 3 8 1 1 1 1 ...\n $ Single_epithelial_cell_size: int  2 7 2 3 2 7 2 2 2 2 ...\n $ Bare_nuclei                : int  1 10 2 4 1 10 10 1 1 1 ...\n $ Bland_chromatin            : int  3 3 3 3 3 9 3 3 1 2 ...\n $ Mitoses                    : int  1 2 1 7 1 7 1 1 1 1 ...\n $ Normal_nucleoli            : int  1 1 1 1 1 1 1 1 5 1 ...\n $ Class                      : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n $ Cell_Morphology            : int  1 4 1 8 1 10 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n\n\n\n\nChecking for Class Imbalance\nClass imbalance is a common issue in classification tasks. Here, we analyze the distribution of the target variable (Class) to assess the extent of imbalance.\n\ndata |&gt;\n  count(Class) |&gt;\n  mutate(Percentage = round((n / nrow(data)) * 100, 2))\n\n      Class   n Percentage\n1    benign 444      65.01\n2 malignant 239      34.99\n\n\nThe dataset shows a mildly imbalanced distribution, with the minority class representing approximately 35% of the data. This level of imbalance typically does not pose significant challenges, and models can often be trained directly on the original data. If the imbalance were severe, techniques such as upsampling or downsampling could be applied.\n\n\nSplitting the Dataset\nTo prepare for model training, we split the dataset into training, validation, and test sets, allocating 70% of the data for training. We use cross-validation to ensure robust evaluation during model development.\n\ntrain &lt;- createDataPartition(data$Class, p = 0.7, list = FALSE)\n\ndata.trn &lt;- data[train, ]\ndata.tst &lt;- data[-train, ]\n\n\nctrl &lt;- trainControl(method = \"cv\",         # K-Fold cross-validation\n                     number = 10,           # 10 folds\n                     returnResamp = 'none',\n                     classProbs = TRUE,   \n                     savePredictions = TRUE,\n                     summaryFunction = twoClassSummary)"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#modeling",
    "href": "posts/Breast_cancer/index_cancer.html#modeling",
    "title": "Exploring the Breast Cancer Dataset",
    "section": "Modeling",
    "text": "Modeling\nNext, we fit different classification models to predict whether a tumor is malignant or benign based on the variables. Finally, we will evaluate the models using their respective confusion matrices and other performance metrics.\n\nLogistic regression\nLogistic regression is one of the simplest and most interpretable classification algorithms. We train the model using cross-validation and evaluate its performance on the test set.\n\nglm.fit &lt;- train(Class ~ .,         \n                 data = data.trn,               \n                 method = \"glm\",          # Generalized linear model\n                 family = \"binomial\",     # Logistic regression\n                 trControl = ctrl,              \n                 metric = \"ROC\")                 \n\nglm.pred &lt;- predict(glm.fit, newdata = data.tst, type = \"prob\")\n\nglm.pred &lt;- glm.pred[, 2]\n\nglm.class &lt;- ifelse(glm.pred &gt; 0.5, \"malignant\", \"benign\")\n\nlogistic_cm &lt;- confusionMatrix(data = as.factor(glm.class),\n                               reference = as.factor(data.tst$Class),\n                               positive = \"malignant\")\nlogistic_cm\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       130         3\n  malignant      3        68\n                                          \n               Accuracy : 0.9706          \n                 95% CI : (0.9371, 0.9891)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9352          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9577          \n            Specificity : 0.9774          \n         Pos Pred Value : 0.9577          \n         Neg Pred Value : 0.9774          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3333          \n   Detection Prevalence : 0.3480          \n      Balanced Accuracy : 0.9676          \n                                          \n       'Positive' Class : malignant       \n                                          \n\n\nTo understand which features contribute most to the model, we use the varImp() function.\n\n# Identify and rank the most important variables\nvarImp(glm.fit)\n\nglm variable importance\n\n                            Overall\nBare_nuclei                  100.00\nClump_thickness               78.57\nCell_Morphology               28.26\nBland_chromatin               27.03\nSingle_epithelial_cell_size   24.21\nMitoses                       16.35\nNormal_nucleoli               10.58\nMarginal_adhesion              0.00\n\n\n\n\nK-Nearest Neighbors (KNN)\nKNN is a simple non-parametric method that predicts the class of a sample based on the majority class of its nearest neighbors.\n\nknn.fit &lt;- train(Class ~ ., data = data.trn, method = \"knn\",\n  trControl = ctrl, \n  preProcess = c(\"center\",\"scale\"), \n  tuneGrid =data.frame(k=seq(5,100,by=5)),\n  metric = \"ROC\")\n  \n\nknn.pred &lt;- predict(knn.fit, data.tst)\n\nknn_cm &lt;- confusionMatrix(as.factor(knn.pred),\n                          reference =as.factor(data.tst$Class),\n                          positive = \"malignant\")\nknn_cm\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       130         6\n  malignant      3        65\n                                          \n               Accuracy : 0.9559          \n                 95% CI : (0.9179, 0.9796)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9018          \n                                          \n Mcnemar's Test P-Value : 0.505           \n                                          \n            Sensitivity : 0.9155          \n            Specificity : 0.9774          \n         Pos Pred Value : 0.9559          \n         Neg Pred Value : 0.9559          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3186          \n   Detection Prevalence : 0.3333          \n      Balanced Accuracy : 0.9465          \n                                          \n       'Positive' Class : malignant       \n                                          \n\nplot(knn.fit)\n\n\n\n\n\n\n\n\n\n\nLinear Discriminant Analysis\nLDA assumes that the predictors follow a normal distribution and aims to maximize the separation between classes.\n\nlda.fit &lt;- train(Class ~ ., \n                 data = data.trn, \n                 method = \"lda\", \n                 trControl = ctrl, \n                 metric = \"ROC\")\n\n# Generate probabilities for LDA\nlda.pred &lt;- predict(lda.fit, newdata = data.tst, type = \"prob\")\nlda.class &lt;- ifelse(lda.pred[, \"malignant\"] &gt; 0.5, \"malignant\", \"benign\")\n\n\nlda_cm &lt;- confusionMatrix(data = as.factor(lda.class),\n                          reference = as.factor(data.tst$Class),\n                          positive = \"malignant\")\nlda_cm\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       130         5\n  malignant      3        66\n                                          \n               Accuracy : 0.9608          \n                 95% CI : (0.9242, 0.9829)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.913           \n                                          \n Mcnemar's Test P-Value : 0.7237          \n                                          \n            Sensitivity : 0.9296          \n            Specificity : 0.9774          \n         Pos Pred Value : 0.9565          \n         Neg Pred Value : 0.9630          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3235          \n   Detection Prevalence : 0.3382          \n      Balanced Accuracy : 0.9535          \n                                          \n       'Positive' Class : malignant       \n                                          \n\n\n\n\nQuadradic Discriminant Analysis\nQDA is similar to LDA but allows for each class to have its own covariance matrix, making it more flexible for non-linear boundaries.\n\nqda.fit &lt;- train(Class ~ ., \n                 data = data.trn, \n                 method = \"qda\", \n                 trControl = ctrl, \n                 metric = \"ROC\")\n\n\nqda.pred &lt;- predict(qda.fit, newdata = data.tst, type = \"prob\")\nqda.class &lt;- ifelse(qda.pred[, \"malignant\"] &gt; 0.5, \"malignant\", \"benign\")\n\n\nqda_cm &lt;- confusionMatrix(data = as.factor(qda.class),\n                          reference = as.factor(data.tst$Class),\n                          positive = \"malignant\")\nqda_cm\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       122         0\n  malignant     11        71\n                                          \n               Accuracy : 0.9461          \n                 95% CI : (0.9056, 0.9728)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8853          \n                                          \n Mcnemar's Test P-Value : 0.002569        \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.9173          \n         Pos Pred Value : 0.8659          \n         Neg Pred Value : 1.0000          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3480          \n   Detection Prevalence : 0.4020          \n      Balanced Accuracy : 0.9586          \n                                          \n       'Positive' Class : malignant       \n                                          \n\n\n\n\nNaive Bayes\nNaive Bayes is a probabilistic classifier based on Bayes’ theorem, assuming independence among predictors.\n\nnb.fit &lt;- train(Class ~ ., \n                data = data.trn, \n                method = \"naive_bayes\", \n                trControl = ctrl, \n                metric = \"ROC\")\n\n\nnb.pred &lt;- predict(nb.fit, newdata = data.tst, type = \"prob\")\n\n\nnb.class &lt;- ifelse(nb.pred[, \"malignant\"] &gt; 0.5, \"malignant\", \"benign\")\n\n\nnb_cm &lt;- confusionMatrix(data = as.factor(nb.class),\n                         reference = as.factor(data.tst$Class),\n                         positive = \"malignant\")\nnb_cm\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       124         0\n  malignant      9        71\n                                          \n               Accuracy : 0.9559          \n                 95% CI : (0.9179, 0.9796)\n    No Information Rate : 0.652           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9056          \n                                          \n Mcnemar's Test P-Value : 0.007661        \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.9323          \n         Pos Pred Value : 0.8875          \n         Neg Pred Value : 1.0000          \n             Prevalence : 0.3480          \n         Detection Rate : 0.3480          \n   Detection Prevalence : 0.3922          \n      Balanced Accuracy : 0.9662          \n                                          \n       'Positive' Class : malignant"
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#performance",
    "href": "posts/Breast_cancer/index_cancer.html#performance",
    "title": "Exploring the Breast Cancer Dataset",
    "section": "Performance",
    "text": "Performance\n\nAUC-ROC Curve Analysis\n\n\nCode\n# ---- Function to calculate and return the ROC curves for a given mode -------\ncalculate_roc &lt;- function(model, test_data, response_var, positive_class, predictor_type = \"prob\") {\n  \n  # Generate predicted probabilities\n  pred_probs &lt;- predict(model, newdata = test_data, type = predictor_type)\n  \n  # Ensure the response variable is a factor with correct levels\n  response &lt;- test_data[[response_var]]\n  \n  # Calculate ROC curve\n  roc_curve &lt;- roc(\n    response = response,\n    predictor = pred_probs[, positive_class], # Probability for the positive class\n    levels = levels(response),\n    direction = \"&lt;\"  # Higher values indicate the positive class\n  )\n  \n  return(roc_curve)\n}\n\n# ---- Calculate ROC curves for all models -------\n# Logistic Regression\nroc_glm &lt;- calculate_roc(\n  model = glm.fit,\n  test_data = data.tst,\n  response_var = \"Class\",\n  positive_class = \"malignant\"\n)\n\n# KNN\nroc_knn &lt;- calculate_roc(\n  model = knn.fit,\n  test_data = data.tst,\n  response_var = \"Class\",\n  positive_class = \"malignant\"\n)\n\n# LDA\nroc_lda &lt;- calculate_roc(\n  model = lda.fit,\n  test_data = data.tst,\n  response_var = \"Class\",\n  positive_class = \"malignant\")\n\n\n# QDA\nroc_qda &lt;- calculate_roc(\n  model = qda.fit,\n  test_data = data.tst,\n  response_var = \"Class\",\n  positive_class = \"malignant\"\n)\n\n# Naive Bayes\nroc_nb &lt;- calculate_roc(\n  model = nb.fit,\n  test_data = data.tst,\n  response_var = \"Class\",\n  positive_class = \"malignant\"\n)\n\n# Extract AUC values\nauc_glm &lt;- round(auc(roc_glm), 2)\nauc_knn &lt;- round(auc(roc_knn), 2)\nauc_lda &lt;- round(auc(roc_lda), 2)\nauc_qda &lt;- round(auc(roc_qda), 2)\nauc_nb &lt;- round(auc(roc_nb), 2)\n\n# ------ Graph --------\npar(pty = \"s\") # Set square aspect ratio\n\n# Plot ROC Curves\nplot(roc_glm, col = \"#1B9E77\", lwd = 3, percent = TRUE,\n     main = \"ROC Curve Comparison\",\n     xlab = \"False Positive rate\", \n     ylab = \"True Positive rate\", \n     legacy.axes = TRUE)\n\nplot(roc_knn, col = \"#D95F02\", lwd = 3, percent = TRUE, add = T)\n\nplot(roc_lda, col = \"#7570B3\", lwd = 3, percent = TRUE, add = T)\n\nplot(roc_qda, col = \"#E7298A\", lwd = 3, percent = TRUE, add = T)\n\nplot(roc_nb, col = \"#66A61E\", lwd = 3, percent = TRUE, add = T)\n\n# Add Legend with AUC values\nlegend(\"bottomright\", \n       legend = c(\n         paste(\"Logistic (AUC =\", auc_glm, \")\"),\n         paste(\"KNN (AUC =\", auc_knn, \")\"),\n         paste(\"LDA (AUC =\", auc_lda, \")\"),\n         paste(\"QDA (AUC =\", auc_qda, \")\"),\n         paste(\"Naive Bayes (AUC =\", auc_nb, \")\")\n       ), \n       col = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\"), \n       lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\n# Reset par() variables\npar(pty = \"m\")\n\n\nThe AUC-ROC curves show how well each model can discriminate between malignant and benign at all classification thresholds. An AUC of 1.0 indicates a perfect classifier that can distinguish between the two classes without error. Logistic regression and KNN achieved an AUC of 1, indicating perfect classification performance. LDA, QDA, and Naive Bayes also performed well with AUC values close to 1.\n\n\nPrecision Recall Curve\n\n\nCode\n# Correct Precision-Recall Curve Plotting with PRROC\nplot_pr_curve &lt;- function(model, test_data, true_labels, positive_class, add = FALSE, color = \"blue\") {\n  # Generate predicted probabilities for the positive class\n  pred_probs &lt;- predict(model, newdata = test_data, type = \"prob\")[, positive_class]\n  \n  # Generate PR curve\n  pr &lt;- pr.curve(scores.class0 = pred_probs, \n                 weights.class0 = ifelse(true_labels == positive_class, 1, 0), \n                 curve = TRUE)\n  \n  # Plot PR curve\n  plot(pr, main = \"Precision-Recall Curve \", add = add, col = color, auc.main = FALSE, legend = FALSE)\n  \n  # Return AUC value\n  return(pr$auc.integral)\n}\n\n# Generate PR Curves for All Models\npr_aucs &lt;- c(\n  Logistic = plot_pr_curve(glm.fit, data.tst, data.tst$Class, \"malignant\", color = \"#1B9E77\"),\n  KNN = plot_pr_curve(knn.fit, data.tst, data.tst$Class, \"malignant\", add = TRUE, color = \"#D95F02\"),\n  LDA = plot_pr_curve(lda.fit, data.tst, data.tst$Class, \"malignant\", add = TRUE, color = \"#7570B3\"),\n  QDA = plot_pr_curve(qda.fit, data.tst, data.tst$Class, \"malignant\", add = TRUE, color = \"#E7298A\"),\n  NaiveBayes = plot_pr_curve(nb.fit, data.tst, data.tst$Class, \"malignant\", add = TRUE, color = \"#66A61E\")\n)\n\n# Add Legend with AUC\nlegend(\"bottomleft\", \n       legend = paste(names(pr_aucs), \"(AUC =\", round(pr_aucs, 2), \")\"), \n       col = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\"), \n       lty = 1, cex = 0.8, lwd = 2)\n\n\n\n\n\n\n\n\n\nSince the dataset is imbalanced, the false positive rate in the ROC-AUC curve is often replaced with precision to better evaluate model performance. In this context, the minority class (malignant) is important, so we prioritize models that effectively minimize false negatives. Logistic regression, KNN, and LDA all have AUC of 0.99 while QDA and Naive bayes performs slightly lower.\n\n\nAccuracy\n\n\nCode\n        #----- Data frame of model performance metrics --------\n\nmodel_data &lt;- data.frame(\n  Model = c(\"Logistic Regression\", \"KNN\", \"LDA\", \"QDA\",\n            \"Naive Bayes\"),\n   # accuracy values\n  accuracy = c(logistic_cm$overall[\"Accuracy\"],\n               knn_cm$overall[\"Accuracy\"],\n               lda_cm$overall[\"Accuracy\"],\n               qda_cm$overall[\"Accuracy\"],\n               nb_cm$overall[\"Accuracy\"]),\n   # Lower confidence interval\n  Lower = c(logistic_cm$overall[\"AccuracyLower\"],\n            knn_cm$overall[\"AccuracyLower\"],\n            lda_cm$overall[\"AccuracyLower\"],\n            qda_cm$overall[\"AccuracyLower\"],\n            nb_cm$overall[\"AccuracyLower\"]),\n  # Upper confidence interval\n  Upper = c(logistic_cm$overall[\"AccuracyUpper\"],\n            knn_cm$overall[\"AccuracyUpper\"],\n            lda_cm$overall[\"AccuracyUpper\"],\n            qda_cm$overall[\"AccuracyUpper\"],\n            nb_cm$overall[\"AccuracyUpper\"]))\n  \n\n                #----- plot -----\n\nggplot(model_data, aes(x = accuracy, y = Model)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2, color = \"blue\") +\n  labs(title = \"\",\n       x = \"Accuracy\",\n       y = \"\") +\n  scale_x_continuous(limits = c(0.88, 1)) + \n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.y = element_text(size = 12),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nAccuracy provides a direct (baseline) measure of model performance on correctly classified instances. All the models performed well on this metric."
  },
  {
    "objectID": "posts/Breast_cancer/index_cancer.html#conclusion",
    "href": "posts/Breast_cancer/index_cancer.html#conclusion",
    "title": "Exploring the Breast Cancer Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nLogistic Regression consistently outperformed other models across all metrics, hence emerges as the preferred model for this dataset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! I’m Jallow",
    "section": "",
    "text": "I hold a bachelor’s degree in biological chemistry. I am passionate about science and epistemology. I created this website to share what I’m learning and topics that captivate me. Welcome to my learning journey!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)), interval = \"confidence\") #  This function can be used to produce confidence and prediction intervals. Here, the numeric values are for prediction\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)), interval = \"prediction\")\nplot(lstat, medv)\nabline(lm.fit, lwd = 3) # lwd increases the width of the regression line by a factor of three (3). This works for plot() and lines() function too.\nabline(lm.fit, col = \"red\")"
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html",
    "href": "posts/2024-11-14-statistical-learning/index.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\nThe logistic function is define as\n\\[\np(X) = \\frac{e^{\\beta_o + \\beta_1X}}{1\\;+ e^{\\beta_o + \\beta_1X}}\\\\\n\\tag{1}\\]\nRearranging terms,\n\\[\\begin{gather*}\ne^{\\beta_o + \\beta_1X} = p(X) + p(X)\\cdot{ e^{\\beta_o + \\beta_1X}}\\\\\n\np(X) = e^{\\beta_o + \\beta_1X} - p(X) \\cdot e^{\\beta_o + \\beta_1X}\\\\\n\ne^{\\beta_o + \\beta_1X} \\cdot [1 - p(X)] = p(X)\\\\\n\n\\end{gather*}\\]\nTherefore the logit is representated as\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_o + \\beta_1X}\n\\tag{2}\\]\n\nIt was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(µk,σ2) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n\n\\[\np_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2)}\n              {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2)}\n\\tag{3}\\]\nThe discriminant function is\n\\[\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma_2} + \\log(\\pi_k)\n\\tag{4}\\]\nTo show Equation 3 is equals to Equation 4, we first assume \\(\\sigma_1^2\\;=\\;...\\;=\\sigma_k^2\\). Hence,\n\\[\np_k(x) = \\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}\n              {\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)}\n\\]\nNext, we take the \\(\\log(p_K(X))\\) to linearized the function. \\[\n\\log(p_k(x)) = \\log(\\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}{\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)})\n\\] Rearranging terms\n\\[\n\\log(p_k(x)) = \\log(\\pi_k) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 -\n              \\log\\left(\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)\\right)\n\\]\nTo maximize over \\(k\\), any term that’s independent of \\(k\\) is ignored\n\\[\\begin{align}\nf &= \\log(\\pi_k) - \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\\\\n  &= \\log(\\pi_k) - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\\\\n\\end{align}\\]\nSince \\(\\frac{x^2}{2\\sigma^2}\\) is also independent of \\(k\\), it is ignored in order to maximize over \\(k\\)\n\\[\n\\log(\\pi_k) + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}\n\\]\n\nTo Do\nWe now examine the differences between LDA and QDA.\n\n\n\nIf the bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nQDA been the more flexible model will perform better on the training set, but worse on the test set. On the test set, QDA will overfit the data because the true decision boundary is linear.\n\n\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nIn this situation since the decision boundary is non-linear, QDA will perform better in both data sets. A linear model will underfit in this case.\n\n\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n\n\nAs sample size increase, QDA will improve because there is more data to fit and the low bias will offset increase in variance.\n\n\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n\n\nFalse. In this case, QDA will overfit the data.\n\n\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat\\beta_0 = -6\\), \\(\\hat\\beta_1 = 0.05\\), \\(\\hat\\beta_2 = 1\\).\n\n(a). Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.\n\\[\np(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}}\n\\]\nwhen \\(X_1 = 40\\) and \\(X_2 = 3.5\\), \\(p(X) = 0.38\\)\n\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n\n\\[\\begin{gather}\n\n\\log\\left(\\frac{p(X)}{1-p(x)}\\right) = -6 + 0.05X_1 + X_2 \\\\\n\n\\log\\left(\\frac{0.5}{1-0.5}\\right) = -6 + 0.05X_1 + 3.5 \\\\\n\n\\end{gather}\\]\nTherefore, solving the equation \\(0 = −6 + 0.05X_1 + 3.5\\), \\(X_1 = 50\\) hours.\n\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of \\(X\\) for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year.\n\nHint: Recall that the density function for a normal random variable is \\(f(x) =\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\\). You will need to use Bayes’ theorem.\nValue of companies issuing a dividend (D) = \\(D \\sim \\mathcal{N}(10, 36)\\).\nValue \\(v\\) for companies not issuing a dividend \\((D^c)\\) = \\(D^c \\sim \\mathcal{N}(0, 36)\\) and \\(p(D) = 0.8\\).\nBayes theorem:\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} P(X|D)}{P(D) \\cdot{} P(X|D) + P(D^c) \\cdot{} P(X|D^c)} \\\\\n\\end{align}\\]\nSubstitute the Gaussian likelihoods into Bayes theorem\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nFactor out \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) from the numerator and denominator, which cancels out:\n\\[\\begin{align}\nP(D|X)  &= \\frac{P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nSubstitute the given probabilities and means:\n\\[\\begin{align}\nP(D|X)  &= \\frac{0.8 \\times e^{-(4-10)^2/(2 \\times 36)}}\n               {0.8 \\times e^{-(4-10)^2/(2 \\times 36)} + 0.2 \\times e^{-(4-0)^2/(2 \\times 36)}} \\\\\n       &= \\frac{0.8 \\cdot e^{-1/2}}{0.8 \\cdot e^{-1/2} + 0.2 \\cdot e^{-2/9}} \\\\\n\\end{align}\\]\n\nexp(-0.5) * 0.8 / (exp(-0.5) * 0.8 + exp(-2/9) * 0.2)\n\n[1] 0.7518525\n\n\n\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\n\n\nLogistic regression will perform better because it has a lower test error rate. For \\(K = 1\\), the training error rate is always zero because the closest point is always the training point itself, so the model will never make a mistake on the training set. Given that the average error rate for 1-NN is 18%, this implies a test error rate of 36%. Logistic regression, with a test error rate of 30%, is therefore the better choice.\n\n\nThis problem has to do with odds.\n\n\n\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\n\\[\n\\frac{p(x)}{1 - P(x)} = odd\\\\\n\\] \\[\\begin{equation}\np(x) = \\frac{odd}{1 + odd}\\\\\n\\end{equation}\\]\n\\[\\begin{equation}\np(x)  = \\frac{0.37}{1 + 0.37} = 0.27\n\\end{equation}\\]\n\nSuppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?\n\n\\[0.16 / (1 - 0.16)  = 0.19\\]\n\nTo Do\nTo Do\nSuppose that you wish to classify an observation \\(X \\in \\mathbb{R}\\) into apples and oranges. You fit a logistic regression model and find that\n\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\beta_0 + \\hat\\beta_1x)}{1 + \\exp(\\hat\\beta_0 + \\hat\\beta_1x)}\n\\]\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x)}\n{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x) + \\exp(\\hat\\alpha_{apple0} + \\hat\\alpha_{apple1}x)}\n\\]\n\nWhat is the log odds of orange versus apple in your model?\n\n\nThe log odds (\\(\\frac{p(x)}{1 - P(x)}\\)) in my model will be \\(\\hat\\beta_0 + \\hat\\beta_1x\\)\n\n\nWhat is the log odds of orange versus apple in your friend’s model?\n\nLog odds of our friend model:\n\\[\n(\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x\n\\]\n\nSuppose that in your model, \\(\\hat\\beta_0 = 2\\) and \\(\\hat\\beta = −1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\nThe coefficient estimate in my friend’s model:\n\n\\(\\hat\\alpha_{orange0} -\\hat\\alpha_{apple0} = 2\\)\n\\(\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1} = -1\\).\n\n\nNow suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat\\alpha_{orange0} = 1.2\\), \\(\\hat\\alpha_{orange1} = −2\\), \\(\\hat\\alpha_{apple0} = 3\\), \\(\\hat\\alpha_{apple1} = 0.6\\). What are the coefficient estimates in your model?\n\nThe coefficients in my model would be \\(\\hat\\beta_0 = 1.2 - 3 = -1.8\\) and \\(\\hat\\beta_1 = -2 - 0.6 = -2.6\\)\n\nFinally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\n\nThey should agree everytime (i.e, pridictions and log odd between any pair of classes will remain the same, regardless of coding). The cofficient will be different because of the choice of baseline.\n\n\n\n\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n\n\n\nProduce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n\n\nlibrary(ISLR2) # data sets\nlibrary(tidyverse)\nlibrary(psych) # for correlation plots\nlibrary(MASS) # for lda() and qda()\nlibrary(class) # for knn()\nlibrary(e1071) # for naiveBayes()\ndata(\"Weekly\")\nattach(Weekly)\n\n\nstr(Weekly)\n\n'data.frame':   1089 obs. of  9 variables:\n $ Year     : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...\n $ Lag1     : num  0.816 -0.27 -2.576 3.514 0.712 ...\n $ Lag2     : num  1.572 0.816 -0.27 -2.576 3.514 ...\n $ Lag3     : num  -3.936 1.572 0.816 -0.27 -2.576 ...\n $ Lag4     : num  -0.229 -3.936 1.572 0.816 -0.27 ...\n $ Lag5     : num  -3.484 -0.229 -3.936 1.572 0.816 ...\n $ Volume   : num  0.155 0.149 0.16 0.162 0.154 ...\n $ Today    : num  -0.27 -2.576 3.514 0.712 1.178 ...\n $ Direction: Factor w/ 2 levels \"Down\",\"Up\": 1 1 2 2 2 1 2 2 2 1 ...\n\nWeekly$Year &lt;- as.factor(Weekly$Year)\n\npairs.panels(Weekly[1:8], cex.labels = 1, ellipses = FALSE)\n\n\n\n\n\n\n\n# total number of times the market had a positive or negative return\ntable(Weekly$Direction)\n\n\nDown   Up \n 484  605 \n\n\n\nThere is a strong positive correlation between the volume of shares traded and the year. From 2005 to 2010, the volume of shares began increasing exponentially. Additionally, between 1990 and 2010, the market had a total of 605 positive returns and 484 negative returns, respectively.\n\n\nUse the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\n\nglm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n               family = binomial)\n\nsummary(glm.fit)$coef\n\n               Estimate Std. Error    z value    Pr(&gt;|z|)\n(Intercept)  0.26686414 0.08592961  3.1056134 0.001898848\nLag1        -0.04126894 0.02641026 -1.5626099 0.118144368\nLag2         0.05844168 0.02686499  2.1753839 0.029601361\nLag3        -0.01606114 0.02666299 -0.6023760 0.546923890\nLag4        -0.02779021 0.02646332 -1.0501409 0.293653342\nLag5        -0.01447206 0.02638478 -0.5485006 0.583348244\nVolume      -0.02274153 0.03689812 -0.6163330 0.537674762\n\n\nAmong all the predictors, only Lag2 is statistically significant.\n\nCompute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n\n\nglm.prob &lt;- predict(glm.fit, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down   54  48\n    Up    430 557\n\n# training error rate:\nmean(glm.pred != Direction)\n\n[1] 0.4389348\n\n\n\nThe logistic regression model have an error rate of 43.8%. The logistic regression model have underwhelming p-values and perhaps removing variables that are not significant (useful) may improve the model. The model is also overly optimistic because the model was train and tested on the same data.\n\n\nNow fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n\n\ntrain.data &lt;- Weekly[Year &lt; 2009, ]\ntest.data &lt;- Weekly[Year &gt; 2008, ]\n\nglm.fit &lt;- glm(Direction ~ Lag2, family = binomial,\n               data = train.data)\n\nsummary(glm.fit)$coef\n\n              Estimate Std. Error  z value   Pr(&gt;|z|)\n(Intercept) 0.20325743 0.06428036 3.162046 0.00156665\nLag2        0.05809527 0.02870446 2.023911 0.04297934\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    9  5\n    Up     34 56\n\nmean(glm.pred == test.data$Direction)\n\n[1] 0.625\n\n# Test error rate\nmean(glm.pred != test.data$Direction)\n\n[1] 0.375\n\n\n\nThe model improved with Lag2 as the only the predictor. Test error rate is 37.5% which is better than random guessing.\n\n\nRepeat (d) using LDA.\n\n\n# lda() is part of the MASS library\nlda.fit &lt;- lda(Direction ~ Lag2, data = train.data)\n\nplot(lda.fit)\n\n\n\n\n\n\n\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    9  5\n  Up     34 56\n\n# test error rate\nmean(lda.pred$class != test.data$Direction)\n\n[1] 0.375\n\n\n\nRepeat (d) using QDA.\n\n\nqda.fit &lt;- qda(Direction ~ Lag2, data = train.data)\n\nqda.pred &lt;- predict(qda.fit, newdata = test.data)\n\ntable(qda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    0  0\n  Up     43 61\n\n# test error rate\nmean(qda.pred$class != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nRepeat (d) using KNN with \\(K = 1\\).\n\n\ntrain &lt;- (Year &lt; 2009) \n\ntrain.Direction &lt;- Weekly$Direction[train] # vector for the train class labels\ntest.Direction &lt;- Weekly$Direction[!train] \n\n# knn() is part of the 'Class' library\nset.seed(1)\nknn.pred &lt;- knn(Weekly[train, \"Lag2\", drop = FALSE],\n                Weekly[!train, \"Lag2\", drop = FALSE], \n                train.Direction, k = 1)\n\n\ntable(knn.pred, test.Direction)\n\n        test.Direction\nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# test error rate\nmean(knn.pred != test.Direction)\n\n[1] 0.5\n\n\n\nRepeat (d) using naive Bayes.\n\n\nnb.fit &lt;- naiveBayes(Direction ~ Lag2, data = train.data)\n\nnb.pred &lt;- predict(nb.fit, test.data)\n\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    0  0\n   Up     43 61\n\n# test error rate\nmean(nb.pred != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nWhich of these methods appears to provide the best results on this data?\n\n\nLogistic regression and linear discriminant analysis methods provide the best results on this data.\n\n\nExperiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for \\(K\\) in the KNN classifier.\n\n\n# Logistic Regression\nglm.fit &lt;- glm(Direction ~ Lag2 + Lag3 + Lag4,\n               data = train.data,\n               family = binomial)\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    8  5\n    Up     35 56\n\ncat(\"Logistic error rate: \", mean(glm.pred != test.data$Direction))\n\nLogistic error rate:  0.3846154\n\n# Linear Discreminant analysis\nlda.fit &lt;- lda(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    8  5\n  Up     35 56\n\ncat(\"LDA error rate: \", mean(lda.pred$class != test.data$Direction))\n\nLDA error rate:  0.3846154\n\n# Naiv Bayes\nnb.fit &lt;- naiveBayes(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nnb.pred &lt;- predict(nb.fit, test.data)\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    7 10\n   Up     36 51\n\ncat(\"Naive Bayes error rate: \", mean(nb.pred != test.data$Direction))\n\nNaive Bayes error rate:  0.4423077\n\n# KNN\n\n# Calculate error rates for each k\nerror_rates &lt;- sapply(1:100, function(k) {\n  train &lt;- (Year &lt; 2009)\n  set.seed(1)\n  \n  knn.pred &lt;- knn(Weekly[train, 2:4, drop = FALSE],\n                  Weekly[!train, 2:4, drop = FALSE], \n                  Weekly$Direction[train], k = k)\n  \n  error_rate &lt;- mean(knn.pred != test.Direction)\n  return(error_rate)\n})\n\n# Find the k with the minimum error rate\nmin_error_rate &lt;- min(error_rates)\nbest_k &lt;- which.min(error_rates)\ntable(knn.pred, Weekly$Direction[!train])\n\n        \nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# Print the result\ncat(\"The best k is:\", best_k, \"with an error rate of:\", min_error_rate, \"\\n\")\n\nThe best k is: 26 with an error rate of: 0.3365385 \n\n\n\nAfter trying different classification methods, KNN with a \\(k = 26\\) has the lowest test error rate."
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html#exercises",
    "href": "posts/2024-11-14-statistical-learning/index.html#exercises",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\nThe logistic function is define as\n\\[\np(X) = \\frac{e^{\\beta_o + \\beta_1X}}{1\\;+ e^{\\beta_o + \\beta_1X}}\\\\\n\\tag{1}\\]\nRearranging terms,\n\\[\\begin{gather*}\ne^{\\beta_o + \\beta_1X} = p(X) + p(X)\\cdot{ e^{\\beta_o + \\beta_1X}}\\\\\n\np(X) = e^{\\beta_o + \\beta_1X} - p(X) \\cdot e^{\\beta_o + \\beta_1X}\\\\\n\ne^{\\beta_o + \\beta_1X} \\cdot [1 - p(X)] = p(X)\\\\\n\n\\end{gather*}\\]\nTherefore the logit is representated as\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_o + \\beta_1X}\n\\tag{2}\\]\n\nIt was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(µk,σ2) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n\n\\[\np_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2)}\n              {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2)}\n\\tag{3}\\]\nThe discriminant function is\n\\[\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma_2} + \\log(\\pi_k)\n\\tag{4}\\]\nTo show Equation 3 is equals to Equation 4, we first assume \\(\\sigma_1^2\\;=\\;...\\;=\\sigma_k^2\\). Hence,\n\\[\np_k(x) = \\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}\n              {\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)}\n\\]\nNext, we take the \\(\\log(p_K(X))\\) to linearized the function. \\[\n\\log(p_k(x)) = \\log(\\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}{\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)})\n\\] Rearranging terms\n\\[\n\\log(p_k(x)) = \\log(\\pi_k) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 -\n              \\log\\left(\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)\\right)\n\\]\nTo maximize over \\(k\\), any term that’s independent of \\(k\\) is ignored\n\\[\\begin{align}\nf &= \\log(\\pi_k) - \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\\\\n  &= \\log(\\pi_k) - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\\\\n\\end{align}\\]\nSince \\(\\frac{x^2}{2\\sigma^2}\\) is also independent of \\(k\\), it is ignored in order to maximize over \\(k\\)\n\\[\n\\log(\\pi_k) + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}\n\\]\n\nTo Do\nWe now examine the differences between LDA and QDA.\n\n\n\nIf the bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nQDA been the more flexible model will perform better on the training set, but worse on the test set. On the test set, QDA will overfit the data because the true decision boundary is linear.\n\n\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nIn this situation since the decision boundary is non-linear, QDA will perform better in both data sets. A linear model will underfit in this case.\n\n\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n\n\nAs sample size increase, QDA will improve because there is more data to fit and the low bias will offset increase in variance.\n\n\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n\n\nFalse. In this case, QDA will overfit the data.\n\n\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat\\beta_0 = -6\\), \\(\\hat\\beta_1 = 0.05\\), \\(\\hat\\beta_2 = 1\\).\n\n(a). Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.\n\\[\np(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}}\n\\]\nwhen \\(X_1 = 40\\) and \\(X_2 = 3.5\\), \\(p(X) = 0.38\\)\n\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n\n\\[\\begin{gather}\n\n\\log\\left(\\frac{p(X)}{1-p(x)}\\right) = -6 + 0.05X_1 + X_2 \\\\\n\n\\log\\left(\\frac{0.5}{1-0.5}\\right) = -6 + 0.05X_1 + 3.5 \\\\\n\n\\end{gather}\\]\nTherefore, solving the equation \\(0 = −6 + 0.05X_1 + 3.5\\), \\(X_1 = 50\\) hours.\n\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of \\(X\\) for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year.\n\nHint: Recall that the density function for a normal random variable is \\(f(x) =\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\\). You will need to use Bayes’ theorem.\nValue of companies issuing a dividend (D) = \\(D \\sim \\mathcal{N}(10, 36)\\).\nValue \\(v\\) for companies not issuing a dividend \\((D^c)\\) = \\(D^c \\sim \\mathcal{N}(0, 36)\\) and \\(p(D) = 0.8\\).\nBayes theorem:\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} P(X|D)}{P(D) \\cdot{} P(X|D) + P(D^c) \\cdot{} P(X|D^c)} \\\\\n\\end{align}\\]\nSubstitute the Gaussian likelihoods into Bayes theorem\n\\[\\begin{align}\nP(D|X) &= \\frac{P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nFactor out \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) from the numerator and denominator, which cancels out:\n\\[\\begin{align}\nP(D|X)  &= \\frac{P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2}}\n               {P(D) \\cdot{} e^{-(x-\\mu_D)^2/2\\sigma^2} +\n                P(D^c) \\cdot{} e^{-(x-\\mu_{D^c})^2/2\\sigma^2}} \\\\\n\\end{align}\\]\nSubstitute the given probabilities and means:\n\\[\\begin{align}\nP(D|X)  &= \\frac{0.8 \\times e^{-(4-10)^2/(2 \\times 36)}}\n               {0.8 \\times e^{-(4-10)^2/(2 \\times 36)} + 0.2 \\times e^{-(4-0)^2/(2 \\times 36)}} \\\\\n       &= \\frac{0.8 \\cdot e^{-1/2}}{0.8 \\cdot e^{-1/2} + 0.2 \\cdot e^{-2/9}} \\\\\n\\end{align}\\]\n\nexp(-0.5) * 0.8 / (exp(-0.5) * 0.8 + exp(-2/9) * 0.2)\n\n[1] 0.7518525\n\n\n\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\n\n\nLogistic regression will perform better because it has a lower test error rate. For \\(K = 1\\), the training error rate is always zero because the closest point is always the training point itself, so the model will never make a mistake on the training set. Given that the average error rate for 1-NN is 18%, this implies a test error rate of 36%. Logistic regression, with a test error rate of 30%, is therefore the better choice.\n\n\nThis problem has to do with odds.\n\n\n\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\n\\[\n\\frac{p(x)}{1 - P(x)} = odd\\\\\n\\] \\[\\begin{equation}\np(x) = \\frac{odd}{1 + odd}\\\\\n\\end{equation}\\]\n\\[\\begin{equation}\np(x)  = \\frac{0.37}{1 + 0.37} = 0.27\n\\end{equation}\\]\n\nSuppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?\n\n\\[0.16 / (1 - 0.16)  = 0.19\\]\n\nTo Do\nTo Do\nSuppose that you wish to classify an observation \\(X \\in \\mathbb{R}\\) into apples and oranges. You fit a logistic regression model and find that\n\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\beta_0 + \\hat\\beta_1x)}{1 + \\exp(\\hat\\beta_0 + \\hat\\beta_1x)}\n\\]\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x)}\n{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x) + \\exp(\\hat\\alpha_{apple0} + \\hat\\alpha_{apple1}x)}\n\\]\n\nWhat is the log odds of orange versus apple in your model?\n\n\nThe log odds (\\(\\frac{p(x)}{1 - P(x)}\\)) in my model will be \\(\\hat\\beta_0 + \\hat\\beta_1x\\)\n\n\nWhat is the log odds of orange versus apple in your friend’s model?\n\nLog odds of our friend model:\n\\[\n(\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x\n\\]\n\nSuppose that in your model, \\(\\hat\\beta_0 = 2\\) and \\(\\hat\\beta = −1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\nThe coefficient estimate in my friend’s model:\n\n\\(\\hat\\alpha_{orange0} -\\hat\\alpha_{apple0} = 2\\)\n\\(\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1} = -1\\).\n\n\nNow suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat\\alpha_{orange0} = 1.2\\), \\(\\hat\\alpha_{orange1} = −2\\), \\(\\hat\\alpha_{apple0} = 3\\), \\(\\hat\\alpha_{apple1} = 0.6\\). What are the coefficient estimates in your model?\n\nThe coefficients in my model would be \\(\\hat\\beta_0 = 1.2 - 3 = -1.8\\) and \\(\\hat\\beta_1 = -2 - 0.6 = -2.6\\)\n\nFinally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\n\nThey should agree everytime (i.e, pridictions and log odd between any pair of classes will remain the same, regardless of coding). The cofficient will be different because of the choice of baseline.\n\n\n\n\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n\n\n\nProduce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n\n\nlibrary(ISLR2) # data sets\nlibrary(tidyverse)\nlibrary(psych) # for correlation plots\nlibrary(MASS) # for lda() and qda()\nlibrary(class) # for knn()\nlibrary(e1071) # for naiveBayes()\ndata(\"Weekly\")\nattach(Weekly)\n\n\nstr(Weekly)\n\n'data.frame':   1089 obs. of  9 variables:\n $ Year     : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...\n $ Lag1     : num  0.816 -0.27 -2.576 3.514 0.712 ...\n $ Lag2     : num  1.572 0.816 -0.27 -2.576 3.514 ...\n $ Lag3     : num  -3.936 1.572 0.816 -0.27 -2.576 ...\n $ Lag4     : num  -0.229 -3.936 1.572 0.816 -0.27 ...\n $ Lag5     : num  -3.484 -0.229 -3.936 1.572 0.816 ...\n $ Volume   : num  0.155 0.149 0.16 0.162 0.154 ...\n $ Today    : num  -0.27 -2.576 3.514 0.712 1.178 ...\n $ Direction: Factor w/ 2 levels \"Down\",\"Up\": 1 1 2 2 2 1 2 2 2 1 ...\n\nWeekly$Year &lt;- as.factor(Weekly$Year)\n\npairs.panels(Weekly[1:8], cex.labels = 1, ellipses = FALSE)\n\n\n\n\n\n\n\n# total number of times the market had a positive or negative return\ntable(Weekly$Direction)\n\n\nDown   Up \n 484  605 \n\n\n\nThere is a strong positive correlation between the volume of shares traded and the year. From 2005 to 2010, the volume of shares began increasing exponentially. Additionally, between 1990 and 2010, the market had a total of 605 positive returns and 484 negative returns, respectively.\n\n\nUse the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\n\nglm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n               family = binomial)\n\nsummary(glm.fit)$coef\n\n               Estimate Std. Error    z value    Pr(&gt;|z|)\n(Intercept)  0.26686414 0.08592961  3.1056134 0.001898848\nLag1        -0.04126894 0.02641026 -1.5626099 0.118144368\nLag2         0.05844168 0.02686499  2.1753839 0.029601361\nLag3        -0.01606114 0.02666299 -0.6023760 0.546923890\nLag4        -0.02779021 0.02646332 -1.0501409 0.293653342\nLag5        -0.01447206 0.02638478 -0.5485006 0.583348244\nVolume      -0.02274153 0.03689812 -0.6163330 0.537674762\n\n\nAmong all the predictors, only Lag2 is statistically significant.\n\nCompute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n\n\nglm.prob &lt;- predict(glm.fit, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down   54  48\n    Up    430 557\n\n# training error rate:\nmean(glm.pred != Direction)\n\n[1] 0.4389348\n\n\n\nThe logistic regression model have an error rate of 43.8%. The logistic regression model have underwhelming p-values and perhaps removing variables that are not significant (useful) may improve the model. The model is also overly optimistic because the model was train and tested on the same data.\n\n\nNow fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n\n\ntrain.data &lt;- Weekly[Year &lt; 2009, ]\ntest.data &lt;- Weekly[Year &gt; 2008, ]\n\nglm.fit &lt;- glm(Direction ~ Lag2, family = binomial,\n               data = train.data)\n\nsummary(glm.fit)$coef\n\n              Estimate Std. Error  z value   Pr(&gt;|z|)\n(Intercept) 0.20325743 0.06428036 3.162046 0.00156665\nLag2        0.05809527 0.02870446 2.023911 0.04297934\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    9  5\n    Up     34 56\n\nmean(glm.pred == test.data$Direction)\n\n[1] 0.625\n\n# Test error rate\nmean(glm.pred != test.data$Direction)\n\n[1] 0.375\n\n\n\nThe model improved with Lag2 as the only the predictor. Test error rate is 37.5% which is better than random guessing.\n\n\nRepeat (d) using LDA.\n\n\n# lda() is part of the MASS library\nlda.fit &lt;- lda(Direction ~ Lag2, data = train.data)\n\nplot(lda.fit)\n\n\n\n\n\n\n\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    9  5\n  Up     34 56\n\n# test error rate\nmean(lda.pred$class != test.data$Direction)\n\n[1] 0.375\n\n\n\nRepeat (d) using QDA.\n\n\nqda.fit &lt;- qda(Direction ~ Lag2, data = train.data)\n\nqda.pred &lt;- predict(qda.fit, newdata = test.data)\n\ntable(qda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    0  0\n  Up     43 61\n\n# test error rate\nmean(qda.pred$class != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nRepeat (d) using KNN with \\(K = 1\\).\n\n\ntrain &lt;- (Year &lt; 2009) \n\ntrain.Direction &lt;- Weekly$Direction[train] # vector for the train class labels\ntest.Direction &lt;- Weekly$Direction[!train] \n\n# knn() is part of the 'Class' library\nset.seed(1)\nknn.pred &lt;- knn(Weekly[train, \"Lag2\", drop = FALSE],\n                Weekly[!train, \"Lag2\", drop = FALSE], \n                train.Direction, k = 1)\n\n\ntable(knn.pred, test.Direction)\n\n        test.Direction\nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# test error rate\nmean(knn.pred != test.Direction)\n\n[1] 0.5\n\n\n\nRepeat (d) using naive Bayes.\n\n\nnb.fit &lt;- naiveBayes(Direction ~ Lag2, data = train.data)\n\nnb.pred &lt;- predict(nb.fit, test.data)\n\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    0  0\n   Up     43 61\n\n# test error rate\nmean(nb.pred != test.data$Direction)\n\n[1] 0.4134615\n\n\n\nWhich of these methods appears to provide the best results on this data?\n\n\nLogistic regression and linear discriminant analysis methods provide the best results on this data.\n\n\nExperiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for \\(K\\) in the KNN classifier.\n\n\n# Logistic Regression\nglm.fit &lt;- glm(Direction ~ Lag2 + Lag3 + Lag4,\n               data = train.data,\n               family = binomial)\n\nglm.prob &lt;- predict(glm.fit, newdata = test.data, type = \"response\")\nglm.pred &lt;- ifelse(glm.prob &gt; 0.5, \"Up\", \"Down\")\n\ntable(glm.pred, test.data$Direction)\n\n        \nglm.pred Down Up\n    Down    8  5\n    Up     35 56\n\ncat(\"Logistic error rate: \", mean(glm.pred != test.data$Direction))\n\nLogistic error rate:  0.3846154\n\n# Linear Discreminant analysis\nlda.fit &lt;- lda(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nlda.pred &lt;- predict(lda.fit, newdata = test.data)\n\ntable(lda.pred$class, test.data$Direction)\n\n      \n       Down Up\n  Down    8  5\n  Up     35 56\n\ncat(\"LDA error rate: \", mean(lda.pred$class != test.data$Direction))\n\nLDA error rate:  0.3846154\n\n# Naiv Bayes\nnb.fit &lt;- naiveBayes(Direction ~ Lag2 + Lag3 + Lag4, data = train.data)\nnb.pred &lt;- predict(nb.fit, test.data)\n\ntable(nb.pred, test.data$Direction)\n\n       \nnb.pred Down Up\n   Down    7 10\n   Up     36 51\n\ncat(\"Naive Bayes error rate: \", mean(nb.pred != test.data$Direction))\n\nNaive Bayes error rate:  0.4423077\n\n# KNN\n\n# Calculate error rates for each k\nerror_rates &lt;- sapply(1:100, function(k) {\n  train &lt;- (Year &lt; 2009)\n  set.seed(1)\n  \n  knn.pred &lt;- knn(Weekly[train, 2:4, drop = FALSE],\n                  Weekly[!train, 2:4, drop = FALSE], \n                  Weekly$Direction[train], k = k)\n  \n  error_rate &lt;- mean(knn.pred != test.Direction)\n  return(error_rate)\n})\n\n# Find the k with the minimum error rate\nmin_error_rate &lt;- min(error_rates)\nbest_k &lt;- which.min(error_rates)\ntable(knn.pred, Weekly$Direction[!train])\n\n        \nknn.pred Down Up\n    Down   21 30\n    Up     22 31\n\n# Print the result\ncat(\"The best k is:\", best_k, \"with an error rate of:\", min_error_rate, \"\\n\")\n\nThe best k is: 26 with an error rate of: 0.3365385 \n\n\n\nAfter trying different classification methods, KNN with a \\(k = 26\\) has the lowest test error rate."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html",
    "href": "posts/jku-thesis-template/index.html",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "",
    "text": "This template is based on the Masters/Doctoral Thesis, LaTeX Template, Version 2.5 (27 Aug 2017), which Eli Holmes adapted for Quarto. I further modified the Quarto template to fulfill JKU’s formal criteria for diploma theses.\nQuarto offers several advantages over traditional word processors like Microsoft Word for writing long-form documents such as theses. For example, Quarto enhances reproducibility by seamlessly integrating data analysis and documentation. It ensures consistent formatting throughout the document and supports version control tools like Git. This allows you to focus entirely on the content of your thesis while the template takes care of the technical requirements of formatting and organization."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#clone-the-template",
    "href": "posts/jku-thesis-template/index.html#clone-the-template",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "1. Clone the Template",
    "text": "1. Clone the Template\nNavigate to the directory where you want to create your thesis files. Run the following command in a terminal to download the template and its associated folders:\n\nquarto use template jallow-code/jku-quarto-thesis\n\nYou will be prompted to specify an empty directory name for the template files. Provide a name for the new directory, which will store your thesis files."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#render-the-template",
    "href": "posts/jku-thesis-template/index.html#render-the-template",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "2. Render the Template",
    "text": "2. Render the Template\nOnce the template is installed, change into the new directory:\n\ncd &lt;new-directory-name&gt;\n\nTo generate the document, use the command:\n\nquarto render\n\n\nTroubleshooting Errors\nIf you encounter the following error during rendering:\n\nERROR: \ncompilation failed - missing packages (automatic install disabled)\nLaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H &lt;return&gt;  for immediate help.\n ...\nl.872 \\end{CSLReferences}\nThis error is likely due to outdated versions of Quarto or TinyTeX. To resolve it:\n\nUpdate your TinyTex installation.\nEnsure you are using the latest version of Quarto."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#thesis-type-and-frontmatter",
    "href": "posts/jku-thesis-template/index.html#thesis-type-and-frontmatter",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Thesis Type and Frontmatter",
    "text": "Thesis Type and Frontmatter\nThe template is configured for a Bachelor’s thesis by default. To change this, edit the _quarto.yml file. For example:\n\nthesis:\n  type: Bachelor Thesis # Or \"Master Thesis\", \"Doctoral Dissertation\", etc.\n  degree-name: Bachelor of Science\n  group: Biological Chemistry # name of your study program\n  supervisor:\n    primary: \"Assoz. Univ.-Prof. Dr. Ian Teasdale\"\n    co: \"Dipl.-Ing. Michael Kneidinger\"\n  university: Johannes Kepler Universität Linz\n  department: Institute of Polymer Chemistry\n  faculty: Faculty of Engineering and Natural Sciences\nYou can also customize elements of the front matter (e.g., acknowledgments, dedication) by commenting out or modifying the respective sections in_quarto.yml. For instance, to exclude the dedication page:\n\n# dedication: \"Frontmatter/dedication.tex\""
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#adding-symbols-or-abbreviations",
    "href": "posts/jku-thesis-template/index.html#adding-symbols-or-abbreviations",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Adding Symbols or Abbreviations",
    "text": "Adding Symbols or Abbreviations\nThe Frontmatter folder contains LaTeX files for abbreviations, symbols, and other sections. If needed, search online for LaTeX code examples to add symbols or format abbreviations."
  }
]