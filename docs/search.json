[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU\n\n\n\nResources\n\n\n\nI’m sharing a Quarto template I customized for my bachelor’s thesis at Johannes Kepler University Linz.\n\n\n\nAbdoulie Jallow\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\n\nLearn\n\n\nR\n\n\n\nIn this blog post, I’ll share my solutions to exercises from An Introduction to Statistical Learning by Gareth M. James, Daniela Witten, Trevor Hastie, and Robert…\n\n\n\nAbdoulie Jallow\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html",
    "href": "posts/2024-11-14-statistical-learning/index.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "The association between predictors (\\(X = X_1, X_2,...,X_p\\)) and a quantitative response (Y) can be expressed as \\(Y = f(X) + \\epsilon\\). Where \\(f\\) is some fixed unknown function, and \\(\\epsilon\\) is a random error term with a zero mean. Statistical learning refers to a large collection of tools for estimating \\(f\\). These tools can be classified either supervised or unsupervised. Supervised learning involves estimating an output based on one or more inputs. In contrast, unsupervised learning involves inputs but no supervising outputs. There are two primary reasons for predicting \\(f\\):\n\nPrediction: This applies to situations were inputs (X) are readily available, but the output (Y) can not be easily obtained. \\(Y\\) can be estimated using \\(\\hat{Y} = \\hat{f}(X)\\) where \\(\\hat{f}\\) is our estimate for the true function \\(f\\). In this setting, one is not typically concerned with the exact form of \\(\\hat{f}\\) given it yields accurate predictions for \\(Y\\). The accuracy of \\(\\hat{f}\\) depends on two quantities known as reducible error and Irreducible error. The reducible error can be improved by using an appropriate statistical learning method to estimate \\(f\\). The Irreducible error is the variability associated with the error term (\\(Var(\\epsilon)\\)). Since \\(Y\\) also depends on \\(\\epsilon\\), it is impossible to achieve a perfect estimate of \\(Y\\). The irreducible error will always be greater than zero, as it may include unmeasured variables or unmeasurable variations influencing \\(Y\\).\nInference: In this setting we’re interested in the association between \\(Y\\) and \\(X\\) hence the exact form of \\(\\hat{f}\\) is very important. One may be interested in answering the following questions: Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\n\\(f\\) can be estimated using either parametric or non-parametric methods. parametric methods involve first assuming the shape of \\(f\\) (e.g., linear) and then using training data to fit or train the model. These methods are less flexible but more interpretable. Non-parametric methods do not make assumptions about the shape of \\(f\\), allowing for a wider range of possible forms. However, they tend to be less interpretable. Variables can be characterized as either quantitative or qualitative (also known as categorical). Problems with a quantitative response are often referred to as regression problems, while those involving a qualitative response are often referred to as classification problems. In regression settings the measure for the accuracy of a model is given by the mean squared error (MSE) . We choose a statistical learning method that gives the lowest test MSE. The test MSE depends on variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\), and var(\\(\\epsilon\\)). To achieve a low test MSE, one needs to choose a model that simultaneously results in low variance and low bias. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases.\nIn classification settings, model accuracy is evaluated by the proportion of incorrect predictions made when applying the estimated function \\(\\hat{f}\\) to a set of test observations. This proportion is referred to as the test error rate. A good classifier minimizes the test error rate, achieving the highest possible accuracy on unseen data. The Bayes classifier is an approach in classification that minimizes the test error rate by assigning each observation to the class with the highest conditional probability, \\(\\Pr(Y = j | X = x_0)\\). The decision boundary separating classes is called the Bayes decision boundary. This classifier achieves the lowest possible error rate, known as the Bayes error rate, which arises due to overlapping classes in the data. However, because the true conditional probabilities are generally unknown, the Bayes classifier serves as a theoretical benchmark. In practice, methods like k-Nearest Neighbors (KNN) estimate these probabilities by considering the closest \\(K\\) training points to a test observation. KNN assigns the class with the majority vote among these neighbors. The flexibility of KNN depends on \\(K\\); small \\(K\\) values result in high variance but low bias, while large \\(K\\) values lead to high bias but low variance. The right balance is important, as overly flexible models overfit the training data, and less flexible ones underfit. The tradeoff is illustrated by the characteristic U-shape of the test error as flexibility increases.\n\n\n\n\n\n\n\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n\nThe sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small.\n\n\nA flexible method will be better because the model does not need to work hard to find patterns in the data hence reducing the likelihood of overfitting. Additionally, a flexible method in this situation will results in a lower test mean squared error (MSE), as the reduction in bias compensates for the increased variance.\n\n\nThe number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small.\n\n\nIn this situation an Inflexible method preferable because there is higher chance for the model to pickup patterns that may not exist in the test data.\n\n\nThe relationship between the predictors and response is highly non-linear.\n\n\nA flexible method is preferred because an inflexible method will lead to high bias.\n\n\nThe variance of the error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high.\n\n\nIn this case, an Inflexible method will perform better. With a flexible method, it may be difficult to differentiate the true pattern from the noise in the data. Flexible method have a higher variance because they fit more data.\n\n\n\n\nExplain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide \\(n\\) and \\(p\\).\n\nWe collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n\n\n\nsample size: 500\npredictors: profit, number of employees, and industry (3)\nregression problem because the response is quantitative\nWe are most interested in making inference.\n\n\n\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n\n\n\nsample size: 20\npredictors: 13\nclassification problem because the response is qualitative (success or failure)\nWe are interested in making predictions.\n\n\n\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n\n\n\nsample size: 52\npredictors: 3\nregression problem because the response is quantitative\nWe are interested in making prediction.\n\n\n\n\n\nWe now revisit the bias-variance decomposition.\n\nProvide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.\nExplain why each of the five curves has the shape displayed in part (a).\n\n\n\nSquared Bias: Decreases as model flexibility increases since more flexible methods are better at capturing complex patterns in the data.\nVariance: Increases with greater flexibility, as flexible models are more sensitive to variations in the training data.\nTraining Error: Decreases with increasing flexibility because more complex models can closely fit the training data.\nTest Error: Initially decreases with flexibility due to reduced bias but eventually increases as overfitting leads to higher variance.\nBayes Error: Remains constant, as it is independent of the model’s complexity and reflects the inherent noise in the data.\n\n\n\n\n\nYou will now think of some real-life applications for statistical learning.\n\nDescribe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\nEmail Classification (Spam or Not Spam):\nVariables (\\(p\\)): Certain keywords, email structure, sender information\nAim: Prediction\nFactors Associated with Winning an Election:\nVariables (\\(p\\)): Campaign budget, market trends, poll results, etc.\nAim: Inference\nFood Spoilage (Healthy or Not Healthy for Consumption): Variables (\\(p\\)): Color, texture, smell, etc.\nAim: Prediction\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\nAnalyzing the Effect of Marketing Budget on Sales Response: Total revenue Variables (\\(p\\)): Marketing budget allocated to different channels (e.g., social media, TV ads, print media), time of year, and product price. Aim: Inference\nPredicting Housing Prices Response Variable: Sale price of a house. Predictors: Square footage, number of bedrooms, number of bathrooms, location etc Aim: Prediction\n\n\nDescribe three real-life applications in which cluster analysis might be useful.\n\n\n\nGenomic data analysis for example RNA seq.\nfinding categories in illnesses, cells, or organisms\n\n\n\n\n\nWhat are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?\n\nInflexible methods are more interpretable and are generally preferred in cases with small \\(n\\) (sample size) and large \\(p\\) (number of predictors). However, they may suffer from high bias when the true underlying function is non-linear. Flexible methods, while capable of capturing non-linear patterns, are prone to overfitting, leading to high variance in error. In situations with a large \\(n\\) and small \\(p\\), flexible methods are preferred.\n\n\n\n\nDescribe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?\n\nParametric methods are more interpretable because they rely on a predefined model to generate the response. They also tend to perform better than non-parametric methods in high-dimensional settings, as they require estimating fewer parameters. However, a key disadvantage is that the model may not accurately reflect reality. If the assumed model is far from the true underlying relationship, the resulting estimates can be poor.\nIn contrast, non-parametric methods are highly flexible and can adapt to a wide range of underlying patterns. However, this flexibility comes at a cost: they require a larger number of observations to produce accurate estimates, as they do not rely on a small set of parameters and are more prone to overfitting when data is limited\n\n\n\n\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable.\n\n\n\nObs.\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(Y\\)\n\n\n\n\n1\n0\n3\n0\nRed\n\n\n2\n2\n0\n0\nRed\n\n\n3\n0\n1\n3\nRed\n\n\n4\n0\n1\n2\nGreen\n\n\n5\n-1\n0\n1\nGreen\n\n\n6\n1\n1\n1\nRed\n\n\n\nSuppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X_1 = X_2 = X_3 = 0\\) using \\(K\\)-nearest neighbors.\n\nCompute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\).\n\n\n# Define the data\ndata &lt;- data.frame(\n  Obs = 1:6,\n  X1 = c(0, 2, 0, 0, -1, 1),\n  X2 = c(3, 0, 1, 1, 0, 1),\n  X3 = c(0, 0, 3, 2, 1, 1),\n  Y = c(\"Red\", \"Red\", \"Red\", \"Green\", \"Green\", \"Red\")\n)\n\n# Test point\ntest_point &lt;- c(0, 0, 0)\n\n# Compute Euclidean distances\ndata$Distance &lt;- sqrt((data$X1 - test_point[1])^2 + \n                      (data$X2 - test_point[2])^2 + \n                      (data$X3 - test_point[3])^2)\n\n#Sort data by distance\nsorted_data &lt;- data[order(data$Distance), ]\n\n# Display distances\nprint(sorted_data[, c(\"Obs\", \"Distance\", \"Y\")])\n\n  Obs Distance     Y\n5   5 1.414214 Green\n6   6 1.732051   Red\n2   2 2.000000   Red\n4   4 2.236068 Green\n1   1 3.000000   Red\n3   3 3.162278   Red\n\n\n\nWhat is our prediction with \\(K = 1\\)? Why?\n\n\n# Prediction for K = 1\nk1_prediction &lt;- sorted_data$Y[1]\ncat(\"Prediction for K = 1:\", k1_prediction, \"\\n\")\n\nPrediction for K = 1: Green \n\n\nGreen have the smallest distance (obs. 5)\n\nWhat is our prediction with \\(K = 3\\)? Why?\n\n\n# Prediction for K = 3\nk3_neighbors &lt;- sorted_data$Y[1:3]\nk3_prediction &lt;- names(which.max(table(k3_neighbors))) # table() returns counts \ncat(\"Prediction for K = 3:\", k3_prediction, \"\\n\")\n\nPrediction for K = 3: Red \n\n\n\nIf the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for \\(K\\) to be large or small? Why?\n\n\nIf the Bayes decision boundary is highly non-linear, a smaller \\(k\\) is better. This is because smaller \\(k\\) will capture the non-linear patterns in data. A large \\(k\\) may lead to underfiting since the true boundary is highly non-linear.\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(GGally) # ggpairs function\nlibrary(ISLR2) # Boston data set\n\n\n\nThis exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\nPrivate : Public/private indicator\nApps : Number of applications received\nAccept : Number of applicants accepted\nEnroll : Number of new students enrolled\nTop10perc : New students from top 10% of high school class\nTop25perc : New students from top 25% of high school class\nF.Undergrad : Number of full-time undergraduates\nP.Undergrad : Number of part-time undergraduates\nOutstate : Out-of-state tuition\nRoom.Board : Room and board costs\nBooks : Estimated book costs\nPersonal : Estimated personal spending\nPhD : Percent of faculty with Ph.D.’s\nTerminal : Percent of faculty with terminal degree\nS.F.Ratio : Student/faculty ratio\nperc.alumni : Percent of alumni who donate\nExpend : Instructional expenditure per student\nGrad.Rate : Graduation rate\n\nBefore reading the data into R, it can be viewed in Excel or a text editor.\n\nUse the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\n\n\ncollege &lt;- read.csv(\"College.csv\")\n\n\nLook at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:\n\n\nrownames(college) &lt;- college[, 1]\nView(college)\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try\n\ncollege &lt;- college [, -1]\nView(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n\n\nUse the summary() function to produce a numerical summary of the variables in the data set.\n\n\n\nUse the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10].\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private.\nCreate a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.\n\n\nElite &lt;- rep(\"No\", nrow(college))\nElite[college$Top10perc &gt; 50] &lt;- \"Yes\"\nElite &lt;- as.factor(Elite)\ncollege &lt;- data.frame(college, Elite)\n\nUse the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\nUse the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\n\n\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\ncollege$Private &lt;- college$Private == \"Yes\"\npairs(college[, 1:10], cex = 0.2)\n\n\n\n\n\n\n\nplot(college$Outstate ~ factor(college$Private), xlab = \"Private\", ylab = \"Outstate\")\n\n\n\n\n\n\n\ncollege$Elite &lt;- factor(ifelse(college$Top10perc &gt; 50, \"Yes\", \"No\"))\nsummary(college$Elite)\n\n No Yes \n699  78 \n\nplot(college$Outstate ~ college$Elite, xlab = \"Elite\", ylab = \"Outstate\")\n\n\n\n\n\n\n\npar(mfrow = c(2,2))\nfor (n in c(10, 20, 30, 40)) {\n  hist(college$Enroll, breaks = n, main = paste(\"n =\", n), xlab = \"Enroll\")\n}\n\n\n\n\n\n\n\n\n\n\n\nThis exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n\ndata &lt;- read.table(\"Auto.data\", header = TRUE, na.strings = \"?\")\ndata &lt;- na.omit(data)\n\n\nWhich of the predictors are quantitative, and which are qualitative?\n\n\nQuantitative: mpg,cylinders,displacement,horsepower, weight, acceleration, year. Qualitative: name, origin.\n\n\nWhat is the range of each quantitative predictor? You can answer this using the range() function.\n\n\nsapply(data[ ,1:7], range)\n\n      mpg cylinders displacement horsepower weight acceleration year\n[1,]  9.0         3           68         46   1613          8.0   70\n[2,] 46.6         8          455        230   5140         24.8   82\n\n\n\nWhat is the mean and standard deviation of each quantitative predictor?\n\n\ndata[, 1:7] |&gt;\n  pivot_longer(everything()) |&gt;\n  group_by(name) |&gt;\n  summarise(\n    Mean = mean(value),\n    SD = sd(value)\n  ) |&gt;\n  knitr::kable()\n\n\n\n\nname\nMean\nSD\n\n\n\n\nacceleration\n15.541327\n2.758864\n\n\ncylinders\n5.471939\n1.705783\n\n\ndisplacement\n194.411990\n104.644004\n\n\nhorsepower\n104.469388\n38.491160\n\n\nmpg\n23.445918\n7.805008\n\n\nweight\n2977.584184\n849.402560\n\n\nyear\n75.979592\n3.683737\n\n\n\n\n\n\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\n\ndata[-(10:85), 1:7] |&gt;\n  pivot_longer(everything()) |&gt;\n  group_by(name) |&gt;\n  summarise(\n    Range = diff(range(value)),\n    Mean = mean(value),\n    SD = sd(value)\n  ) |&gt;\n  knitr::kable()\n\n\n\n\nname\nRange\nMean\nSD\n\n\n\n\nacceleration\n16.3\n15.726899\n2.693721\n\n\ncylinders\n5.0\n5.373418\n1.654179\n\n\ndisplacement\n387.0\n187.240506\n99.678367\n\n\nhorsepower\n184.0\n100.721519\n35.708853\n\n\nmpg\n35.6\n24.404430\n7.867283\n\n\nweight\n3348.0\n2935.971519\n811.300208\n\n\nyear\n12.0\n77.145570\n3.106217\n\n\n\n\n\n\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\n\n\nggpairs(data[, 1:7])\n\n\n\n\n\n\n\n\n\nYes, there are both positive and negative correlation between the variables.\n\n\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\nYes, since other variables are correlated. However, horsepower, weight and displacement are highly related.\n\n\n\n\nThis exercise involves the Boston housing data set.\n\nTo begin, load in the Boston data set. The Boston data set is part of the ISLR2 library in R.\n\nNow the data set is contained in the object Boston.\n\nView(Boston)\n\nRead about the data set:\n\n?Boston\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\ndim(Boston)\n\n[1] 506  13\n\n\n\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\n\n\nggpairs(Boston[, 1:6])\n\n\n\n\n\n\n\n\n\nThere are no significant correlations associated with capita crime\n\n\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\nYes, there is mild positive correlation with rad and tax\n\n\ncor_boston &lt;- cor(Boston)\ncor_col &lt;- hcl.colors(33, \"Purple-Green\")\np &lt;- heatmap(cor_boston, col = cor_col, zlim = c(-1, 1), symm = TRUE, margins = c(8, 8), Rowv = NA)\nlegend(\"right\", c(1, rep(\"\", 15), 0, rep(\"\", 15), -1), fill = rev(cor_col), \n  bty = \"n\", border = \"transparent\", y.intersp = 0.5)\n\n\n\n\n\n\n\n\n\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\n\n# top 5% of crime,tax,and pupil-teacher ratios\ncat(\"High crime:\", sum(Boston$crim &gt; quantile(Boston$crim, 0.95)), \"\\n\") \n\nHigh crime: 26 \n\ncat(\"High tax:\", sum(Boston$tax &gt; quantile(Boston$tax, 0.95)), \"\\n\")\n\nHigh tax: 5 \n\ncat(\"Pupil-teacher ratio:\", sum(Boston$ptratio &gt; quantile(Boston$ptratio, 0.95)),\"\\n\")\n\nPupil-teacher ratio: 18 \n\n\n\nHow many of the census tracts in this data set bound the Charles river?\n\n\nsum(Boston$chas==1)\n\n[1] 35\n\n\n\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\n\nmedian_ptratio &lt;- median(Boston$ptratio)\ncat(\"Median pupil-teacher ratio:\", median_ptratio, \"\\n\")\n\nMedian pupil-teacher ratio: 19.05 \n\n\n\nWhich census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.\n\n\n# whichsuburbs that have the lowest median property values.\nwhich(Boston$medv == min(Boston$medv))\n\n# Values of other predictors for suburb 406\nBoston[406,]\n\nrange(Boston$crim)\n\n\nThere are two suburbs that have the lowest median property values. crime is negatively correlated with median property value.\n\n\nIn this data set, how many of the census tract average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n\ncat(\"More than 7 rooms:\",sum(Boston$rm &gt; 7),\"\\n\")\n\nMore than 7 rooms: 64 \n\ncat(\"More than 8 rooms:\", sum(Boston$rm &gt; 8), \"\\n\")\n\nMore than 8 rooms: 13 \n\n\nMaking summaries:\n\ntracts_more_than_8 &lt;- Boston[Boston$rm &gt; 8, ]\n# Reshape data to long format using pivot_longer()\ndata_long &lt;- tracts_more_than_8 |&gt;\n  pivot_longer(cols = -rm, names_to = \"Predictor\", values_to = \"Value\")\n\n# Create boxplots with facet_wrap\nggplot(data_long, aes(x = Predictor, y = Value, fill = Predictor)) +\n  geom_boxplot(outlier.color = \"red\", outlier.size = 1.5) +\n  facet_wrap(~ Predictor, scales = \"free\", ncol = 4) +  # Each plot gets its own scale\n  theme_minimal() +\n  labs(title = \"Boxplots for Predictors with More Than 8 Rooms\",\n       x = \"Predictors\",\n       y = \"Values\") +\n  theme(axis.text.x = element_blank(),  # Remove x-axis text (not meaningful here)\n        axis.ticks.x = element_blank(),\n        strip.text = element_text(size = 10, face = \"bold\"))"
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html#summary",
    "href": "posts/2024-11-14-statistical-learning/index.html#summary",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "The association between predictors (\\(X = X_1, X_2,...,X_p\\)) and a quantitative response (Y) can be expressed as \\(Y = f(X) + \\epsilon\\). Where \\(f\\) is some fixed unknown function, and \\(\\epsilon\\) is a random error term with a zero mean. Statistical learning refers to a large collection of tools for estimating \\(f\\). These tools can be classified either supervised or unsupervised. Supervised learning involves estimating an output based on one or more inputs. In contrast, unsupervised learning involves inputs but no supervising outputs. There are two primary reasons for predicting \\(f\\):\n\nPrediction: This applies to situations were inputs (X) are readily available, but the output (Y) can not be easily obtained. \\(Y\\) can be estimated using \\(\\hat{Y} = \\hat{f}(X)\\) where \\(\\hat{f}\\) is our estimate for the true function \\(f\\). In this setting, one is not typically concerned with the exact form of \\(\\hat{f}\\) given it yields accurate predictions for \\(Y\\). The accuracy of \\(\\hat{f}\\) depends on two quantities known as reducible error and Irreducible error. The reducible error can be improved by using an appropriate statistical learning method to estimate \\(f\\). The Irreducible error is the variability associated with the error term (\\(Var(\\epsilon)\\)). Since \\(Y\\) also depends on \\(\\epsilon\\), it is impossible to achieve a perfect estimate of \\(Y\\). The irreducible error will always be greater than zero, as it may include unmeasured variables or unmeasurable variations influencing \\(Y\\).\nInference: In this setting we’re interested in the association between \\(Y\\) and \\(X\\) hence the exact form of \\(\\hat{f}\\) is very important. One may be interested in answering the following questions: Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\n\\(f\\) can be estimated using either parametric or non-parametric methods. parametric methods involve first assuming the shape of \\(f\\) (e.g., linear) and then using training data to fit or train the model. These methods are less flexible but more interpretable. Non-parametric methods do not make assumptions about the shape of \\(f\\), allowing for a wider range of possible forms. However, they tend to be less interpretable. Variables can be characterized as either quantitative or qualitative (also known as categorical). Problems with a quantitative response are often referred to as regression problems, while those involving a qualitative response are often referred to as classification problems. In regression settings the measure for the accuracy of a model is given by the mean squared error (MSE) . We choose a statistical learning method that gives the lowest test MSE. The test MSE depends on variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\), and var(\\(\\epsilon\\)). To achieve a low test MSE, one needs to choose a model that simultaneously results in low variance and low bias. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases.\nIn classification settings, model accuracy is evaluated by the proportion of incorrect predictions made when applying the estimated function \\(\\hat{f}\\) to a set of test observations. This proportion is referred to as the test error rate. A good classifier minimizes the test error rate, achieving the highest possible accuracy on unseen data. The Bayes classifier is an approach in classification that minimizes the test error rate by assigning each observation to the class with the highest conditional probability, \\(\\Pr(Y = j | X = x_0)\\). The decision boundary separating classes is called the Bayes decision boundary. This classifier achieves the lowest possible error rate, known as the Bayes error rate, which arises due to overlapping classes in the data. However, because the true conditional probabilities are generally unknown, the Bayes classifier serves as a theoretical benchmark. In practice, methods like k-Nearest Neighbors (KNN) estimate these probabilities by considering the closest \\(K\\) training points to a test observation. KNN assigns the class with the majority vote among these neighbors. The flexibility of KNN depends on \\(K\\); small \\(K\\) values result in high variance but low bias, while large \\(K\\) values lead to high bias but low variance. The right balance is important, as overly flexible models overfit the training data, and less flexible ones underfit. The tradeoff is illustrated by the characteristic U-shape of the test error as flexibility increases."
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html#exercises",
    "href": "posts/2024-11-14-statistical-learning/index.html#exercises",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n\nThe sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small.\n\n\nA flexible method will be better because the model does not need to work hard to find patterns in the data hence reducing the likelihood of overfitting. Additionally, a flexible method in this situation will results in a lower test mean squared error (MSE), as the reduction in bias compensates for the increased variance.\n\n\nThe number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small.\n\n\nIn this situation an Inflexible method preferable because there is higher chance for the model to pickup patterns that may not exist in the test data.\n\n\nThe relationship between the predictors and response is highly non-linear.\n\n\nA flexible method is preferred because an inflexible method will lead to high bias.\n\n\nThe variance of the error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high.\n\n\nIn this case, an Inflexible method will perform better. With a flexible method, it may be difficult to differentiate the true pattern from the noise in the data. Flexible method have a higher variance because they fit more data.\n\n\n\n\nExplain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide \\(n\\) and \\(p\\).\n\nWe collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n\n\n\nsample size: 500\npredictors: profit, number of employees, and industry (3)\nregression problem because the response is quantitative\nWe are most interested in making inference.\n\n\n\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n\n\n\nsample size: 20\npredictors: 13\nclassification problem because the response is qualitative (success or failure)\nWe are interested in making predictions.\n\n\n\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n\n\n\nsample size: 52\npredictors: 3\nregression problem because the response is quantitative\nWe are interested in making prediction.\n\n\n\n\n\nWe now revisit the bias-variance decomposition.\n\nProvide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.\nExplain why each of the five curves has the shape displayed in part (a).\n\n\n\nSquared Bias: Decreases as model flexibility increases since more flexible methods are better at capturing complex patterns in the data.\nVariance: Increases with greater flexibility, as flexible models are more sensitive to variations in the training data.\nTraining Error: Decreases with increasing flexibility because more complex models can closely fit the training data.\nTest Error: Initially decreases with flexibility due to reduced bias but eventually increases as overfitting leads to higher variance.\nBayes Error: Remains constant, as it is independent of the model’s complexity and reflects the inherent noise in the data.\n\n\n\n\n\nYou will now think of some real-life applications for statistical learning.\n\nDescribe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\nEmail Classification (Spam or Not Spam):\nVariables (\\(p\\)): Certain keywords, email structure, sender information\nAim: Prediction\nFactors Associated with Winning an Election:\nVariables (\\(p\\)): Campaign budget, market trends, poll results, etc.\nAim: Inference\nFood Spoilage (Healthy or Not Healthy for Consumption): Variables (\\(p\\)): Color, texture, smell, etc.\nAim: Prediction\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\nAnalyzing the Effect of Marketing Budget on Sales Response: Total revenue Variables (\\(p\\)): Marketing budget allocated to different channels (e.g., social media, TV ads, print media), time of year, and product price. Aim: Inference\nPredicting Housing Prices Response Variable: Sale price of a house. Predictors: Square footage, number of bedrooms, number of bathrooms, location etc Aim: Prediction\n\n\nDescribe three real-life applications in which cluster analysis might be useful.\n\n\n\nGenomic data analysis for example RNA seq.\nfinding categories in illnesses, cells, or organisms\n\n\n\n\n\nWhat are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?\n\nInflexible methods are more interpretable and are generally preferred in cases with small \\(n\\) (sample size) and large \\(p\\) (number of predictors). However, they may suffer from high bias when the true underlying function is non-linear. Flexible methods, while capable of capturing non-linear patterns, are prone to overfitting, leading to high variance in error. In situations with a large \\(n\\) and small \\(p\\), flexible methods are preferred.\n\n\n\n\nDescribe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?\n\nParametric methods are more interpretable because they rely on a predefined model to generate the response. They also tend to perform better than non-parametric methods in high-dimensional settings, as they require estimating fewer parameters. However, a key disadvantage is that the model may not accurately reflect reality. If the assumed model is far from the true underlying relationship, the resulting estimates can be poor.\nIn contrast, non-parametric methods are highly flexible and can adapt to a wide range of underlying patterns. However, this flexibility comes at a cost: they require a larger number of observations to produce accurate estimates, as they do not rely on a small set of parameters and are more prone to overfitting when data is limited\n\n\n\n\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable.\n\n\n\nObs.\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(Y\\)\n\n\n\n\n1\n0\n3\n0\nRed\n\n\n2\n2\n0\n0\nRed\n\n\n3\n0\n1\n3\nRed\n\n\n4\n0\n1\n2\nGreen\n\n\n5\n-1\n0\n1\nGreen\n\n\n6\n1\n1\n1\nRed\n\n\n\nSuppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X_1 = X_2 = X_3 = 0\\) using \\(K\\)-nearest neighbors.\n\nCompute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\).\n\n\n# Define the data\ndata &lt;- data.frame(\n  Obs = 1:6,\n  X1 = c(0, 2, 0, 0, -1, 1),\n  X2 = c(3, 0, 1, 1, 0, 1),\n  X3 = c(0, 0, 3, 2, 1, 1),\n  Y = c(\"Red\", \"Red\", \"Red\", \"Green\", \"Green\", \"Red\")\n)\n\n# Test point\ntest_point &lt;- c(0, 0, 0)\n\n# Compute Euclidean distances\ndata$Distance &lt;- sqrt((data$X1 - test_point[1])^2 + \n                      (data$X2 - test_point[2])^2 + \n                      (data$X3 - test_point[3])^2)\n\n#Sort data by distance\nsorted_data &lt;- data[order(data$Distance), ]\n\n# Display distances\nprint(sorted_data[, c(\"Obs\", \"Distance\", \"Y\")])\n\n  Obs Distance     Y\n5   5 1.414214 Green\n6   6 1.732051   Red\n2   2 2.000000   Red\n4   4 2.236068 Green\n1   1 3.000000   Red\n3   3 3.162278   Red\n\n\n\nWhat is our prediction with \\(K = 1\\)? Why?\n\n\n# Prediction for K = 1\nk1_prediction &lt;- sorted_data$Y[1]\ncat(\"Prediction for K = 1:\", k1_prediction, \"\\n\")\n\nPrediction for K = 1: Green \n\n\nGreen have the smallest distance (obs. 5)\n\nWhat is our prediction with \\(K = 3\\)? Why?\n\n\n# Prediction for K = 3\nk3_neighbors &lt;- sorted_data$Y[1:3]\nk3_prediction &lt;- names(which.max(table(k3_neighbors))) # table() returns counts \ncat(\"Prediction for K = 3:\", k3_prediction, \"\\n\")\n\nPrediction for K = 3: Red \n\n\n\nIf the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for \\(K\\) to be large or small? Why?\n\n\nIf the Bayes decision boundary is highly non-linear, a smaller \\(k\\) is better. This is because smaller \\(k\\) will capture the non-linear patterns in data. A large \\(k\\) may lead to underfiting since the true boundary is highly non-linear.\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(GGally) # ggpairs function\nlibrary(ISLR2) # Boston data set\n\n\n\nThis exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\nPrivate : Public/private indicator\nApps : Number of applications received\nAccept : Number of applicants accepted\nEnroll : Number of new students enrolled\nTop10perc : New students from top 10% of high school class\nTop25perc : New students from top 25% of high school class\nF.Undergrad : Number of full-time undergraduates\nP.Undergrad : Number of part-time undergraduates\nOutstate : Out-of-state tuition\nRoom.Board : Room and board costs\nBooks : Estimated book costs\nPersonal : Estimated personal spending\nPhD : Percent of faculty with Ph.D.’s\nTerminal : Percent of faculty with terminal degree\nS.F.Ratio : Student/faculty ratio\nperc.alumni : Percent of alumni who donate\nExpend : Instructional expenditure per student\nGrad.Rate : Graduation rate\n\nBefore reading the data into R, it can be viewed in Excel or a text editor.\n\nUse the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\n\n\ncollege &lt;- read.csv(\"College.csv\")\n\n\nLook at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:\n\n\nrownames(college) &lt;- college[, 1]\nView(college)\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try\n\ncollege &lt;- college [, -1]\nView(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n\n\nUse the summary() function to produce a numerical summary of the variables in the data set.\n\n\n\nUse the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10].\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private.\nCreate a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.\n\n\nElite &lt;- rep(\"No\", nrow(college))\nElite[college$Top10perc &gt; 50] &lt;- \"Yes\"\nElite &lt;- as.factor(Elite)\ncollege &lt;- data.frame(college, Elite)\n\nUse the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\nUse the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\n\n\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\ncollege$Private &lt;- college$Private == \"Yes\"\npairs(college[, 1:10], cex = 0.2)\n\n\n\n\n\n\n\nplot(college$Outstate ~ factor(college$Private), xlab = \"Private\", ylab = \"Outstate\")\n\n\n\n\n\n\n\ncollege$Elite &lt;- factor(ifelse(college$Top10perc &gt; 50, \"Yes\", \"No\"))\nsummary(college$Elite)\n\n No Yes \n699  78 \n\nplot(college$Outstate ~ college$Elite, xlab = \"Elite\", ylab = \"Outstate\")\n\n\n\n\n\n\n\npar(mfrow = c(2,2))\nfor (n in c(10, 20, 30, 40)) {\n  hist(college$Enroll, breaks = n, main = paste(\"n =\", n), xlab = \"Enroll\")\n}\n\n\n\n\n\n\n\n\n\n\n\nThis exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n\ndata &lt;- read.table(\"Auto.data\", header = TRUE, na.strings = \"?\")\ndata &lt;- na.omit(data)\n\n\nWhich of the predictors are quantitative, and which are qualitative?\n\n\nQuantitative: mpg,cylinders,displacement,horsepower, weight, acceleration, year. Qualitative: name, origin.\n\n\nWhat is the range of each quantitative predictor? You can answer this using the range() function.\n\n\nsapply(data[ ,1:7], range)\n\n      mpg cylinders displacement horsepower weight acceleration year\n[1,]  9.0         3           68         46   1613          8.0   70\n[2,] 46.6         8          455        230   5140         24.8   82\n\n\n\nWhat is the mean and standard deviation of each quantitative predictor?\n\n\ndata[, 1:7] |&gt;\n  pivot_longer(everything()) |&gt;\n  group_by(name) |&gt;\n  summarise(\n    Mean = mean(value),\n    SD = sd(value)\n  ) |&gt;\n  knitr::kable()\n\n\n\n\nname\nMean\nSD\n\n\n\n\nacceleration\n15.541327\n2.758864\n\n\ncylinders\n5.471939\n1.705783\n\n\ndisplacement\n194.411990\n104.644004\n\n\nhorsepower\n104.469388\n38.491160\n\n\nmpg\n23.445918\n7.805008\n\n\nweight\n2977.584184\n849.402560\n\n\nyear\n75.979592\n3.683737\n\n\n\n\n\n\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\n\ndata[-(10:85), 1:7] |&gt;\n  pivot_longer(everything()) |&gt;\n  group_by(name) |&gt;\n  summarise(\n    Range = diff(range(value)),\n    Mean = mean(value),\n    SD = sd(value)\n  ) |&gt;\n  knitr::kable()\n\n\n\n\nname\nRange\nMean\nSD\n\n\n\n\nacceleration\n16.3\n15.726899\n2.693721\n\n\ncylinders\n5.0\n5.373418\n1.654179\n\n\ndisplacement\n387.0\n187.240506\n99.678367\n\n\nhorsepower\n184.0\n100.721519\n35.708853\n\n\nmpg\n35.6\n24.404430\n7.867283\n\n\nweight\n3348.0\n2935.971519\n811.300208\n\n\nyear\n12.0\n77.145570\n3.106217\n\n\n\n\n\n\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\n\n\nggpairs(data[, 1:7])\n\n\n\n\n\n\n\n\n\nYes, there are both positive and negative correlation between the variables.\n\n\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\nYes, since other variables are correlated. However, horsepower, weight and displacement are highly related.\n\n\n\n\nThis exercise involves the Boston housing data set.\n\nTo begin, load in the Boston data set. The Boston data set is part of the ISLR2 library in R.\n\nNow the data set is contained in the object Boston.\n\nView(Boston)\n\nRead about the data set:\n\n?Boston\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\ndim(Boston)\n\n[1] 506  13\n\n\n\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\n\n\nggpairs(Boston[, 1:6])\n\n\n\n\n\n\n\n\n\nThere are no significant correlations associated with capita crime\n\n\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\nYes, there is mild positive correlation with rad and tax\n\n\ncor_boston &lt;- cor(Boston)\ncor_col &lt;- hcl.colors(33, \"Purple-Green\")\np &lt;- heatmap(cor_boston, col = cor_col, zlim = c(-1, 1), symm = TRUE, margins = c(8, 8), Rowv = NA)\nlegend(\"right\", c(1, rep(\"\", 15), 0, rep(\"\", 15), -1), fill = rev(cor_col), \n  bty = \"n\", border = \"transparent\", y.intersp = 0.5)\n\n\n\n\n\n\n\n\n\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\n\n# top 5% of crime,tax,and pupil-teacher ratios\ncat(\"High crime:\", sum(Boston$crim &gt; quantile(Boston$crim, 0.95)), \"\\n\") \n\nHigh crime: 26 \n\ncat(\"High tax:\", sum(Boston$tax &gt; quantile(Boston$tax, 0.95)), \"\\n\")\n\nHigh tax: 5 \n\ncat(\"Pupil-teacher ratio:\", sum(Boston$ptratio &gt; quantile(Boston$ptratio, 0.95)),\"\\n\")\n\nPupil-teacher ratio: 18 \n\n\n\nHow many of the census tracts in this data set bound the Charles river?\n\n\nsum(Boston$chas==1)\n\n[1] 35\n\n\n\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\n\nmedian_ptratio &lt;- median(Boston$ptratio)\ncat(\"Median pupil-teacher ratio:\", median_ptratio, \"\\n\")\n\nMedian pupil-teacher ratio: 19.05 \n\n\n\nWhich census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.\n\n\n# whichsuburbs that have the lowest median property values.\nwhich(Boston$medv == min(Boston$medv))\n\n# Values of other predictors for suburb 406\nBoston[406,]\n\nrange(Boston$crim)\n\n\nThere are two suburbs that have the lowest median property values. crime is negatively correlated with median property value.\n\n\nIn this data set, how many of the census tract average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n\ncat(\"More than 7 rooms:\",sum(Boston$rm &gt; 7),\"\\n\")\n\nMore than 7 rooms: 64 \n\ncat(\"More than 8 rooms:\", sum(Boston$rm &gt; 8), \"\\n\")\n\nMore than 8 rooms: 13 \n\n\nMaking summaries:\n\ntracts_more_than_8 &lt;- Boston[Boston$rm &gt; 8, ]\n# Reshape data to long format using pivot_longer()\ndata_long &lt;- tracts_more_than_8 |&gt;\n  pivot_longer(cols = -rm, names_to = \"Predictor\", values_to = \"Value\")\n\n# Create boxplots with facet_wrap\nggplot(data_long, aes(x = Predictor, y = Value, fill = Predictor)) +\n  geom_boxplot(outlier.color = \"red\", outlier.size = 1.5) +\n  facet_wrap(~ Predictor, scales = \"free\", ncol = 4) +  # Each plot gets its own scale\n  theme_minimal() +\n  labs(title = \"Boxplots for Predictors with More Than 8 Rooms\",\n       x = \"Predictors\",\n       y = \"Values\") +\n  theme(axis.text.x = element_blank(),  # Remove x-axis text (not meaningful here)\n        axis.ticks.x = element_blank(),\n        strip.text = element_text(size = 10, face = \"bold\"))"
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html#exercises-1",
    "href": "posts/2024-11-14-statistical-learning/index.html#exercises-1",
    "title": "An Introduction to Statistical Learning",
    "section": "Exercises",
    "text": "Exercises\n\nConceptual\n\n\nApplied"
  },
  {
    "objectID": "posts/2024-11-14-statistical-learning/index.html#exercises-2",
    "href": "posts/2024-11-14-statistical-learning/index.html#exercises-2",
    "title": "An Introduction to Statistical Learning",
    "section": "Exercises",
    "text": "Exercises\n\nConceptual\n\n\nApplied"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)), interval = \"confidence\") #  This function can be used to produce confidence and prediction intervals. Here, the numeric values are for prediction\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)), interval = \"prediction\")\nplot(lstat, medv)\nabline(lm.fit, lwd = 3) # lwd increases the width of the regression line by a factor of three (3). This works for plot() and lines() function too.\nabline(lm.fit, col = \"red\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abdoulie Jallow",
    "section": "",
    "text": "Hello! I’m Abdoulie, although most people call me Jallow. I have a bachelor’s degree in biological chemistry and am deeply interested in science, epistemology, and lifelong learning. Some of my favorite thinkers include Karl Popper, David Deutsch, and Nick Lane. I created this website to document and share my learning journey. Feel free to explore and see what I’m working on!"
  },
  {
    "objectID": "posts/jku-thesis-template/index.html",
    "href": "posts/jku-thesis-template/index.html",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "",
    "text": "This template is based on the Masters/Doctoral Thesis, LaTeX Template, Version 2.5 (27 Aug 2017), which Eli Holmes updated for Quarto. I further modified the Quarto template to fulfill JKU’s criteria for diploma theses and doctoral dissertations. Using Quarto was motivated by the difficulties I had when writing my first bachelor thesis in Microsoft Word. The main difficulties were ensuring consistent citation formatting, text alignment, and maintaining a tidy version-controlled workflow. Quarto effectively addresses these challenges. Above all, it combines data analysis and documentation into one platform thereby improving reproducibility."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#clone-the-template",
    "href": "posts/jku-thesis-template/index.html#clone-the-template",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "1. Clone the Template",
    "text": "1. Clone the Template\nNavigate to the directory where you want to create your thesis files. Run the following command in a terminal to download the template and its associated folders:\n\nquarto use template jallow-code/jku-quarto-thesis\n\nYou will be prompted to specify an empty directory name for the template files. Provide a name for the new directory, which will store your thesis files."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#render-the-template",
    "href": "posts/jku-thesis-template/index.html#render-the-template",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "2. Render the Template",
    "text": "2. Render the Template\nOnce the template is installed, change into the new directory:\n\ncd &lt;new-directory-name&gt;\n\nTo generate the document, use the command:\n\nquarto render\n\n\nTroubleshooting Errors\nIf you encounter the following error during rendering:\n\nERROR: \ncompilation failed - missing packages (automatic install disabled)\nLaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H &lt;return&gt;  for immediate help.\n ...\nl.872 \\end{CSLReferences}\nThis error is likely due to outdated versions of Quarto or TinyTeX. To resolve it:\n\nUpdate your TinyTex installation.\nEnsure you are using the latest version of Quarto."
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#thesis-type-and-frontmatter",
    "href": "posts/jku-thesis-template/index.html#thesis-type-and-frontmatter",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Thesis Type and Frontmatter",
    "text": "Thesis Type and Frontmatter\nThe template is configured for a Bachelor’s thesis by default. To change this, edit the _quarto.yml file. For example:\n\nthesis:\n  type: Bachelor Thesis # Or \"Master Thesis\", \"Doctoral Dissertation\", etc.\n  degree-name: Bachelor of Science\n  group: Biological Chemistry # name of your study program\n  supervisor:\n    primary: \"Assoz. Univ.-Prof. Dr. Ian Teasdale\"\n    co: \"Dipl.-Ing. Michael Kneidinger\"\n  university: Johannes Kepler Universität Linz\n  department: Institute of Polymer Chemistry\n  faculty: Faculty of Engineering and Natural Sciences\nYou can also customize elements of the front matter (e.g., acknowledgments, dedication) by commenting out or modifying the respective sections in_quarto.yml. For instance, to exclude the dedication page:\n\n# dedication: \"Frontmatter/dedication.tex\""
  },
  {
    "objectID": "posts/jku-thesis-template/index.html#adding-symbols-or-abbreviations",
    "href": "posts/jku-thesis-template/index.html#adding-symbols-or-abbreviations",
    "title": "A Quarto Thesis Template for Bachelor and Diploma Projects at JKU",
    "section": "Adding Symbols or Abbreviations",
    "text": "Adding Symbols or Abbreviations\nThe Frontmatter folder contains LaTeX files for abbreviations, symbols, and other sections. If needed, search online for LaTeX code examples to add symbols or format abbreviations."
  }
]