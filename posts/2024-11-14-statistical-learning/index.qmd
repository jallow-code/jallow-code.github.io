---
title: "An Introduction to Statistical Learning"
description: "In this blog post, I’ll share my solutions to exercises from *An Introduction to Statistical Learning* by Gareth M. James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. Additionally, I’ll provide a brief summary of each chapter to help contextualize the exercises and key concepts covered in the book."
author:
  - name: Abdoulie Jallow
    url: https://jallow-code.github.io/
date: 14 11 2024
date-modified: last-modified
categories: [Learn, R] # self-defined categories
citation: 
  url: https://jallow-code.github.io/posts/2024-11-14-statistical-learning/ 
image: stat_image.jpg
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
toc-title: Table of Contents
---

# Statistical Learning

## Summary

The association between predictors ($X = X_1, X_2,...,X_p$) and a quantitative response (Y) can be expressed as $Y = f(X) + \epsilon$. Where $f$ is some fixed unknown function, and $\epsilon$ is a random error term with a zero mean. Statistical learning refers to a large collection of tools for estimating $f$. These tools can be classified either supervised or unsupervised. Supervised learning involves estimating an output based on one or more inputs. In contrast, unsupervised learning involves inputs but no supervising outputs. There are two primary reasons for predicting $f$:

-   **Prediction**: This applies to situations were inputs (X) are readily available, but the output (Y) can not be easily obtained. $Y$ can be estimated using $\hat{Y} = \hat{f}(X)$ where $\hat{f}$ is our estimate for the true function $f$. In this setting, one is not typically concerned with the exact form of $\hat{f}$ given it yields accurate predictions for $Y$. The accuracy of $\hat{f}$ depends on two quantities known as *reducible error* and *Irreducible error*. The reducible error can be improved by using an appropriate statistical learning method to estimate $f$. The Irreducible error is the variability associated with the error term ($Var(\epsilon)$). Since $Y$ also depends on $\epsilon$, it is impossible to achieve a perfect estimate of $Y$. The irreducible error will always be greater than zero, as it may include unmeasured variables or unmeasurable variations influencing $Y$.

-   **Inference:** In this setting we're interested in the association between $Y$ and $X$ hence the exact form of $\hat{f}$ is very important. One may be interested in answering the following questions: Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?

$f$ can be estimated using either **parametric** or **non-parametric** methods. parametric methods involve first assuming the shape of $f$ (e.g., linear) and then using training data to fit or train the model. These methods are less flexible but more interpretable. Non-parametric methods do not make assumptions about the shape of $f$, allowing for a wider range of possible forms. However, they tend to be less interpretable. Variables can be characterized as either quantitative or qualitative (also known as categorical). Problems with a quantitative response are often referred to as regression problems, while those involving a qualitative response are often referred to as classification problems. In regression settings the measure for the accuracy of a model is given by the **mean squared error** (MSE) . We choose a statistical learning method that gives the lowest *test MSE*. The *test MSE* depends on **variance** of $\hat{f}(x_0)$, the squared **bias** of $\hat{f}(x_0)$, and var($\epsilon$). To achieve a low *test MSE*, one needs to choose a model that simultaneously results in *low variance* and *low bias*. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. 

In classification settings, model accuracy is evaluated by the proportion of incorrect predictions made when applying the estimated function $\hat{f}$ to a set of test observations. This proportion is referred to as the **test error rate**. A good classifier minimizes the test error rate, achieving the highest possible accuracy on unseen data. The **Bayes classifier** is an approach in classification that minimizes the test error rate by assigning each observation to the class with the highest conditional probability, $\Pr(Y = j | X = x_0)$. The decision boundary separating classes is called the **Bayes decision boundary**. This classifier achieves the lowest possible error rate, known as the **Bayes error rate**, which arises due to overlapping classes in the data. However, because the true conditional probabilities are generally unknown, the Bayes classifier serves as a theoretical benchmark. In practice, methods like **k-Nearest Neighbors (k-NN)** estimate these probabilities by considering the closest $K$ training points to a test observation. k-NN assigns the class with the majority vote among these neighbors. The flexibility of k-NN depends on $K$; small $K$ values result in high variance but low bias, while large $K$ values lead to high bias but low variance. The right balance is important, as overly flexible models overfit the training data, and less flexible ones underfit. The tradeoff is illustrated by the characteristic U-shape of the test error as flexibility increases.

## Exercises

### Conceptual

